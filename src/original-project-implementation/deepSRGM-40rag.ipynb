{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6My7bxkUNBA0",
    "outputId": "f6ca1cdb-93d5-4777-b7d7-78f3609f2f22"
   },
   "source": [
    "# Trial with full 40-rag Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/teamdaedulus\n"
     ]
    }
   ],
   "source": [
    "%cd /home/teamdaedulus/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IHqzijIt1KkP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bPeWSOzV1w5F"
   },
   "outputs": [],
   "source": [
    "# \n",
    "# input_size = 128\n",
    "# hidden_size = 768\n",
    "# num_layers = 1\n",
    "# num_classes = 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JQ07tRnXNdJe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: X - torch.Size([72000, 5000]), Y - torch.Size([72000])\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.from_numpy(np.load(\"data/X_train40.npy\")).long()\n",
    "Y_train = torch.from_numpy(np.load(\"data/Y_train40.npy\")).long()\n",
    "print(f\"Training data: X - {X_train.shape}, Y - {Y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lzndq-fYCGg4",
    "outputId": "3f0b7576-3cf1-4e5a-a3ec-b22b601cf0ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "torch.cuda.set_device(1)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ww3lJELsH-9J"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EC1B-F1x6O8w"
   },
   "outputs": [],
   "source": [
    "# Code inspired from the following blog post\n",
    "# https://towardsdatascience.com/nlp-learning-series-part-3-attention-cnn-and-what-not-for-text-classification-4313930ed566\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)        \n",
    "        self.supports_masking = True\n",
    "        \n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.kaiming_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim \n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7YiE6NGXnQe4"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_length=5000, embedding_size=128, hidden_size=768,\n",
    "                num_layers=1, num_classes=40, vocab_size=209, drop_prob=0.5):\n",
    "      super(Model, self).__init__()\n",
    "      self.num_layers = num_layers\n",
    "      self.hidden_size = hidden_size\n",
    "      self.fc1 = nn.Linear(hidden_size, 384)\n",
    "      self.fc2 = nn.Linear(384, num_classes)\n",
    "      # self.batchNorm1d = nn.BatchNorm1d(input_length)\n",
    "      self.dropout = nn.Dropout(0.3)\n",
    "      self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers,\n",
    "                          dropout=drop_prob, batch_first=True)\n",
    "      self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "      self.attention_layer = Attention(hidden_size, input_length)\n",
    "      self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "      batch_size = x.size(0)\n",
    "      embeds = self.embeddings(x)\n",
    "      # print(\"embeds \", embeds.shape)\n",
    "      h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "      c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "      out, _ = self.lstm(embeds, (h0, c0)) #We don't need the hidden tensor\n",
    "      # print(\"lstm \", out.shape)\n",
    "      # out = self.batchNorm1d(out)\n",
    "      out = self.attention_layer(out)\n",
    "      # print(\"attention \", out.shape)\n",
    "     \n",
    "      out = self.relu(self.fc1(out))\n",
    "      # print(\"fc1 \", out.shape)\n",
    "      out = self.dropout(out)\n",
    "      out = self.fc2(out)\n",
    "      # print(\"output \", out.shape)\n",
    "\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oAUsE7T33CDm",
    "outputId": "fda32be9-00d8-4496-8627-327f4e3b518e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pipenv/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "# transfer weights of the 10rag dataset model\n",
    "pretrained_dict = torch.load(\"lstm_30_model.pth\")\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "# 1. filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k not in ['fc2.bias', 'fc2.weight']}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model_dict.update(pretrained_dict) \n",
    "# 3. load the new state dict\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reloading saved model\n",
    "\n",
    "model = Model().to(device)\n",
    "model.load_state_dict(torch.load(\"models40/lstm_25_checkpoint.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bgwx3dIsOuLD"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 40\n",
    "trainset = TensorDataset(X_train, Y_train)\n",
    "trainloader = DataLoader(trainset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vu0go4vnahLJ"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "phtGZ0DrcaRW",
    "outputId": "642774c3-d830-4a36-8e81-88505c47a0c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Batches Done: 100/1800 | Loss: 3.416\n",
      "Epoch: 1 | Batches Done: 200/1800 | Loss: 2.779\n",
      "Epoch: 1 | Batches Done: 300/1800 | Loss: 2.425\n",
      "Epoch: 1 | Batches Done: 400/1800 | Loss: 2.208\n",
      "Epoch: 1 | Batches Done: 500/1800 | Loss: 2.200\n",
      "Epoch: 1 | Batches Done: 600/1800 | Loss: 2.010\n",
      "Epoch: 1 | Batches Done: 700/1800 | Loss: 1.915\n",
      "Epoch: 1 | Batches Done: 800/1800 | Loss: 1.785\n",
      "Epoch: 1 | Batches Done: 900/1800 | Loss: 1.823\n",
      "Epoch: 1 | Batches Done: 1000/1800 | Loss: 1.735\n",
      "Epoch: 1 | Batches Done: 1100/1800 | Loss: 1.636\n",
      "Epoch: 1 | Batches Done: 1200/1800 | Loss: 1.583\n",
      "Epoch: 1 | Batches Done: 1300/1800 | Loss: 1.515\n",
      "Epoch: 1 | Batches Done: 1400/1800 | Loss: 1.529\n",
      "Epoch: 1 | Batches Done: 1500/1800 | Loss: 1.603\n",
      "Epoch: 1 | Batches Done: 1600/1800 | Loss: 1.459\n",
      "Epoch: 1 | Batches Done: 1700/1800 | Loss: 1.428\n",
      "Epoch: 1 | Batches Done: 1800/1800 | Loss: 1.436\n",
      "==================================================\n",
      "EPOCH 1 OVERALL LOSS: 1.916\n",
      "==================================================\n",
      "Epoch: 2 | Batches Done: 100/1800 | Loss: 1.380\n",
      "Epoch: 2 | Batches Done: 200/1800 | Loss: 1.328\n",
      "Epoch: 2 | Batches Done: 300/1800 | Loss: 1.347\n",
      "Epoch: 2 | Batches Done: 400/1800 | Loss: 1.384\n",
      "Epoch: 2 | Batches Done: 500/1800 | Loss: 1.338\n",
      "Epoch: 2 | Batches Done: 600/1800 | Loss: 1.304\n",
      "Epoch: 2 | Batches Done: 700/1800 | Loss: 1.313\n",
      "Epoch: 2 | Batches Done: 800/1800 | Loss: 1.379\n",
      "Epoch: 2 | Batches Done: 900/1800 | Loss: 1.338\n",
      "Epoch: 2 | Batches Done: 1000/1800 | Loss: 1.293\n",
      "Epoch: 2 | Batches Done: 1100/1800 | Loss: 1.207\n",
      "Epoch: 2 | Batches Done: 1200/1800 | Loss: 1.278\n",
      "Epoch: 2 | Batches Done: 1300/1800 | Loss: 1.245\n",
      "Epoch: 2 | Batches Done: 1400/1800 | Loss: 1.190\n",
      "Epoch: 2 | Batches Done: 1500/1800 | Loss: 1.140\n",
      "Epoch: 2 | Batches Done: 1600/1800 | Loss: 1.128\n",
      "Epoch: 2 | Batches Done: 1700/1800 | Loss: 1.148\n",
      "Epoch: 2 | Batches Done: 1800/1800 | Loss: 1.122\n",
      "==================================================\n",
      "EPOCH 2 OVERALL LOSS: 1.270\n",
      "==================================================\n",
      "Epoch: 3 | Batches Done: 100/1800 | Loss: 1.216\n",
      "Epoch: 3 | Batches Done: 200/1800 | Loss: 1.138\n",
      "Epoch: 3 | Batches Done: 300/1800 | Loss: 1.095\n",
      "Epoch: 3 | Batches Done: 400/1800 | Loss: 1.049\n",
      "Epoch: 3 | Batches Done: 500/1800 | Loss: 1.193\n",
      "Epoch: 3 | Batches Done: 600/1800 | Loss: 1.133\n",
      "Epoch: 3 | Batches Done: 700/1800 | Loss: 1.087\n",
      "Epoch: 3 | Batches Done: 800/1800 | Loss: 1.052\n",
      "Epoch: 3 | Batches Done: 900/1800 | Loss: 1.028\n",
      "Epoch: 3 | Batches Done: 1000/1800 | Loss: 1.029\n",
      "Epoch: 3 | Batches Done: 1100/1800 | Loss: 0.973\n",
      "Epoch: 3 | Batches Done: 1200/1800 | Loss: 0.970\n",
      "Epoch: 3 | Batches Done: 1300/1800 | Loss: 1.021\n",
      "Epoch: 3 | Batches Done: 1400/1800 | Loss: 0.998\n",
      "Epoch: 3 | Batches Done: 1500/1800 | Loss: 0.963\n",
      "Epoch: 3 | Batches Done: 1600/1800 | Loss: 0.967\n",
      "Epoch: 3 | Batches Done: 1700/1800 | Loss: 0.975\n",
      "Epoch: 3 | Batches Done: 1800/1800 | Loss: 0.994\n",
      "==================================================\n",
      "EPOCH 3 OVERALL LOSS: 1.049\n",
      "==================================================\n",
      "Epoch: 4 | Batches Done: 100/1800 | Loss: 0.923\n",
      "Epoch: 4 | Batches Done: 200/1800 | Loss: 0.920\n",
      "Epoch: 4 | Batches Done: 300/1800 | Loss: 0.873\n",
      "Epoch: 4 | Batches Done: 400/1800 | Loss: 0.893\n",
      "Epoch: 4 | Batches Done: 500/1800 | Loss: 0.871\n",
      "Epoch: 4 | Batches Done: 600/1800 | Loss: 0.859\n",
      "Epoch: 4 | Batches Done: 700/1800 | Loss: 0.863\n",
      "Epoch: 4 | Batches Done: 800/1800 | Loss: 0.836\n",
      "Epoch: 4 | Batches Done: 900/1800 | Loss: 0.831\n",
      "Epoch: 4 | Batches Done: 1000/1800 | Loss: 0.855\n",
      "Epoch: 4 | Batches Done: 1100/1800 | Loss: 0.943\n",
      "Epoch: 4 | Batches Done: 1200/1800 | Loss: 0.885\n",
      "Epoch: 4 | Batches Done: 1300/1800 | Loss: 0.832\n",
      "Epoch: 4 | Batches Done: 1400/1800 | Loss: 0.859\n",
      "Epoch: 4 | Batches Done: 1500/1800 | Loss: 0.838\n",
      "Epoch: 4 | Batches Done: 1600/1800 | Loss: 0.945\n",
      "Epoch: 4 | Batches Done: 1700/1800 | Loss: 0.875\n",
      "Epoch: 4 | Batches Done: 1800/1800 | Loss: 0.839\n",
      "==================================================\n",
      "EPOCH 4 OVERALL LOSS: 0.874\n",
      "==================================================\n",
      "Epoch: 5 | Batches Done: 100/1800 | Loss: 0.964\n",
      "Epoch: 5 | Batches Done: 200/1800 | Loss: 0.923\n",
      "Epoch: 5 | Batches Done: 300/1800 | Loss: 0.879\n",
      "Epoch: 5 | Batches Done: 400/1800 | Loss: 0.814\n",
      "Epoch: 5 | Batches Done: 500/1800 | Loss: 0.797\n",
      "Epoch: 5 | Batches Done: 600/1800 | Loss: 0.821\n",
      "Epoch: 5 | Batches Done: 700/1800 | Loss: 0.809\n",
      "Epoch: 5 | Batches Done: 800/1800 | Loss: 0.772\n",
      "Epoch: 5 | Batches Done: 900/1800 | Loss: 0.771\n",
      "Epoch: 5 | Batches Done: 1000/1800 | Loss: 0.779\n",
      "Epoch: 5 | Batches Done: 1100/1800 | Loss: 0.825\n",
      "Epoch: 5 | Batches Done: 1200/1800 | Loss: 0.975\n",
      "Epoch: 5 | Batches Done: 1300/1800 | Loss: 0.935\n",
      "Epoch: 5 | Batches Done: 1400/1800 | Loss: 0.826\n",
      "Epoch: 5 | Batches Done: 1500/1800 | Loss: 0.799\n",
      "Epoch: 5 | Batches Done: 1600/1800 | Loss: 0.760\n",
      "Epoch: 5 | Batches Done: 1700/1800 | Loss: 0.742\n",
      "Epoch: 5 | Batches Done: 1800/1800 | Loss: 0.753\n",
      "==================================================\n",
      "EPOCH 5 OVERALL LOSS: 0.830\n",
      "==================================================\n",
      "Saving model weights at Epoch 5 ...\n",
      "Epoch: 6 | Batches Done: 100/1800 | Loss: 0.721\n",
      "Epoch: 6 | Batches Done: 200/1800 | Loss: 0.726\n",
      "Epoch: 6 | Batches Done: 300/1800 | Loss: 0.677\n",
      "Epoch: 6 | Batches Done: 400/1800 | Loss: 0.700\n",
      "Epoch: 6 | Batches Done: 500/1800 | Loss: 0.717\n",
      "Epoch: 6 | Batches Done: 600/1800 | Loss: 0.676\n",
      "Epoch: 6 | Batches Done: 700/1800 | Loss: 0.689\n",
      "Epoch: 6 | Batches Done: 800/1800 | Loss: 0.669\n",
      "Epoch: 6 | Batches Done: 900/1800 | Loss: 0.662\n",
      "Epoch: 6 | Batches Done: 1000/1800 | Loss: 0.682\n",
      "Epoch: 6 | Batches Done: 1100/1800 | Loss: 0.688\n",
      "Epoch: 6 | Batches Done: 1200/1800 | Loss: 0.706\n",
      "Epoch: 6 | Batches Done: 1300/1800 | Loss: 0.646\n",
      "Epoch: 6 | Batches Done: 1400/1800 | Loss: 0.643\n",
      "Epoch: 6 | Batches Done: 1500/1800 | Loss: 0.632\n",
      "Epoch: 6 | Batches Done: 1600/1800 | Loss: 0.644\n",
      "Epoch: 6 | Batches Done: 1700/1800 | Loss: 0.639\n",
      "Epoch: 6 | Batches Done: 1800/1800 | Loss: 0.660\n",
      "==================================================\n",
      "EPOCH 6 OVERALL LOSS: 0.677\n",
      "==================================================\n",
      "Epoch: 7 | Batches Done: 100/1800 | Loss: 0.638\n",
      "Epoch: 7 | Batches Done: 200/1800 | Loss: 0.628\n",
      "Epoch: 7 | Batches Done: 300/1800 | Loss: 0.656\n",
      "Epoch: 7 | Batches Done: 400/1800 | Loss: 0.608\n",
      "Epoch: 7 | Batches Done: 500/1800 | Loss: 0.636\n",
      "Epoch: 7 | Batches Done: 600/1800 | Loss: 0.614\n",
      "Epoch: 7 | Batches Done: 700/1800 | Loss: 0.596\n",
      "Epoch: 7 | Batches Done: 800/1800 | Loss: 0.622\n",
      "Epoch: 7 | Batches Done: 900/1800 | Loss: 0.595\n",
      "Epoch: 7 | Batches Done: 1000/1800 | Loss: 0.596\n",
      "Epoch: 7 | Batches Done: 1100/1800 | Loss: 0.587\n",
      "Epoch: 7 | Batches Done: 1200/1800 | Loss: 0.593\n",
      "Epoch: 7 | Batches Done: 1300/1800 | Loss: 0.592\n",
      "Epoch: 7 | Batches Done: 1400/1800 | Loss: 0.649\n",
      "Epoch: 7 | Batches Done: 1500/1800 | Loss: 0.622\n",
      "Epoch: 7 | Batches Done: 1600/1800 | Loss: 0.639\n",
      "Epoch: 7 | Batches Done: 1700/1800 | Loss: 0.694\n",
      "Epoch: 7 | Batches Done: 1800/1800 | Loss: 0.717\n",
      "==================================================\n",
      "EPOCH 7 OVERALL LOSS: 0.627\n",
      "==================================================\n",
      "Epoch: 8 | Batches Done: 100/1800 | Loss: 0.608\n",
      "Epoch: 8 | Batches Done: 200/1800 | Loss: 0.595\n",
      "Epoch: 8 | Batches Done: 300/1800 | Loss: 0.603\n",
      "Epoch: 8 | Batches Done: 400/1800 | Loss: 0.584\n",
      "Epoch: 8 | Batches Done: 500/1800 | Loss: 0.612\n",
      "Epoch: 8 | Batches Done: 600/1800 | Loss: 0.547\n",
      "Epoch: 8 | Batches Done: 700/1800 | Loss: 0.562\n",
      "Epoch: 8 | Batches Done: 800/1800 | Loss: 0.547\n",
      "Epoch: 8 | Batches Done: 900/1800 | Loss: 0.516\n",
      "Epoch: 8 | Batches Done: 1000/1800 | Loss: 0.570\n",
      "Epoch: 8 | Batches Done: 1100/1800 | Loss: 0.586\n",
      "Epoch: 8 | Batches Done: 1200/1800 | Loss: 0.552\n",
      "Epoch: 8 | Batches Done: 1300/1800 | Loss: 0.552\n",
      "Epoch: 8 | Batches Done: 1400/1800 | Loss: 0.550\n",
      "Epoch: 8 | Batches Done: 1500/1800 | Loss: 0.527\n",
      "Epoch: 8 | Batches Done: 1600/1800 | Loss: 0.548\n",
      "Epoch: 8 | Batches Done: 1700/1800 | Loss: 0.557\n",
      "Epoch: 8 | Batches Done: 1800/1800 | Loss: 0.548\n",
      "==================================================\n",
      "EPOCH 8 OVERALL LOSS: 0.565\n",
      "==================================================\n",
      "Epoch: 9 | Batches Done: 100/1800 | Loss: 0.514\n",
      "Epoch: 9 | Batches Done: 200/1800 | Loss: 0.518\n",
      "Epoch: 9 | Batches Done: 300/1800 | Loss: 0.508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Batches Done: 400/1800 | Loss: 0.504\n",
      "Epoch: 9 | Batches Done: 500/1800 | Loss: 0.505\n",
      "Epoch: 9 | Batches Done: 600/1800 | Loss: 0.546\n",
      "Epoch: 9 | Batches Done: 700/1800 | Loss: 0.521\n",
      "Epoch: 9 | Batches Done: 800/1800 | Loss: 0.483\n",
      "Epoch: 9 | Batches Done: 900/1800 | Loss: 0.513\n",
      "Epoch: 9 | Batches Done: 1000/1800 | Loss: 0.479\n",
      "Epoch: 9 | Batches Done: 1100/1800 | Loss: 0.518\n",
      "Epoch: 9 | Batches Done: 1200/1800 | Loss: 0.511\n",
      "Epoch: 9 | Batches Done: 1300/1800 | Loss: 0.512\n",
      "Epoch: 9 | Batches Done: 1400/1800 | Loss: 0.491\n",
      "Epoch: 9 | Batches Done: 1500/1800 | Loss: 0.479\n",
      "Epoch: 9 | Batches Done: 1600/1800 | Loss: 0.465\n",
      "Epoch: 9 | Batches Done: 1700/1800 | Loss: 0.513\n",
      "Epoch: 9 | Batches Done: 1800/1800 | Loss: 0.501\n",
      "==================================================\n",
      "EPOCH 9 OVERALL LOSS: 0.504\n",
      "==================================================\n",
      "Epoch: 10 | Batches Done: 100/1800 | Loss: 0.518\n",
      "Epoch: 10 | Batches Done: 200/1800 | Loss: 0.561\n",
      "Epoch: 10 | Batches Done: 300/1800 | Loss: 0.510\n",
      "Epoch: 10 | Batches Done: 400/1800 | Loss: 0.476\n",
      "Epoch: 10 | Batches Done: 500/1800 | Loss: 0.464\n",
      "Epoch: 10 | Batches Done: 600/1800 | Loss: 0.423\n",
      "Epoch: 10 | Batches Done: 700/1800 | Loss: 0.433\n",
      "Epoch: 10 | Batches Done: 800/1800 | Loss: 0.444\n",
      "Epoch: 10 | Batches Done: 900/1800 | Loss: 0.457\n",
      "Epoch: 10 | Batches Done: 1000/1800 | Loss: 0.459\n",
      "Epoch: 10 | Batches Done: 1100/1800 | Loss: 0.449\n",
      "Epoch: 10 | Batches Done: 1200/1800 | Loss: 0.452\n",
      "Epoch: 10 | Batches Done: 1300/1800 | Loss: 0.453\n",
      "Epoch: 10 | Batches Done: 1400/1800 | Loss: 0.445\n",
      "Epoch: 10 | Batches Done: 1500/1800 | Loss: 0.458\n",
      "Epoch: 10 | Batches Done: 1600/1800 | Loss: 0.452\n",
      "Epoch: 10 | Batches Done: 1700/1800 | Loss: 0.424\n",
      "Epoch: 10 | Batches Done: 1800/1800 | Loss: 0.445\n",
      "==================================================\n",
      "EPOCH 10 OVERALL LOSS: 0.462\n",
      "==================================================\n",
      "Saving model weights at Epoch 10 ...\n",
      "Epoch: 11 | Batches Done: 100/1800 | Loss: 0.401\n",
      "Epoch: 11 | Batches Done: 200/1800 | Loss: 0.424\n",
      "Epoch: 11 | Batches Done: 300/1800 | Loss: 0.407\n",
      "Epoch: 11 | Batches Done: 400/1800 | Loss: 0.428\n",
      "Epoch: 11 | Batches Done: 500/1800 | Loss: 0.447\n",
      "Epoch: 11 | Batches Done: 600/1800 | Loss: 0.448\n",
      "Epoch: 11 | Batches Done: 700/1800 | Loss: 0.436\n",
      "Epoch: 11 | Batches Done: 800/1800 | Loss: 0.420\n",
      "Epoch: 11 | Batches Done: 900/1800 | Loss: 0.452\n",
      "Epoch: 11 | Batches Done: 1000/1800 | Loss: 0.464\n",
      "Epoch: 11 | Batches Done: 1100/1800 | Loss: 0.446\n",
      "Epoch: 11 | Batches Done: 1200/1800 | Loss: 0.453\n",
      "Epoch: 11 | Batches Done: 1300/1800 | Loss: 0.462\n",
      "Epoch: 11 | Batches Done: 1400/1800 | Loss: 0.442\n",
      "Epoch: 11 | Batches Done: 1500/1800 | Loss: 0.522\n",
      "Epoch: 11 | Batches Done: 1600/1800 | Loss: 0.491\n",
      "Epoch: 11 | Batches Done: 1700/1800 | Loss: 0.430\n",
      "Epoch: 11 | Batches Done: 1800/1800 | Loss: 0.425\n",
      "==================================================\n",
      "EPOCH 11 OVERALL LOSS: 0.444\n",
      "==================================================\n",
      "Epoch: 12 | Batches Done: 100/1800 | Loss: 0.379\n",
      "Epoch: 12 | Batches Done: 200/1800 | Loss: 0.412\n",
      "Epoch: 12 | Batches Done: 300/1800 | Loss: 0.402\n",
      "Epoch: 12 | Batches Done: 400/1800 | Loss: 0.417\n",
      "Epoch: 12 | Batches Done: 500/1800 | Loss: 0.396\n",
      "Epoch: 12 | Batches Done: 600/1800 | Loss: 0.420\n",
      "Epoch: 12 | Batches Done: 700/1800 | Loss: 0.407\n",
      "Epoch: 12 | Batches Done: 800/1800 | Loss: 0.413\n",
      "Epoch: 12 | Batches Done: 900/1800 | Loss: 0.376\n",
      "Epoch: 12 | Batches Done: 1000/1800 | Loss: 0.388\n",
      "Epoch: 12 | Batches Done: 1100/1800 | Loss: 0.393\n",
      "Epoch: 12 | Batches Done: 1200/1800 | Loss: 0.410\n",
      "Epoch: 12 | Batches Done: 1300/1800 | Loss: 0.427\n",
      "Epoch: 12 | Batches Done: 1400/1800 | Loss: 0.405\n",
      "Epoch: 12 | Batches Done: 1500/1800 | Loss: 0.401\n",
      "Epoch: 12 | Batches Done: 1600/1800 | Loss: 0.396\n",
      "Epoch: 12 | Batches Done: 1700/1800 | Loss: 0.409\n",
      "Epoch: 12 | Batches Done: 1800/1800 | Loss: 0.349\n",
      "==================================================\n",
      "EPOCH 12 OVERALL LOSS: 0.400\n",
      "==================================================\n",
      "Epoch: 13 | Batches Done: 100/1800 | Loss: 0.323\n",
      "Epoch: 13 | Batches Done: 200/1800 | Loss: 0.341\n",
      "Epoch: 13 | Batches Done: 300/1800 | Loss: 0.352\n",
      "Epoch: 13 | Batches Done: 400/1800 | Loss: 0.372\n",
      "Epoch: 13 | Batches Done: 500/1800 | Loss: 0.391\n",
      "Epoch: 13 | Batches Done: 600/1800 | Loss: 0.364\n",
      "Epoch: 13 | Batches Done: 700/1800 | Loss: 0.389\n",
      "Epoch: 13 | Batches Done: 800/1800 | Loss: 0.356\n",
      "Epoch: 13 | Batches Done: 900/1800 | Loss: 0.340\n",
      "Epoch: 13 | Batches Done: 1000/1800 | Loss: 0.365\n",
      "Epoch: 13 | Batches Done: 1100/1800 | Loss: 0.366\n",
      "Epoch: 13 | Batches Done: 1200/1800 | Loss: 0.343\n",
      "Epoch: 13 | Batches Done: 1300/1800 | Loss: 0.324\n",
      "Epoch: 13 | Batches Done: 1400/1800 | Loss: 0.371\n",
      "Epoch: 13 | Batches Done: 1500/1800 | Loss: 0.376\n",
      "Epoch: 13 | Batches Done: 1600/1800 | Loss: 0.352\n",
      "Epoch: 13 | Batches Done: 1700/1800 | Loss: 0.351\n",
      "Epoch: 13 | Batches Done: 1800/1800 | Loss: 0.345\n",
      "==================================================\n",
      "EPOCH 13 OVERALL LOSS: 0.357\n",
      "==================================================\n",
      "Epoch: 14 | Batches Done: 100/1800 | Loss: 0.336\n",
      "Epoch: 14 | Batches Done: 200/1800 | Loss: 0.336\n",
      "Epoch: 14 | Batches Done: 300/1800 | Loss: 0.331\n",
      "Epoch: 14 | Batches Done: 400/1800 | Loss: 0.330\n",
      "Epoch: 14 | Batches Done: 500/1800 | Loss: 0.319\n",
      "Epoch: 14 | Batches Done: 700/1800 | Loss: 0.332\n",
      "Epoch: 14 | Batches Done: 800/1800 | Loss: 0.341\n",
      "Epoch: 14 | Batches Done: 900/1800 | Loss: 0.335\n",
      "Epoch: 14 | Batches Done: 1000/1800 | Loss: 0.333\n",
      "Epoch: 14 | Batches Done: 1100/1800 | Loss: 0.339\n",
      "Epoch: 14 | Batches Done: 1200/1800 | Loss: 0.327\n",
      "Epoch: 14 | Batches Done: 1300/1800 | Loss: 0.350\n",
      "Epoch: 14 | Batches Done: 1400/1800 | Loss: 0.334\n",
      "Epoch: 14 | Batches Done: 1500/1800 | Loss: 0.350\n",
      "Epoch: 14 | Batches Done: 1600/1800 | Loss: 0.338\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "model.train()\n",
    "losslist = []\n",
    "for epoch in range(15):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    epoch_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print(f'Epoch: {epoch+1} | Batches Done: {i+1}/1800 | Loss: {(running_loss/100):.3f}')\n",
    "            epoch_loss += running_loss\n",
    "            running_loss = 0.0\n",
    "    losslist.append(epoch_loss/1800)\n",
    "    print(\"=\"*50)\n",
    "    print(f\"EPOCH {epoch+1} OVERALL LOSS: {losslist[-1]:.3f}\")\n",
    "    print(\"=\"*50)\n",
    "    if epoch % 5 == 4:\n",
    "        path = f\"models40/lstm_{epoch+1}_checkpoint.pth\"\n",
    "        print(f\"Saving model weights at Epoch {epoch+1} ...\")\n",
    "        torch.save(model.state_dict(), path)\n",
    "print(\"Finished Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Batches Done: 100/1800 | Loss: 0.424\n",
      "Epoch: 11 | Batches Done: 200/1800 | Loss: 0.430\n",
      "Epoch: 11 | Batches Done: 300/1800 | Loss: 0.410\n",
      "Epoch: 11 | Batches Done: 400/1800 | Loss: 0.402\n",
      "Epoch: 11 | Batches Done: 500/1800 | Loss: 0.452\n",
      "Epoch: 11 | Batches Done: 600/1800 | Loss: 0.421\n",
      "Epoch: 11 | Batches Done: 700/1800 | Loss: 0.408\n",
      "Epoch: 11 | Batches Done: 800/1800 | Loss: 0.404\n",
      "Epoch: 11 | Batches Done: 900/1800 | Loss: 0.426\n",
      "Epoch: 11 | Batches Done: 1000/1800 | Loss: 0.434\n",
      "Epoch: 11 | Batches Done: 1100/1800 | Loss: 0.415\n",
      "Epoch: 11 | Batches Done: 1200/1800 | Loss: 0.421\n",
      "Epoch: 11 | Batches Done: 1300/1800 | Loss: 0.451\n",
      "Epoch: 11 | Batches Done: 1400/1800 | Loss: 0.436\n",
      "Epoch: 11 | Batches Done: 1500/1800 | Loss: 0.395\n",
      "Epoch: 11 | Batches Done: 1600/1800 | Loss: 0.380\n",
      "Epoch: 11 | Batches Done: 1700/1800 | Loss: 0.393\n",
      "Epoch: 11 | Batches Done: 1800/1800 | Loss: 0.459\n",
      "==================================================\n",
      "EPOCH 11 OVERALL LOSS: 0.420\n",
      "==================================================\n",
      "Epoch: 12 | Batches Done: 100/1800 | Loss: 0.380\n",
      "Epoch: 12 | Batches Done: 200/1800 | Loss: 0.394\n",
      "Epoch: 12 | Batches Done: 300/1800 | Loss: 0.423\n",
      "Epoch: 12 | Batches Done: 400/1800 | Loss: 0.409\n",
      "Epoch: 12 | Batches Done: 500/1800 | Loss: 0.416\n",
      "Epoch: 12 | Batches Done: 600/1800 | Loss: 0.424\n",
      "Epoch: 12 | Batches Done: 700/1800 | Loss: 0.623\n",
      "Epoch: 12 | Batches Done: 800/1800 | Loss: 0.448\n",
      "Epoch: 12 | Batches Done: 900/1800 | Loss: 0.401\n",
      "Epoch: 12 | Batches Done: 1000/1800 | Loss: 0.384\n",
      "Epoch: 12 | Batches Done: 1100/1800 | Loss: 0.391\n",
      "Epoch: 12 | Batches Done: 1200/1800 | Loss: 0.393\n",
      "Epoch: 12 | Batches Done: 1300/1800 | Loss: 0.353\n",
      "Epoch: 12 | Batches Done: 1400/1800 | Loss: 0.367\n",
      "Epoch: 12 | Batches Done: 1500/1800 | Loss: 0.367\n",
      "Epoch: 12 | Batches Done: 1600/1800 | Loss: 0.367\n",
      "Epoch: 12 | Batches Done: 1700/1800 | Loss: 0.386\n",
      "Epoch: 12 | Batches Done: 1800/1800 | Loss: 0.366\n",
      "==================================================\n",
      "EPOCH 12 OVERALL LOSS: 0.405\n",
      "==================================================\n",
      "Epoch: 13 | Batches Done: 100/1800 | Loss: 0.340\n",
      "Epoch: 13 | Batches Done: 200/1800 | Loss: 0.331\n",
      "Epoch: 13 | Batches Done: 300/1800 | Loss: 0.352\n",
      "Epoch: 13 | Batches Done: 400/1800 | Loss: 0.341\n",
      "Epoch: 13 | Batches Done: 500/1800 | Loss: 0.357\n",
      "Epoch: 13 | Batches Done: 600/1800 | Loss: 0.337\n",
      "Epoch: 13 | Batches Done: 700/1800 | Loss: 0.352\n",
      "Epoch: 13 | Batches Done: 800/1800 | Loss: 0.340\n",
      "Epoch: 13 | Batches Done: 900/1800 | Loss: 0.360\n",
      "Epoch: 13 | Batches Done: 1000/1800 | Loss: 0.327\n",
      "Epoch: 13 | Batches Done: 1100/1800 | Loss: 0.353\n",
      "Epoch: 13 | Batches Done: 1200/1800 | Loss: 0.326\n",
      "Epoch: 13 | Batches Done: 1300/1800 | Loss: 0.333\n",
      "Epoch: 13 | Batches Done: 1400/1800 | Loss: 0.354\n",
      "Epoch: 13 | Batches Done: 1500/1800 | Loss: 0.353\n",
      "Epoch: 13 | Batches Done: 1600/1800 | Loss: 0.364\n",
      "Epoch: 13 | Batches Done: 1700/1800 | Loss: 0.354\n",
      "Epoch: 13 | Batches Done: 1800/1800 | Loss: 0.355\n",
      "==================================================\n",
      "EPOCH 13 OVERALL LOSS: 0.346\n",
      "==================================================\n",
      "Epoch: 14 | Batches Done: 100/1800 | Loss: 0.332\n",
      "Epoch: 14 | Batches Done: 200/1800 | Loss: 0.333\n",
      "Epoch: 14 | Batches Done: 300/1800 | Loss: 0.378\n",
      "Epoch: 14 | Batches Done: 400/1800 | Loss: 0.367\n",
      "Epoch: 14 | Batches Done: 500/1800 | Loss: 0.344\n",
      "Epoch: 14 | Batches Done: 600/1800 | Loss: 0.314\n",
      "Epoch: 14 | Batches Done: 700/1800 | Loss: 0.309\n",
      "Epoch: 14 | Batches Done: 800/1800 | Loss: 0.331\n",
      "Epoch: 14 | Batches Done: 900/1800 | Loss: 0.324\n",
      "Epoch: 14 | Batches Done: 1000/1800 | Loss: 0.324\n",
      "Epoch: 14 | Batches Done: 1100/1800 | Loss: 0.382\n",
      "Epoch: 14 | Batches Done: 1200/1800 | Loss: 0.343\n",
      "Epoch: 14 | Batches Done: 1300/1800 | Loss: 0.389\n",
      "Epoch: 14 | Batches Done: 1400/1800 | Loss: 0.343\n",
      "Epoch: 14 | Batches Done: 1500/1800 | Loss: 0.320\n",
      "Epoch: 14 | Batches Done: 1600/1800 | Loss: 0.328\n",
      "Epoch: 14 | Batches Done: 1700/1800 | Loss: 0.338\n",
      "Epoch: 14 | Batches Done: 1800/1800 | Loss: 0.295\n",
      "==================================================\n",
      "EPOCH 14 OVERALL LOSS: 0.338\n",
      "==================================================\n",
      "Epoch: 15 | Batches Done: 100/1800 | Loss: 0.298\n",
      "Epoch: 15 | Batches Done: 200/1800 | Loss: 0.311\n",
      "Epoch: 15 | Batches Done: 300/1800 | Loss: 0.302\n",
      "Epoch: 15 | Batches Done: 400/1800 | Loss: 0.319\n",
      "Epoch: 15 | Batches Done: 500/1800 | Loss: 0.306\n",
      "Epoch: 15 | Batches Done: 600/1800 | Loss: 0.304\n",
      "Epoch: 15 | Batches Done: 700/1800 | Loss: 0.337\n",
      "Epoch: 15 | Batches Done: 800/1800 | Loss: 0.300\n",
      "Epoch: 15 | Batches Done: 900/1800 | Loss: 0.297\n",
      "Epoch: 15 | Batches Done: 1000/1800 | Loss: 0.364\n",
      "Epoch: 15 | Batches Done: 1100/1800 | Loss: 0.410\n",
      "Epoch: 15 | Batches Done: 1200/1800 | Loss: 0.333\n",
      "Epoch: 15 | Batches Done: 1300/1800 | Loss: 0.333\n",
      "Epoch: 15 | Batches Done: 1400/1800 | Loss: 0.321\n",
      "Epoch: 15 | Batches Done: 1500/1800 | Loss: 0.328\n",
      "Epoch: 15 | Batches Done: 1600/1800 | Loss: 0.309\n",
      "Epoch: 15 | Batches Done: 1700/1800 | Loss: 0.303\n",
      "Epoch: 15 | Batches Done: 1800/1800 | Loss: 0.308\n",
      "==================================================\n",
      "EPOCH 15 OVERALL LOSS: 0.321\n",
      "==================================================\n",
      "Saving model weights at Epoch 15 ...\n",
      "Epoch: 16 | Batches Done: 100/1800 | Loss: 0.300\n",
      "Epoch: 16 | Batches Done: 200/1800 | Loss: 0.293\n",
      "Epoch: 16 | Batches Done: 300/1800 | Loss: 0.343\n",
      "Epoch: 16 | Batches Done: 400/1800 | Loss: 0.286\n",
      "Epoch: 16 | Batches Done: 500/1800 | Loss: 0.271\n",
      "Epoch: 16 | Batches Done: 600/1800 | Loss: 0.273\n",
      "Epoch: 16 | Batches Done: 700/1800 | Loss: 0.267\n",
      "Epoch: 16 | Batches Done: 800/1800 | Loss: 0.276\n",
      "Epoch: 16 | Batches Done: 900/1800 | Loss: 0.298\n",
      "Epoch: 16 | Batches Done: 1000/1800 | Loss: 0.307\n",
      "Epoch: 16 | Batches Done: 1100/1800 | Loss: 0.264\n",
      "Epoch: 16 | Batches Done: 1200/1800 | Loss: 0.291\n",
      "Epoch: 16 | Batches Done: 1300/1800 | Loss: 0.288\n",
      "Epoch: 16 | Batches Done: 1400/1800 | Loss: 0.259\n",
      "Epoch: 16 | Batches Done: 1500/1800 | Loss: 0.304\n",
      "Epoch: 16 | Batches Done: 1600/1800 | Loss: 0.735\n",
      "Epoch: 16 | Batches Done: 1700/1800 | Loss: 0.421\n",
      "Epoch: 16 | Batches Done: 1800/1800 | Loss: 0.338\n",
      "==================================================\n",
      "EPOCH 16 OVERALL LOSS: 0.323\n",
      "==================================================\n",
      "Epoch: 17 | Batches Done: 100/1800 | Loss: 0.320\n",
      "Epoch: 17 | Batches Done: 200/1800 | Loss: 0.306\n",
      "Epoch: 17 | Batches Done: 300/1800 | Loss: 0.297\n",
      "Epoch: 17 | Batches Done: 400/1800 | Loss: 0.335\n",
      "Epoch: 17 | Batches Done: 500/1800 | Loss: 0.308\n",
      "Epoch: 17 | Batches Done: 600/1800 | Loss: 0.286\n",
      "Epoch: 17 | Batches Done: 700/1800 | Loss: 0.267\n",
      "Epoch: 17 | Batches Done: 800/1800 | Loss: 0.273\n",
      "Epoch: 17 | Batches Done: 900/1800 | Loss: 0.253\n",
      "Epoch: 17 | Batches Done: 1000/1800 | Loss: 0.281\n",
      "Epoch: 17 | Batches Done: 1100/1800 | Loss: 0.265\n",
      "Epoch: 17 | Batches Done: 1200/1800 | Loss: 0.265\n",
      "Epoch: 17 | Batches Done: 1300/1800 | Loss: 0.231\n",
      "Epoch: 17 | Batches Done: 1400/1800 | Loss: 0.269\n",
      "Epoch: 17 | Batches Done: 1500/1800 | Loss: 0.303\n",
      "Epoch: 17 | Batches Done: 1600/1800 | Loss: 0.284\n",
      "Epoch: 17 | Batches Done: 1700/1800 | Loss: 0.250\n",
      "Epoch: 17 | Batches Done: 1800/1800 | Loss: 0.267\n",
      "==================================================\n",
      "EPOCH 17 OVERALL LOSS: 0.281\n",
      "==================================================\n",
      "Epoch: 18 | Batches Done: 100/1800 | Loss: 0.247\n",
      "Epoch: 18 | Batches Done: 200/1800 | Loss: 0.237\n",
      "Epoch: 18 | Batches Done: 300/1800 | Loss: 0.240\n",
      "Epoch: 18 | Batches Done: 400/1800 | Loss: 0.234\n",
      "Epoch: 18 | Batches Done: 500/1800 | Loss: 0.261\n",
      "Epoch: 18 | Batches Done: 600/1800 | Loss: 0.252\n",
      "Epoch: 18 | Batches Done: 700/1800 | Loss: 0.248\n",
      "Epoch: 18 | Batches Done: 800/1800 | Loss: 0.252\n",
      "Epoch: 18 | Batches Done: 900/1800 | Loss: 0.252\n",
      "Epoch: 18 | Batches Done: 1000/1800 | Loss: 0.258\n",
      "Epoch: 18 | Batches Done: 1100/1800 | Loss: 0.239\n",
      "Epoch: 18 | Batches Done: 1200/1800 | Loss: 0.226\n",
      "Epoch: 18 | Batches Done: 1300/1800 | Loss: 0.223\n",
      "Epoch: 18 | Batches Done: 1400/1800 | Loss: 0.241\n",
      "Epoch: 18 | Batches Done: 1500/1800 | Loss: 0.236\n",
      "Epoch: 18 | Batches Done: 1600/1800 | Loss: 0.249\n",
      "Epoch: 18 | Batches Done: 1700/1800 | Loss: 0.257\n",
      "Epoch: 18 | Batches Done: 1800/1800 | Loss: 0.228\n",
      "==================================================\n",
      "EPOCH 18 OVERALL LOSS: 0.243\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Batches Done: 100/1800 | Loss: 0.201\n",
      "Epoch: 19 | Batches Done: 200/1800 | Loss: 0.222\n",
      "Epoch: 19 | Batches Done: 300/1800 | Loss: 0.235\n",
      "Epoch: 19 | Batches Done: 400/1800 | Loss: 0.215\n",
      "Epoch: 19 | Batches Done: 500/1800 | Loss: 0.225\n",
      "Epoch: 19 | Batches Done: 600/1800 | Loss: 0.268\n",
      "Epoch: 19 | Batches Done: 700/1800 | Loss: 0.252\n",
      "Epoch: 19 | Batches Done: 800/1800 | Loss: 0.234\n",
      "Epoch: 19 | Batches Done: 900/1800 | Loss: 0.230\n",
      "Epoch: 19 | Batches Done: 1000/1800 | Loss: 0.245\n"
     ]
    }
   ],
   "source": [
    "# Continue training\n",
    "# training the model\n",
    "model.train()\n",
    "losslist = []\n",
    "for epoch in range(10, 31):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    epoch_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print(f'Epoch: {epoch+1} | Batches Done: {i+1}/1800 | Loss: {(running_loss/100):.3f}')\n",
    "            epoch_loss += running_loss\n",
    "            running_loss = 0.0\n",
    "    losslist.append(epoch_loss/1800)\n",
    "    print(\"=\"*50)\n",
    "    print(f\"EPOCH {epoch+1} OVERALL LOSS: {losslist[-1]:.3f}\")\n",
    "    print(\"=\"*50)\n",
    "    if epoch % 5 == 4:\n",
    "        path = f\"models40/lstm_{epoch+1}_checkpoint.pth\"\n",
    "        print(f\"Saving model weights at Epoch {epoch+1} ...\")\n",
    "        torch.save(model.state_dict(), path)\n",
    "print(\"Finished Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curve\n",
    "\n",
    "data = open(\"40_losslist_lstm.txt\").read().strip().split(\"\\n\")\n",
    "data = np.array(list(map(lambda x: eval(x.split()[-1]), data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "234//18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "losslist=[]\n",
    "for i in range(13):\n",
    "    start = i*18\n",
    "    epoch_data = np.mean(data[start:start+18])\n",
    "    losslist.append(epoch_data)\n",
    "losslist = np.array(losslist)\n",
    "np.save(\"40_lstm_losslist.npy\", losslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losslist = np.load(\"40_lstm_losslist.npy\")\n",
    "plt.plot(list(np.arange(1,len(losslist)+1)), losslist)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.savefig(\"plots/40_lstm_loss_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: X - torch.Size([24000, 5000]), Y - torch.Size([24000])\n"
     ]
    }
   ],
   "source": [
    "# load test datasets \n",
    "\n",
    "X_test = torch.from_numpy(np.load(\"data/X_test40.npy\")).long()\n",
    "Y_test = torch.from_numpy(np.load(\"data/Y_test40.npy\")).long()\n",
    "print(f\"Training data: X - {X_test.shape}, Y - {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sample(preds, labels, threshold):\n",
    "    matched = float(torch.sum(preds==labels))\n",
    "    if matched/len(preds) >= threshold:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def evaluate(net, X_test, Y_test, verbose=False, threshold=0.6):\n",
    "    net.eval()\n",
    "    N = len(Y_test)//200 # no of music samples in test set\n",
    "    correct = 0\n",
    "    for i in range(N):\n",
    "        start = i*200\n",
    "        with torch.no_grad():\n",
    "            out = net.forward(X_test[start:start+200].to(device))\n",
    "        preds = torch.argmax(out, axis=-1)\n",
    "        labels = Y_test[start:start+200].to(device)\n",
    "        if evaluate_sample(preds, labels, threshold):\n",
    "            correct+=1\n",
    "            if verbose:\n",
    "                print(f\"Sample {i+1}/{N} classified as CORRECT\")\n",
    "        elif verbose:\n",
    "            print(f\"Sample {i+1}/{N} classified as INCORRECT\")    \n",
    "\n",
    "    accuracy = correct/N\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Accuracy of the model on {N} unseen music samples: {(accuracy*100):.2f}%\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load(\"lstm_30_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1/120 classified as CORRECT\n",
      "Sample 2/120 classified as CORRECT\n",
      "Sample 3/120 classified as CORRECT\n",
      "Sample 4/120 classified as CORRECT\n",
      "Sample 5/120 classified as INCORRECT\n",
      "Sample 6/120 classified as CORRECT\n",
      "Sample 7/120 classified as INCORRECT\n",
      "Sample 8/120 classified as INCORRECT\n",
      "Sample 9/120 classified as INCORRECT\n",
      "Sample 10/120 classified as INCORRECT\n",
      "Sample 11/120 classified as CORRECT\n",
      "Sample 12/120 classified as INCORRECT\n",
      "Sample 13/120 classified as CORRECT\n",
      "Sample 14/120 classified as CORRECT\n",
      "Sample 15/120 classified as CORRECT\n",
      "Sample 16/120 classified as CORRECT\n",
      "Sample 17/120 classified as CORRECT\n",
      "Sample 18/120 classified as CORRECT\n",
      "Sample 19/120 classified as CORRECT\n",
      "Sample 20/120 classified as CORRECT\n",
      "Sample 21/120 classified as CORRECT\n",
      "Sample 22/120 classified as CORRECT\n",
      "Sample 23/120 classified as CORRECT\n",
      "Sample 24/120 classified as CORRECT\n",
      "Sample 25/120 classified as CORRECT\n",
      "Sample 26/120 classified as CORRECT\n",
      "Sample 27/120 classified as CORRECT\n",
      "Sample 28/120 classified as INCORRECT\n",
      "Sample 29/120 classified as CORRECT\n",
      "Sample 30/120 classified as INCORRECT\n",
      "Sample 31/120 classified as CORRECT\n",
      "Sample 32/120 classified as CORRECT\n",
      "Sample 33/120 classified as CORRECT\n",
      "Sample 34/120 classified as INCORRECT\n",
      "Sample 35/120 classified as INCORRECT\n",
      "Sample 36/120 classified as INCORRECT\n",
      "Sample 37/120 classified as CORRECT\n",
      "Sample 38/120 classified as CORRECT\n",
      "Sample 39/120 classified as CORRECT\n",
      "Sample 40/120 classified as CORRECT\n",
      "Sample 41/120 classified as INCORRECT\n",
      "Sample 42/120 classified as INCORRECT\n",
      "Sample 43/120 classified as CORRECT\n",
      "Sample 44/120 classified as CORRECT\n",
      "Sample 45/120 classified as CORRECT\n",
      "Sample 46/120 classified as CORRECT\n",
      "Sample 47/120 classified as INCORRECT\n",
      "Sample 48/120 classified as INCORRECT\n",
      "Sample 49/120 classified as INCORRECT\n",
      "Sample 50/120 classified as CORRECT\n",
      "Sample 51/120 classified as CORRECT\n",
      "Sample 52/120 classified as INCORRECT\n",
      "Sample 53/120 classified as INCORRECT\n",
      "Sample 54/120 classified as INCORRECT\n",
      "Sample 55/120 classified as INCORRECT\n",
      "Sample 56/120 classified as INCORRECT\n",
      "Sample 57/120 classified as CORRECT\n",
      "Sample 58/120 classified as CORRECT\n",
      "Sample 59/120 classified as CORRECT\n",
      "Sample 60/120 classified as CORRECT\n",
      "Sample 61/120 classified as CORRECT\n",
      "Sample 62/120 classified as INCORRECT\n",
      "Sample 63/120 classified as CORRECT\n",
      "Sample 64/120 classified as CORRECT\n",
      "Sample 65/120 classified as CORRECT\n",
      "Sample 66/120 classified as CORRECT\n",
      "Sample 67/120 classified as INCORRECT\n",
      "Sample 68/120 classified as INCORRECT\n",
      "Sample 69/120 classified as INCORRECT\n",
      "Sample 70/120 classified as INCORRECT\n",
      "Sample 71/120 classified as CORRECT\n",
      "Sample 72/120 classified as INCORRECT\n",
      "Sample 73/120 classified as CORRECT\n",
      "Sample 74/120 classified as INCORRECT\n",
      "Sample 75/120 classified as CORRECT\n",
      "Sample 76/120 classified as CORRECT\n",
      "Sample 77/120 classified as CORRECT\n",
      "Sample 78/120 classified as CORRECT\n",
      "Sample 79/120 classified as INCORRECT\n",
      "Sample 80/120 classified as CORRECT\n",
      "Sample 81/120 classified as CORRECT\n",
      "Sample 82/120 classified as INCORRECT\n",
      "Sample 83/120 classified as CORRECT\n",
      "Sample 84/120 classified as CORRECT\n",
      "Sample 85/120 classified as CORRECT\n",
      "Sample 86/120 classified as INCORRECT\n",
      "Sample 87/120 classified as INCORRECT\n",
      "Sample 88/120 classified as CORRECT\n",
      "Sample 89/120 classified as CORRECT\n",
      "Sample 90/120 classified as CORRECT\n",
      "Sample 91/120 classified as CORRECT\n",
      "Sample 92/120 classified as CORRECT\n",
      "Sample 93/120 classified as INCORRECT\n",
      "Sample 94/120 classified as INCORRECT\n",
      "Sample 95/120 classified as CORRECT\n",
      "Sample 96/120 classified as CORRECT\n",
      "Sample 97/120 classified as CORRECT\n",
      "Sample 98/120 classified as INCORRECT\n",
      "Sample 99/120 classified as CORRECT\n",
      "Sample 100/120 classified as CORRECT\n",
      "Sample 101/120 classified as CORRECT\n",
      "Sample 102/120 classified as CORRECT\n",
      "Sample 103/120 classified as INCORRECT\n",
      "Sample 104/120 classified as CORRECT\n",
      "Sample 105/120 classified as CORRECT\n",
      "Sample 106/120 classified as CORRECT\n",
      "Sample 107/120 classified as CORRECT\n",
      "Sample 108/120 classified as CORRECT\n",
      "Sample 109/120 classified as INCORRECT\n",
      "Sample 110/120 classified as CORRECT\n",
      "Sample 111/120 classified as CORRECT\n",
      "Sample 112/120 classified as INCORRECT\n",
      "Sample 113/120 classified as INCORRECT\n",
      "Sample 114/120 classified as INCORRECT\n",
      "Sample 115/120 classified as CORRECT\n",
      "Sample 116/120 classified as CORRECT\n",
      "Sample 117/120 classified as INCORRECT\n",
      "Sample 118/120 classified as CORRECT\n",
      "Sample 119/120 classified as CORRECT\n",
      "Sample 120/120 classified as CORRECT\n",
      "==================================================\n",
      "Accuracy of the model on 120 unseen music samples: 65.83%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, X_test, Y_test, threshold=0.6, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Accuracy of the model on 120 unseen music samples: 75.00%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, X_test, Y_test, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_naive(net, X_test, Y_test):\n",
    "    net.eval()\n",
    "    N = len(Y_test)//200 # no of music samples in test set\n",
    "    correct = 0\n",
    "    for i in range(N):\n",
    "        start = i*200\n",
    "        with torch.no_grad():\n",
    "            out = net.forward(X_test[start:start+200].to(device))\n",
    "        preds = torch.argmax(out, axis=-1)\n",
    "        labels = Y_test[start:start+200].to(device)\n",
    "        correct += torch.sum(preds==labels)\n",
    "        \n",
    "    accuracy = float(correct)/len(Y_test)\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Accuracy on given data: {(accuracy*100):.2f}%\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Accuracy on given data: 98.61%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "evaluate_naive(net, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Accuracy on given data: 64.75%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "evaluate_naive(model, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"lstm_losslist.npy\", losslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losslist = np.load(\"lstm_losslist.npy\")\n",
    "plt.plot(list(np.arange(1,len(losslist)+1)), losslist)\n",
    "plt.title(\"Average Loss vs Epoch for LSTM with Attention model\\n\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.savefig(\"plots/lstm_loss_plot.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "deepSRGM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
