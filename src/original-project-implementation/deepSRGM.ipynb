{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6My7bxkUNBA0",
    "outputId": "f6ca1cdb-93d5-4777-b7d7-78f3609f2f22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/teamdaedulus\n"
     ]
    }
   ],
   "source": [
    "# Navigating to the project directory\n",
    "\n",
    "%cd /home/teamdaedulus/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IHqzijIt1KkP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load preprocessed features for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JQ07tRnXNdJe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: X - torch.Size([18000, 5000]), Y - torch.Size([18000])\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.from_numpy(np.load(\"data/X_train.npy\")).long()\n",
    "Y_train = torch.from_numpy(np.load(\"data/Y_train.npy\")).long()\n",
    "print(f\"Training data: X - {X_train.shape}, Y - {Y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set device to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lzndq-fYCGg4",
    "outputId": "3f0b7576-3cf1-4e5a-a3ec-b22b601cf0ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww3lJELsH-9J"
   },
   "source": [
    "### Define required model and layers\n",
    "\n",
    "#### Define Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "EC1B-F1x6O8w"
   },
   "outputs": [],
   "source": [
    "# Code inspired from the following blog post\n",
    "# https://towardsdatascience.com/nlp-learning-series-part-3-attention-cnn-and-what-not-for-text-classification-4313930ed566\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)        \n",
    "        self.supports_masking = True\n",
    "        \n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.kaiming_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim \n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the deepSRGM model for Raga Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "7YiE6NGXnQe4"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_length=5000, embedding_size=128, hidden_size=768,\n",
    "                num_layers=1, num_classes=10, vocab_size=209, drop_prob=0.5):\n",
    "      super(Model, self).__init__()\n",
    "      self.num_layers = num_layers\n",
    "      self.hidden_size = hidden_size\n",
    "      self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "    \n",
    "      self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers,\n",
    "                          dropout=drop_prob, batch_first=True)\n",
    "      self.attention_layer = Attention(hidden_size, input_length)\n",
    "        \n",
    "      self.fc1 = nn.Linear(hidden_size, 384)\n",
    "      self.fc2 = nn.Linear(384, num_classes)\n",
    "    \n",
    "      # self.batchNorm1d = nn.BatchNorm1d(input_length)\n",
    "      self.dropout = nn.Dropout(0.3)      \n",
    "      self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "      batch_size = x.size(0)\n",
    "      embeds = self.embeddings(x)\n",
    "      # print(\"embeds \", embeds.shape)\n",
    "      h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "      c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "      out, _ = self.lstm(embeds, (h0, c0)) #We don't need the hidden tensor\n",
    "      # print(\"lstm \", out.shape)\n",
    "      # out = self.batchNorm1d(out)\n",
    "      out = self.attention_layer(out)\n",
    "      # print(\"attention \", out.shape)\n",
    "     \n",
    "      out = self.relu(self.fc1(out))\n",
    "      # print(\"fc1 \", out.shape)\n",
    "      out = self.dropout(out)\n",
    "      out = self.fc2(out)\n",
    "      # print(\"output \", out.shape)\n",
    "\n",
    "      return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using model to train for Raga Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oAUsE7T33CDm",
    "outputId": "fda32be9-00d8-4496-8627-327f4e3b518e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pipenv/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters for the 10-raga subset classification\n",
    "\n",
    "# input_size = 128\n",
    "# hidden_size = 768\n",
    "# num_layers = 1\n",
    "# num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create PyTorch dataset and dataloader to create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bgwx3dIsOuLD"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 40\n",
    "trainset = TensorDataset(X_train, Y_train)\n",
    "trainloader = DataLoader(trainset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Loss criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vu0go4vnahLJ"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "phtGZ0DrcaRW",
    "outputId": "642774c3-d830-4a36-8e81-88505c47a0c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Batches Done: 15/450 | Loss: 2.299\n",
      "Epoch: 1 | Batches Done: 30/450 | Loss: 2.289\n",
      "Epoch: 1 | Batches Done: 45/450 | Loss: 2.277\n",
      "Epoch: 1 | Batches Done: 60/450 | Loss: 2.233\n",
      "Epoch: 1 | Batches Done: 75/450 | Loss: 2.158\n",
      "Epoch: 1 | Batches Done: 90/450 | Loss: 1.970\n",
      "Epoch: 1 | Batches Done: 105/450 | Loss: 1.794\n",
      "Epoch: 1 | Batches Done: 120/450 | Loss: 1.780\n",
      "Epoch: 1 | Batches Done: 135/450 | Loss: 1.688\n",
      "Epoch: 1 | Batches Done: 150/450 | Loss: 1.634\n",
      "Epoch: 1 | Batches Done: 165/450 | Loss: 1.584\n",
      "Epoch: 1 | Batches Done: 180/450 | Loss: 1.552\n",
      "Epoch: 1 | Batches Done: 195/450 | Loss: 1.525\n",
      "Epoch: 1 | Batches Done: 210/450 | Loss: 1.536\n",
      "Epoch: 1 | Batches Done: 225/450 | Loss: 1.390\n",
      "Epoch: 1 | Batches Done: 240/450 | Loss: 1.397\n",
      "Epoch: 1 | Batches Done: 255/450 | Loss: 1.373\n",
      "Epoch: 1 | Batches Done: 270/450 | Loss: 1.317\n",
      "Epoch: 1 | Batches Done: 285/450 | Loss: 1.334\n",
      "Epoch: 1 | Batches Done: 300/450 | Loss: 1.325\n",
      "Epoch: 1 | Batches Done: 315/450 | Loss: 1.234\n",
      "Epoch: 1 | Batches Done: 330/450 | Loss: 1.256\n",
      "Epoch: 1 | Batches Done: 345/450 | Loss: 1.313\n",
      "Epoch: 1 | Batches Done: 360/450 | Loss: 1.248\n",
      "Epoch: 1 | Batches Done: 375/450 | Loss: 1.170\n",
      "Epoch: 1 | Batches Done: 390/450 | Loss: 1.253\n",
      "Epoch: 1 | Batches Done: 405/450 | Loss: 1.114\n",
      "Epoch: 1 | Batches Done: 420/450 | Loss: 1.195\n",
      "Epoch: 1 | Batches Done: 435/450 | Loss: 1.166\n",
      "Epoch: 1 | Batches Done: 450/450 | Loss: 1.095\n",
      "==================================================\n",
      "EPOCH 1 OVERALL LOSS: 1.550\n",
      "==================================================\n",
      "Epoch: 2 | Batches Done: 15/450 | Loss: 1.075\n",
      "Epoch: 2 | Batches Done: 30/450 | Loss: 1.035\n",
      "Epoch: 2 | Batches Done: 45/450 | Loss: 1.070\n",
      "Epoch: 2 | Batches Done: 60/450 | Loss: 1.074\n",
      "Epoch: 2 | Batches Done: 75/450 | Loss: 1.045\n",
      "Epoch: 2 | Batches Done: 90/450 | Loss: 1.059\n",
      "Epoch: 2 | Batches Done: 105/450 | Loss: 1.000\n",
      "Epoch: 2 | Batches Done: 120/450 | Loss: 0.943\n",
      "Epoch: 2 | Batches Done: 135/450 | Loss: 0.963\n",
      "Epoch: 2 | Batches Done: 150/450 | Loss: 0.955\n",
      "Epoch: 2 | Batches Done: 165/450 | Loss: 1.007\n",
      "Epoch: 2 | Batches Done: 180/450 | Loss: 1.003\n",
      "Epoch: 2 | Batches Done: 195/450 | Loss: 0.990\n",
      "Epoch: 2 | Batches Done: 210/450 | Loss: 0.927\n",
      "Epoch: 2 | Batches Done: 225/450 | Loss: 0.906\n",
      "Epoch: 2 | Batches Done: 240/450 | Loss: 0.973\n",
      "Epoch: 2 | Batches Done: 255/450 | Loss: 0.846\n",
      "Epoch: 2 | Batches Done: 270/450 | Loss: 0.891\n",
      "Epoch: 2 | Batches Done: 285/450 | Loss: 0.845\n",
      "Epoch: 2 | Batches Done: 300/450 | Loss: 0.830\n",
      "Epoch: 2 | Batches Done: 315/450 | Loss: 0.851\n",
      "Epoch: 2 | Batches Done: 330/450 | Loss: 0.862\n",
      "Epoch: 2 | Batches Done: 345/450 | Loss: 0.884\n",
      "Epoch: 2 | Batches Done: 360/450 | Loss: 0.849\n",
      "Epoch: 2 | Batches Done: 375/450 | Loss: 0.847\n",
      "Epoch: 2 | Batches Done: 390/450 | Loss: 0.796\n",
      "Epoch: 2 | Batches Done: 405/450 | Loss: 0.770\n",
      "Epoch: 2 | Batches Done: 420/450 | Loss: 0.779\n",
      "Epoch: 2 | Batches Done: 435/450 | Loss: 0.758\n",
      "Epoch: 2 | Batches Done: 450/450 | Loss: 0.724\n",
      "==================================================\n",
      "EPOCH 2 OVERALL LOSS: 0.919\n",
      "==================================================\n",
      "Epoch: 3 | Batches Done: 15/450 | Loss: 0.732\n",
      "Epoch: 3 | Batches Done: 30/450 | Loss: 0.759\n",
      "Epoch: 3 | Batches Done: 45/450 | Loss: 0.733\n",
      "Epoch: 3 | Batches Done: 60/450 | Loss: 0.690\n",
      "Epoch: 3 | Batches Done: 75/450 | Loss: 0.717\n",
      "Epoch: 3 | Batches Done: 90/450 | Loss: 0.747\n",
      "Epoch: 3 | Batches Done: 105/450 | Loss: 0.737\n",
      "Epoch: 3 | Batches Done: 120/450 | Loss: 0.700\n",
      "Epoch: 3 | Batches Done: 135/450 | Loss: 0.719\n",
      "Epoch: 3 | Batches Done: 150/450 | Loss: 0.669\n",
      "Epoch: 3 | Batches Done: 165/450 | Loss: 0.666\n",
      "Epoch: 3 | Batches Done: 180/450 | Loss: 0.684\n",
      "Epoch: 3 | Batches Done: 195/450 | Loss: 0.709\n",
      "Epoch: 3 | Batches Done: 210/450 | Loss: 0.601\n",
      "Epoch: 3 | Batches Done: 225/450 | Loss: 0.632\n",
      "Epoch: 3 | Batches Done: 240/450 | Loss: 0.723\n",
      "Epoch: 3 | Batches Done: 255/450 | Loss: 0.667\n",
      "Epoch: 3 | Batches Done: 270/450 | Loss: 0.718\n",
      "Epoch: 3 | Batches Done: 285/450 | Loss: 0.669\n",
      "Epoch: 3 | Batches Done: 300/450 | Loss: 0.636\n",
      "Epoch: 3 | Batches Done: 315/450 | Loss: 0.602\n",
      "Epoch: 3 | Batches Done: 330/450 | Loss: 0.634\n",
      "Epoch: 3 | Batches Done: 345/450 | Loss: 0.618\n",
      "Epoch: 3 | Batches Done: 360/450 | Loss: 0.657\n",
      "Epoch: 3 | Batches Done: 375/450 | Loss: 0.642\n",
      "Epoch: 3 | Batches Done: 390/450 | Loss: 0.609\n",
      "Epoch: 3 | Batches Done: 405/450 | Loss: 0.630\n",
      "Epoch: 3 | Batches Done: 420/450 | Loss: 0.627\n",
      "Epoch: 3 | Batches Done: 435/450 | Loss: 0.760\n",
      "Epoch: 3 | Batches Done: 450/450 | Loss: 0.759\n",
      "==================================================\n",
      "EPOCH 3 OVERALL LOSS: 0.681\n",
      "==================================================\n",
      "Epoch: 4 | Batches Done: 15/450 | Loss: 0.697\n",
      "Epoch: 4 | Batches Done: 30/450 | Loss: 0.629\n",
      "Epoch: 4 | Batches Done: 45/450 | Loss: 0.592\n",
      "Epoch: 4 | Batches Done: 60/450 | Loss: 0.613\n",
      "Epoch: 4 | Batches Done: 75/450 | Loss: 0.558\n",
      "Epoch: 4 | Batches Done: 90/450 | Loss: 0.530\n",
      "Epoch: 4 | Batches Done: 105/450 | Loss: 0.586\n",
      "Epoch: 4 | Batches Done: 120/450 | Loss: 0.570\n",
      "Epoch: 4 | Batches Done: 135/450 | Loss: 0.539\n",
      "Epoch: 4 | Batches Done: 150/450 | Loss: 0.555\n",
      "Epoch: 4 | Batches Done: 165/450 | Loss: 0.573\n",
      "Epoch: 4 | Batches Done: 180/450 | Loss: 0.497\n",
      "Epoch: 4 | Batches Done: 195/450 | Loss: 0.484\n",
      "Epoch: 4 | Batches Done: 210/450 | Loss: 0.497\n",
      "Epoch: 4 | Batches Done: 225/450 | Loss: 0.474\n",
      "Epoch: 4 | Batches Done: 240/450 | Loss: 0.529\n",
      "Epoch: 4 | Batches Done: 255/450 | Loss: 0.510\n",
      "Epoch: 4 | Batches Done: 270/450 | Loss: 0.451\n",
      "Epoch: 4 | Batches Done: 285/450 | Loss: 0.444\n",
      "Epoch: 4 | Batches Done: 300/450 | Loss: 0.442\n",
      "Epoch: 4 | Batches Done: 315/450 | Loss: 0.408\n",
      "Epoch: 4 | Batches Done: 330/450 | Loss: 0.473\n",
      "Epoch: 4 | Batches Done: 345/450 | Loss: 0.448\n",
      "Epoch: 4 | Batches Done: 360/450 | Loss: 0.430\n",
      "Epoch: 4 | Batches Done: 375/450 | Loss: 0.425\n",
      "Epoch: 4 | Batches Done: 390/450 | Loss: 0.412\n",
      "Epoch: 4 | Batches Done: 405/450 | Loss: 0.390\n",
      "Epoch: 4 | Batches Done: 420/450 | Loss: 0.432\n",
      "Epoch: 4 | Batches Done: 435/450 | Loss: 0.496\n",
      "Epoch: 4 | Batches Done: 450/450 | Loss: 0.460\n",
      "==================================================\n",
      "EPOCH 4 OVERALL LOSS: 0.505\n",
      "==================================================\n",
      "Epoch: 5 | Batches Done: 15/450 | Loss: 0.397\n",
      "Epoch: 5 | Batches Done: 30/450 | Loss: 0.363\n",
      "Epoch: 5 | Batches Done: 45/450 | Loss: 0.405\n",
      "Epoch: 5 | Batches Done: 60/450 | Loss: 0.360\n",
      "Epoch: 5 | Batches Done: 75/450 | Loss: 0.353\n",
      "Epoch: 5 | Batches Done: 90/450 | Loss: 0.368\n",
      "Epoch: 5 | Batches Done: 105/450 | Loss: 0.396\n",
      "Epoch: 5 | Batches Done: 120/450 | Loss: 0.402\n",
      "Epoch: 5 | Batches Done: 135/450 | Loss: 0.393\n",
      "Epoch: 5 | Batches Done: 150/450 | Loss: 0.326\n",
      "Epoch: 5 | Batches Done: 165/450 | Loss: 0.372\n",
      "Epoch: 5 | Batches Done: 180/450 | Loss: 0.379\n",
      "Epoch: 5 | Batches Done: 195/450 | Loss: 0.415\n",
      "Epoch: 5 | Batches Done: 210/450 | Loss: 0.344\n",
      "Epoch: 5 | Batches Done: 225/450 | Loss: 0.340\n",
      "Epoch: 5 | Batches Done: 240/450 | Loss: 0.369\n",
      "Epoch: 5 | Batches Done: 255/450 | Loss: 0.356\n",
      "Epoch: 5 | Batches Done: 270/450 | Loss: 0.369\n",
      "Epoch: 5 | Batches Done: 285/450 | Loss: 0.394\n",
      "Epoch: 5 | Batches Done: 300/450 | Loss: 0.402\n",
      "Epoch: 5 | Batches Done: 315/450 | Loss: 0.390\n",
      "Epoch: 5 | Batches Done: 330/450 | Loss: 0.335\n",
      "Epoch: 5 | Batches Done: 345/450 | Loss: 0.315\n",
      "Epoch: 5 | Batches Done: 360/450 | Loss: 0.359\n",
      "Epoch: 5 | Batches Done: 375/450 | Loss: 0.323\n",
      "Epoch: 5 | Batches Done: 390/450 | Loss: 0.322\n",
      "Epoch: 5 | Batches Done: 405/450 | Loss: 0.276\n",
      "Epoch: 5 | Batches Done: 420/450 | Loss: 0.311\n",
      "Epoch: 5 | Batches Done: 435/450 | Loss: 0.291\n",
      "Epoch: 5 | Batches Done: 450/450 | Loss: 0.262\n",
      "==================================================\n",
      "EPOCH 5 OVERALL LOSS: 0.356\n",
      "==================================================\n",
      "Saving model weights at Epoch 5 ...\n",
      "Epoch: 6 | Batches Done: 15/450 | Loss: 0.329\n",
      "Epoch: 6 | Batches Done: 30/450 | Loss: 0.299\n",
      "Epoch: 6 | Batches Done: 45/450 | Loss: 0.421\n",
      "Epoch: 6 | Batches Done: 60/450 | Loss: 0.409\n",
      "Epoch: 6 | Batches Done: 75/450 | Loss: 0.403\n",
      "Epoch: 6 | Batches Done: 90/450 | Loss: 0.350\n",
      "Epoch: 6 | Batches Done: 105/450 | Loss: 0.367\n",
      "Epoch: 6 | Batches Done: 120/450 | Loss: 0.375\n",
      "Epoch: 6 | Batches Done: 135/450 | Loss: 0.307\n",
      "Epoch: 6 | Batches Done: 150/450 | Loss: 0.305\n",
      "Epoch: 6 | Batches Done: 165/450 | Loss: 0.345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Batches Done: 180/450 | Loss: 0.303\n",
      "Epoch: 6 | Batches Done: 195/450 | Loss: 0.285\n",
      "Epoch: 6 | Batches Done: 210/450 | Loss: 0.277\n",
      "Epoch: 6 | Batches Done: 225/450 | Loss: 0.269\n",
      "Epoch: 6 | Batches Done: 240/450 | Loss: 0.304\n",
      "Epoch: 6 | Batches Done: 255/450 | Loss: 0.271\n",
      "Epoch: 6 | Batches Done: 270/450 | Loss: 0.317\n",
      "Epoch: 6 | Batches Done: 285/450 | Loss: 0.293\n",
      "Epoch: 6 | Batches Done: 300/450 | Loss: 0.292\n",
      "Epoch: 6 | Batches Done: 315/450 | Loss: 0.239\n",
      "Epoch: 6 | Batches Done: 330/450 | Loss: 0.268\n",
      "Epoch: 6 | Batches Done: 345/450 | Loss: 0.271\n",
      "Epoch: 6 | Batches Done: 360/450 | Loss: 0.300\n",
      "Epoch: 6 | Batches Done: 375/450 | Loss: 0.261\n",
      "Epoch: 6 | Batches Done: 390/450 | Loss: 0.241\n",
      "Epoch: 6 | Batches Done: 405/450 | Loss: 0.250\n",
      "Epoch: 6 | Batches Done: 420/450 | Loss: 0.251\n",
      "Epoch: 6 | Batches Done: 435/450 | Loss: 0.268\n",
      "Epoch: 6 | Batches Done: 450/450 | Loss: 0.279\n",
      "==================================================\n",
      "EPOCH 6 OVERALL LOSS: 0.305\n",
      "==================================================\n",
      "Epoch: 7 | Batches Done: 15/450 | Loss: 0.281\n",
      "Epoch: 7 | Batches Done: 30/450 | Loss: 0.266\n",
      "Epoch: 7 | Batches Done: 45/450 | Loss: 0.273\n",
      "Epoch: 7 | Batches Done: 60/450 | Loss: 0.345\n",
      "Epoch: 7 | Batches Done: 75/450 | Loss: 0.319\n",
      "Epoch: 7 | Batches Done: 90/450 | Loss: 0.308\n",
      "Epoch: 7 | Batches Done: 105/450 | Loss: 0.300\n",
      "Epoch: 7 | Batches Done: 120/450 | Loss: 0.240\n",
      "Epoch: 7 | Batches Done: 135/450 | Loss: 0.247\n",
      "Epoch: 7 | Batches Done: 150/450 | Loss: 0.248\n",
      "Epoch: 7 | Batches Done: 165/450 | Loss: 0.268\n",
      "Epoch: 7 | Batches Done: 180/450 | Loss: 0.247\n",
      "Epoch: 7 | Batches Done: 195/450 | Loss: 0.209\n",
      "Epoch: 7 | Batches Done: 210/450 | Loss: 0.194\n",
      "Epoch: 7 | Batches Done: 225/450 | Loss: 0.265\n",
      "Epoch: 7 | Batches Done: 240/450 | Loss: 0.291\n",
      "Epoch: 7 | Batches Done: 255/450 | Loss: 0.211\n",
      "Epoch: 7 | Batches Done: 270/450 | Loss: 0.372\n",
      "Epoch: 7 | Batches Done: 285/450 | Loss: 0.597\n",
      "Epoch: 7 | Batches Done: 300/450 | Loss: 0.512\n",
      "Epoch: 7 | Batches Done: 315/450 | Loss: 0.410\n",
      "Epoch: 7 | Batches Done: 330/450 | Loss: 0.400\n",
      "Epoch: 7 | Batches Done: 345/450 | Loss: 0.262\n",
      "Epoch: 7 | Batches Done: 360/450 | Loss: 0.248\n",
      "Epoch: 7 | Batches Done: 375/450 | Loss: 0.287\n",
      "Epoch: 7 | Batches Done: 390/450 | Loss: 0.270\n",
      "Epoch: 7 | Batches Done: 405/450 | Loss: 0.262\n",
      "Epoch: 7 | Batches Done: 420/450 | Loss: 0.162\n",
      "Epoch: 7 | Batches Done: 435/450 | Loss: 0.240\n",
      "Epoch: 7 | Batches Done: 450/450 | Loss: 0.235\n",
      "==================================================\n",
      "EPOCH 7 OVERALL LOSS: 0.292\n",
      "==================================================\n",
      "Epoch: 8 | Batches Done: 15/450 | Loss: 0.264\n",
      "Epoch: 8 | Batches Done: 30/450 | Loss: 0.220\n",
      "Epoch: 8 | Batches Done: 45/450 | Loss: 0.220\n",
      "Epoch: 8 | Batches Done: 60/450 | Loss: 0.207\n",
      "Epoch: 8 | Batches Done: 75/450 | Loss: 0.187\n",
      "Epoch: 8 | Batches Done: 90/450 | Loss: 0.241\n",
      "Epoch: 8 | Batches Done: 105/450 | Loss: 0.238\n",
      "Epoch: 8 | Batches Done: 120/450 | Loss: 0.233\n",
      "Epoch: 8 | Batches Done: 135/450 | Loss: 0.262\n",
      "Epoch: 8 | Batches Done: 150/450 | Loss: 0.244\n",
      "Epoch: 8 | Batches Done: 165/450 | Loss: 0.273\n",
      "Epoch: 8 | Batches Done: 180/450 | Loss: 0.215\n",
      "Epoch: 8 | Batches Done: 195/450 | Loss: 0.208\n",
      "Epoch: 8 | Batches Done: 210/450 | Loss: 0.229\n",
      "Epoch: 8 | Batches Done: 225/450 | Loss: 0.290\n",
      "Epoch: 8 | Batches Done: 240/450 | Loss: 0.260\n",
      "Epoch: 8 | Batches Done: 255/450 | Loss: 0.253\n",
      "Epoch: 8 | Batches Done: 270/450 | Loss: 0.266\n",
      "Epoch: 8 | Batches Done: 285/450 | Loss: 0.239\n",
      "Epoch: 8 | Batches Done: 300/450 | Loss: 0.204\n",
      "Epoch: 8 | Batches Done: 315/450 | Loss: 0.265\n",
      "Epoch: 8 | Batches Done: 330/450 | Loss: 0.222\n",
      "Epoch: 8 | Batches Done: 345/450 | Loss: 0.249\n",
      "Epoch: 8 | Batches Done: 360/450 | Loss: 0.272\n",
      "Epoch: 8 | Batches Done: 375/450 | Loss: 0.251\n",
      "Epoch: 8 | Batches Done: 390/450 | Loss: 0.253\n",
      "Epoch: 8 | Batches Done: 405/450 | Loss: 0.245\n",
      "Epoch: 8 | Batches Done: 420/450 | Loss: 0.278\n",
      "Epoch: 8 | Batches Done: 435/450 | Loss: 0.208\n",
      "Epoch: 8 | Batches Done: 450/450 | Loss: 0.162\n",
      "==================================================\n",
      "EPOCH 8 OVERALL LOSS: 0.239\n",
      "==================================================\n",
      "Epoch: 9 | Batches Done: 15/450 | Loss: 0.246\n",
      "Epoch: 9 | Batches Done: 30/450 | Loss: 0.487\n",
      "Epoch: 9 | Batches Done: 45/450 | Loss: 0.296\n",
      "Epoch: 9 | Batches Done: 60/450 | Loss: 0.285\n",
      "Epoch: 9 | Batches Done: 75/450 | Loss: 0.215\n",
      "Epoch: 9 | Batches Done: 90/450 | Loss: 0.234\n",
      "Epoch: 9 | Batches Done: 105/450 | Loss: 0.210\n",
      "Epoch: 9 | Batches Done: 120/450 | Loss: 0.182\n",
      "Epoch: 9 | Batches Done: 135/450 | Loss: 0.226\n",
      "Epoch: 9 | Batches Done: 150/450 | Loss: 0.227\n",
      "Epoch: 9 | Batches Done: 165/450 | Loss: 0.216\n",
      "Epoch: 9 | Batches Done: 180/450 | Loss: 0.201\n",
      "Epoch: 9 | Batches Done: 195/450 | Loss: 0.226\n",
      "Epoch: 9 | Batches Done: 210/450 | Loss: 0.214\n",
      "Epoch: 9 | Batches Done: 225/450 | Loss: 0.239\n",
      "Epoch: 9 | Batches Done: 240/450 | Loss: 0.194\n",
      "Epoch: 9 | Batches Done: 255/450 | Loss: 0.212\n",
      "Epoch: 9 | Batches Done: 270/450 | Loss: 0.208\n",
      "Epoch: 9 | Batches Done: 285/450 | Loss: 0.167\n",
      "Epoch: 9 | Batches Done: 300/450 | Loss: 0.179\n",
      "Epoch: 9 | Batches Done: 315/450 | Loss: 0.212\n",
      "Epoch: 9 | Batches Done: 330/450 | Loss: 0.200\n",
      "Epoch: 9 | Batches Done: 345/450 | Loss: 0.196\n",
      "Epoch: 9 | Batches Done: 360/450 | Loss: 0.158\n",
      "Epoch: 9 | Batches Done: 375/450 | Loss: 0.178\n",
      "Epoch: 9 | Batches Done: 390/450 | Loss: 0.258\n",
      "Epoch: 9 | Batches Done: 405/450 | Loss: 0.148\n",
      "Epoch: 9 | Batches Done: 420/450 | Loss: 0.189\n",
      "Epoch: 9 | Batches Done: 435/450 | Loss: 0.138\n",
      "Epoch: 9 | Batches Done: 450/450 | Loss: 0.201\n",
      "==================================================\n",
      "EPOCH 9 OVERALL LOSS: 0.218\n",
      "==================================================\n",
      "Epoch: 10 | Batches Done: 15/450 | Loss: 0.183\n",
      "Epoch: 10 | Batches Done: 30/450 | Loss: 0.170\n",
      "Epoch: 10 | Batches Done: 45/450 | Loss: 0.146\n",
      "Epoch: 10 | Batches Done: 60/450 | Loss: 0.187\n",
      "Epoch: 10 | Batches Done: 75/450 | Loss: 0.200\n",
      "Epoch: 10 | Batches Done: 90/450 | Loss: 0.200\n",
      "Epoch: 10 | Batches Done: 105/450 | Loss: 0.168\n",
      "Epoch: 10 | Batches Done: 120/450 | Loss: 0.155\n",
      "Epoch: 10 | Batches Done: 135/450 | Loss: 0.182\n",
      "Epoch: 10 | Batches Done: 150/450 | Loss: 0.160\n",
      "Epoch: 10 | Batches Done: 165/450 | Loss: 0.176\n",
      "Epoch: 10 | Batches Done: 180/450 | Loss: 0.176\n",
      "Epoch: 10 | Batches Done: 195/450 | Loss: 0.154\n",
      "Epoch: 10 | Batches Done: 210/450 | Loss: 0.144\n",
      "Epoch: 10 | Batches Done: 225/450 | Loss: 0.156\n",
      "Epoch: 10 | Batches Done: 240/450 | Loss: 0.182\n",
      "Epoch: 10 | Batches Done: 255/450 | Loss: 0.157\n",
      "Epoch: 10 | Batches Done: 270/450 | Loss: 0.142\n",
      "Epoch: 10 | Batches Done: 285/450 | Loss: 0.138\n",
      "Epoch: 10 | Batches Done: 300/450 | Loss: 0.171\n",
      "Epoch: 10 | Batches Done: 315/450 | Loss: 0.157\n",
      "Epoch: 10 | Batches Done: 330/450 | Loss: 0.136\n",
      "Epoch: 10 | Batches Done: 345/450 | Loss: 0.197\n",
      "Epoch: 10 | Batches Done: 360/450 | Loss: 0.176\n",
      "Epoch: 10 | Batches Done: 375/450 | Loss: 0.130\n",
      "Epoch: 10 | Batches Done: 390/450 | Loss: 0.133\n",
      "Epoch: 10 | Batches Done: 405/450 | Loss: 0.207\n",
      "Epoch: 10 | Batches Done: 420/450 | Loss: 0.307\n",
      "Epoch: 10 | Batches Done: 435/450 | Loss: 0.274\n",
      "Epoch: 10 | Batches Done: 450/450 | Loss: 0.242\n",
      "==================================================\n",
      "EPOCH 10 OVERALL LOSS: 0.177\n",
      "==================================================\n",
      "Saving model weights at Epoch 10 ...\n",
      "Epoch: 11 | Batches Done: 15/450 | Loss: 0.200\n",
      "Epoch: 11 | Batches Done: 30/450 | Loss: 0.223\n",
      "Epoch: 11 | Batches Done: 45/450 | Loss: 0.179\n",
      "Epoch: 11 | Batches Done: 60/450 | Loss: 0.176\n",
      "Epoch: 11 | Batches Done: 75/450 | Loss: 0.174\n",
      "Epoch: 11 | Batches Done: 90/450 | Loss: 0.152\n",
      "Epoch: 11 | Batches Done: 105/450 | Loss: 0.144\n",
      "Epoch: 11 | Batches Done: 120/450 | Loss: 0.145\n",
      "Epoch: 11 | Batches Done: 135/450 | Loss: 0.166\n",
      "Epoch: 11 | Batches Done: 150/450 | Loss: 0.284\n",
      "Epoch: 11 | Batches Done: 165/450 | Loss: 0.325\n",
      "Epoch: 11 | Batches Done: 180/450 | Loss: 0.207\n",
      "Epoch: 11 | Batches Done: 195/450 | Loss: 0.226\n",
      "Epoch: 11 | Batches Done: 210/450 | Loss: 0.218\n",
      "Epoch: 11 | Batches Done: 225/450 | Loss: 0.175\n",
      "Epoch: 11 | Batches Done: 240/450 | Loss: 0.160\n",
      "Epoch: 11 | Batches Done: 255/450 | Loss: 0.148\n",
      "Epoch: 11 | Batches Done: 270/450 | Loss: 0.212\n",
      "Epoch: 11 | Batches Done: 285/450 | Loss: 1.029\n",
      "Epoch: 11 | Batches Done: 300/450 | Loss: 0.644\n",
      "Epoch: 11 | Batches Done: 315/450 | Loss: 0.506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Batches Done: 330/450 | Loss: 0.374\n",
      "Epoch: 11 | Batches Done: 345/450 | Loss: 0.357\n",
      "Epoch: 11 | Batches Done: 360/450 | Loss: 0.302\n",
      "Epoch: 11 | Batches Done: 375/450 | Loss: 0.337\n",
      "Epoch: 11 | Batches Done: 390/450 | Loss: 0.274\n",
      "Epoch: 11 | Batches Done: 405/450 | Loss: 0.348\n",
      "Epoch: 11 | Batches Done: 420/450 | Loss: 0.237\n",
      "Epoch: 11 | Batches Done: 435/450 | Loss: 0.313\n",
      "Epoch: 11 | Batches Done: 450/450 | Loss: 0.260\n",
      "==================================================\n",
      "EPOCH 11 OVERALL LOSS: 0.283\n",
      "==================================================\n",
      "Epoch: 12 | Batches Done: 15/450 | Loss: 0.213\n",
      "Epoch: 12 | Batches Done: 30/450 | Loss: 0.243\n",
      "Epoch: 12 | Batches Done: 45/450 | Loss: 0.251\n",
      "Epoch: 12 | Batches Done: 60/450 | Loss: 0.235\n",
      "Epoch: 12 | Batches Done: 75/450 | Loss: 0.253\n",
      "Epoch: 12 | Batches Done: 90/450 | Loss: 0.241\n",
      "Epoch: 12 | Batches Done: 105/450 | Loss: 0.225\n",
      "Epoch: 12 | Batches Done: 120/450 | Loss: 0.203\n",
      "Epoch: 12 | Batches Done: 135/450 | Loss: 0.268\n",
      "Epoch: 12 | Batches Done: 150/450 | Loss: 0.254\n",
      "Epoch: 12 | Batches Done: 165/450 | Loss: 0.259\n",
      "Epoch: 12 | Batches Done: 180/450 | Loss: 0.204\n",
      "Epoch: 12 | Batches Done: 195/450 | Loss: 0.270\n",
      "Epoch: 12 | Batches Done: 210/450 | Loss: 0.231\n",
      "Epoch: 12 | Batches Done: 225/450 | Loss: 0.260\n",
      "Epoch: 12 | Batches Done: 240/450 | Loss: 0.211\n",
      "Epoch: 12 | Batches Done: 255/450 | Loss: 0.219\n",
      "Epoch: 12 | Batches Done: 270/450 | Loss: 0.183\n",
      "Epoch: 12 | Batches Done: 285/450 | Loss: 0.221\n",
      "Epoch: 12 | Batches Done: 300/450 | Loss: 0.167\n",
      "Epoch: 12 | Batches Done: 315/450 | Loss: 0.169\n",
      "Epoch: 12 | Batches Done: 330/450 | Loss: 0.184\n",
      "Epoch: 12 | Batches Done: 345/450 | Loss: 0.185\n",
      "Epoch: 12 | Batches Done: 360/450 | Loss: 0.200\n",
      "Epoch: 12 | Batches Done: 375/450 | Loss: 0.203\n",
      "Epoch: 12 | Batches Done: 390/450 | Loss: 0.205\n",
      "Epoch: 12 | Batches Done: 405/450 | Loss: 0.203\n",
      "Epoch: 12 | Batches Done: 420/450 | Loss: 0.162\n",
      "Epoch: 12 | Batches Done: 435/450 | Loss: 0.192\n",
      "Epoch: 12 | Batches Done: 450/450 | Loss: 0.144\n",
      "==================================================\n",
      "EPOCH 12 OVERALL LOSS: 0.215\n",
      "==================================================\n",
      "Epoch: 13 | Batches Done: 15/450 | Loss: 0.180\n",
      "Epoch: 13 | Batches Done: 30/450 | Loss: 0.187\n",
      "Epoch: 13 | Batches Done: 45/450 | Loss: 0.176\n",
      "Epoch: 13 | Batches Done: 60/450 | Loss: 0.165\n",
      "Epoch: 13 | Batches Done: 75/450 | Loss: 0.165\n",
      "Epoch: 13 | Batches Done: 90/450 | Loss: 0.180\n",
      "Epoch: 13 | Batches Done: 105/450 | Loss: 0.157\n",
      "Epoch: 13 | Batches Done: 120/450 | Loss: 0.188\n",
      "Epoch: 13 | Batches Done: 135/450 | Loss: 0.177\n",
      "Epoch: 13 | Batches Done: 150/450 | Loss: 0.187\n",
      "Epoch: 13 | Batches Done: 165/450 | Loss: 0.117\n",
      "Epoch: 13 | Batches Done: 180/450 | Loss: 0.131\n",
      "Epoch: 13 | Batches Done: 195/450 | Loss: 0.145\n",
      "Epoch: 13 | Batches Done: 210/450 | Loss: 0.134\n",
      "Epoch: 13 | Batches Done: 225/450 | Loss: 0.157\n",
      "Epoch: 13 | Batches Done: 240/450 | Loss: 0.145\n",
      "Epoch: 13 | Batches Done: 255/450 | Loss: 0.180\n",
      "Epoch: 13 | Batches Done: 270/450 | Loss: 0.105\n",
      "Epoch: 13 | Batches Done: 285/450 | Loss: 0.145\n",
      "Epoch: 13 | Batches Done: 300/450 | Loss: 0.198\n",
      "Epoch: 13 | Batches Done: 315/450 | Loss: 0.137\n",
      "Epoch: 13 | Batches Done: 330/450 | Loss: 0.194\n",
      "Epoch: 13 | Batches Done: 345/450 | Loss: 0.148\n",
      "Epoch: 13 | Batches Done: 360/450 | Loss: 0.136\n",
      "Epoch: 13 | Batches Done: 375/450 | Loss: 0.135\n",
      "Epoch: 13 | Batches Done: 390/450 | Loss: 0.152\n",
      "Epoch: 13 | Batches Done: 405/450 | Loss: 0.161\n",
      "Epoch: 13 | Batches Done: 420/450 | Loss: 0.162\n",
      "Epoch: 13 | Batches Done: 435/450 | Loss: 0.141\n",
      "Epoch: 13 | Batches Done: 450/450 | Loss: 0.128\n",
      "==================================================\n",
      "EPOCH 13 OVERALL LOSS: 0.157\n",
      "==================================================\n",
      "Epoch: 14 | Batches Done: 15/450 | Loss: 0.121\n",
      "Epoch: 14 | Batches Done: 30/450 | Loss: 0.132\n",
      "Epoch: 14 | Batches Done: 45/450 | Loss: 0.109\n",
      "Epoch: 14 | Batches Done: 60/450 | Loss: 0.140\n",
      "Epoch: 14 | Batches Done: 75/450 | Loss: 0.111\n",
      "Epoch: 14 | Batches Done: 90/450 | Loss: 0.146\n",
      "Epoch: 14 | Batches Done: 105/450 | Loss: 0.149\n",
      "Epoch: 14 | Batches Done: 120/450 | Loss: 0.254\n",
      "Epoch: 14 | Batches Done: 135/450 | Loss: 0.229\n",
      "Epoch: 14 | Batches Done: 150/450 | Loss: 0.153\n",
      "Epoch: 14 | Batches Done: 165/450 | Loss: 0.149\n",
      "Epoch: 14 | Batches Done: 180/450 | Loss: 0.177\n",
      "Epoch: 14 | Batches Done: 195/450 | Loss: 0.131\n",
      "Epoch: 14 | Batches Done: 210/450 | Loss: 0.105\n",
      "Epoch: 14 | Batches Done: 225/450 | Loss: 0.140\n",
      "Epoch: 14 | Batches Done: 240/450 | Loss: 0.099\n",
      "Epoch: 14 | Batches Done: 255/450 | Loss: 0.134\n",
      "Epoch: 14 | Batches Done: 270/450 | Loss: 0.158\n",
      "Epoch: 14 | Batches Done: 285/450 | Loss: 0.157\n",
      "Epoch: 14 | Batches Done: 300/450 | Loss: 0.128\n",
      "Epoch: 14 | Batches Done: 315/450 | Loss: 0.175\n",
      "Epoch: 14 | Batches Done: 330/450 | Loss: 0.155\n",
      "Epoch: 14 | Batches Done: 345/450 | Loss: 0.127\n",
      "Epoch: 14 | Batches Done: 360/450 | Loss: 0.144\n",
      "Epoch: 14 | Batches Done: 375/450 | Loss: 0.177\n",
      "Epoch: 14 | Batches Done: 390/450 | Loss: 0.169\n",
      "Epoch: 14 | Batches Done: 405/450 | Loss: 0.141\n",
      "Epoch: 14 | Batches Done: 420/450 | Loss: 0.153\n",
      "Epoch: 14 | Batches Done: 435/450 | Loss: 0.148\n",
      "Epoch: 14 | Batches Done: 450/450 | Loss: 0.128\n",
      "==================================================\n",
      "EPOCH 14 OVERALL LOSS: 0.148\n",
      "==================================================\n",
      "Epoch: 15 | Batches Done: 15/450 | Loss: 0.112\n",
      "Epoch: 15 | Batches Done: 30/450 | Loss: 0.132\n",
      "Epoch: 15 | Batches Done: 45/450 | Loss: 0.149\n",
      "Epoch: 15 | Batches Done: 60/450 | Loss: 0.134\n",
      "Epoch: 15 | Batches Done: 75/450 | Loss: 0.144\n",
      "Epoch: 15 | Batches Done: 90/450 | Loss: 0.157\n",
      "Epoch: 15 | Batches Done: 105/450 | Loss: 0.136\n",
      "Epoch: 15 | Batches Done: 120/450 | Loss: 0.114\n",
      "Epoch: 15 | Batches Done: 135/450 | Loss: 0.125\n",
      "Epoch: 15 | Batches Done: 150/450 | Loss: 0.136\n",
      "Epoch: 15 | Batches Done: 165/450 | Loss: 0.140\n",
      "Epoch: 15 | Batches Done: 180/450 | Loss: 0.109\n",
      "Epoch: 15 | Batches Done: 195/450 | Loss: 0.104\n",
      "Epoch: 15 | Batches Done: 210/450 | Loss: 0.107\n",
      "Epoch: 15 | Batches Done: 225/450 | Loss: 0.100\n",
      "Epoch: 15 | Batches Done: 240/450 | Loss: 0.127\n",
      "Epoch: 15 | Batches Done: 255/450 | Loss: 0.111\n",
      "Epoch: 15 | Batches Done: 270/450 | Loss: 0.100\n",
      "Epoch: 15 | Batches Done: 285/450 | Loss: 0.165\n",
      "Epoch: 15 | Batches Done: 300/450 | Loss: 0.126\n",
      "Epoch: 15 | Batches Done: 315/450 | Loss: 0.125\n",
      "Epoch: 15 | Batches Done: 330/450 | Loss: 0.104\n",
      "Epoch: 15 | Batches Done: 345/450 | Loss: 0.136\n",
      "Epoch: 15 | Batches Done: 360/450 | Loss: 0.105\n",
      "Epoch: 15 | Batches Done: 375/450 | Loss: 0.119\n",
      "Epoch: 15 | Batches Done: 390/450 | Loss: 0.137\n",
      "Epoch: 15 | Batches Done: 405/450 | Loss: 0.092\n",
      "Epoch: 15 | Batches Done: 420/450 | Loss: 0.138\n",
      "Epoch: 15 | Batches Done: 435/450 | Loss: 0.104\n",
      "Epoch: 15 | Batches Done: 450/450 | Loss: 0.105\n",
      "==================================================\n",
      "EPOCH 15 OVERALL LOSS: 0.123\n",
      "==================================================\n",
      "Saving model weights at Epoch 15 ...\n",
      "Epoch: 16 | Batches Done: 15/450 | Loss: 0.102\n",
      "Epoch: 16 | Batches Done: 30/450 | Loss: 0.095\n",
      "Epoch: 16 | Batches Done: 45/450 | Loss: 0.126\n",
      "Epoch: 16 | Batches Done: 60/450 | Loss: 0.146\n",
      "Epoch: 16 | Batches Done: 75/450 | Loss: 0.119\n",
      "Epoch: 16 | Batches Done: 90/450 | Loss: 0.107\n",
      "Epoch: 16 | Batches Done: 105/450 | Loss: 0.110\n",
      "Epoch: 16 | Batches Done: 120/450 | Loss: 0.106\n",
      "Epoch: 16 | Batches Done: 135/450 | Loss: 0.096\n",
      "Epoch: 16 | Batches Done: 150/450 | Loss: 0.106\n",
      "Epoch: 16 | Batches Done: 165/450 | Loss: 0.137\n",
      "Epoch: 16 | Batches Done: 180/450 | Loss: 0.120\n",
      "Epoch: 16 | Batches Done: 195/450 | Loss: 0.148\n",
      "Epoch: 16 | Batches Done: 210/450 | Loss: 0.125\n",
      "Epoch: 16 | Batches Done: 225/450 | Loss: 0.107\n",
      "Epoch: 16 | Batches Done: 240/450 | Loss: 0.101\n",
      "Epoch: 16 | Batches Done: 255/450 | Loss: 0.099\n",
      "Epoch: 16 | Batches Done: 270/450 | Loss: 0.116\n",
      "Epoch: 16 | Batches Done: 285/450 | Loss: 0.105\n",
      "Epoch: 16 | Batches Done: 300/450 | Loss: 0.090\n",
      "Epoch: 16 | Batches Done: 315/450 | Loss: 0.116\n",
      "Epoch: 16 | Batches Done: 330/450 | Loss: 0.107\n",
      "Epoch: 16 | Batches Done: 345/450 | Loss: 0.076\n",
      "Epoch: 16 | Batches Done: 360/450 | Loss: 0.127\n",
      "Epoch: 16 | Batches Done: 375/450 | Loss: 0.130\n",
      "Epoch: 16 | Batches Done: 390/450 | Loss: 0.109\n",
      "Epoch: 16 | Batches Done: 405/450 | Loss: 0.104\n",
      "Epoch: 16 | Batches Done: 420/450 | Loss: 0.107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Batches Done: 435/450 | Loss: 0.120\n",
      "Epoch: 16 | Batches Done: 450/450 | Loss: 0.117\n",
      "==================================================\n",
      "EPOCH 16 OVERALL LOSS: 0.112\n",
      "==================================================\n",
      "Epoch: 17 | Batches Done: 15/450 | Loss: 0.107\n",
      "Epoch: 17 | Batches Done: 30/450 | Loss: 0.096\n",
      "Epoch: 17 | Batches Done: 45/450 | Loss: 0.096\n",
      "Epoch: 17 | Batches Done: 60/450 | Loss: 0.072\n",
      "Epoch: 17 | Batches Done: 75/450 | Loss: 0.123\n",
      "Epoch: 17 | Batches Done: 90/450 | Loss: 0.146\n",
      "Epoch: 17 | Batches Done: 105/450 | Loss: 0.206\n",
      "Epoch: 17 | Batches Done: 120/450 | Loss: 0.190\n",
      "Epoch: 17 | Batches Done: 135/450 | Loss: 0.203\n",
      "Epoch: 17 | Batches Done: 150/450 | Loss: 0.380\n",
      "Epoch: 17 | Batches Done: 165/450 | Loss: 0.340\n",
      "Epoch: 17 | Batches Done: 180/450 | Loss: 0.215\n",
      "Epoch: 17 | Batches Done: 195/450 | Loss: 0.158\n",
      "Epoch: 17 | Batches Done: 210/450 | Loss: 0.157\n",
      "Epoch: 17 | Batches Done: 225/450 | Loss: 0.149\n",
      "Epoch: 17 | Batches Done: 240/450 | Loss: 0.167\n",
      "Epoch: 17 | Batches Done: 255/450 | Loss: 0.144\n",
      "Epoch: 17 | Batches Done: 270/450 | Loss: 0.116\n",
      "Epoch: 17 | Batches Done: 285/450 | Loss: 0.129\n",
      "Epoch: 17 | Batches Done: 300/450 | Loss: 0.131\n",
      "Epoch: 17 | Batches Done: 315/450 | Loss: 0.181\n",
      "Epoch: 17 | Batches Done: 330/450 | Loss: 0.125\n",
      "Epoch: 17 | Batches Done: 345/450 | Loss: 0.145\n",
      "Epoch: 17 | Batches Done: 360/450 | Loss: 0.118\n",
      "Epoch: 17 | Batches Done: 375/450 | Loss: 0.151\n",
      "Epoch: 17 | Batches Done: 390/450 | Loss: 0.123\n",
      "Epoch: 17 | Batches Done: 405/450 | Loss: 0.142\n",
      "Epoch: 17 | Batches Done: 420/450 | Loss: 0.122\n",
      "Epoch: 17 | Batches Done: 435/450 | Loss: 0.113\n",
      "Epoch: 17 | Batches Done: 450/450 | Loss: 0.113\n",
      "==================================================\n",
      "EPOCH 17 OVERALL LOSS: 0.155\n",
      "==================================================\n",
      "Epoch: 18 | Batches Done: 15/450 | Loss: 0.109\n",
      "Epoch: 18 | Batches Done: 30/450 | Loss: 0.081\n",
      "Epoch: 18 | Batches Done: 45/450 | Loss: 0.114\n",
      "Epoch: 18 | Batches Done: 60/450 | Loss: 0.079\n",
      "Epoch: 18 | Batches Done: 75/450 | Loss: 0.092\n",
      "Epoch: 18 | Batches Done: 90/450 | Loss: 0.108\n",
      "Epoch: 18 | Batches Done: 105/450 | Loss: 0.101\n",
      "Epoch: 18 | Batches Done: 120/450 | Loss: 0.151\n",
      "Epoch: 18 | Batches Done: 135/450 | Loss: 0.128\n",
      "Epoch: 18 | Batches Done: 150/450 | Loss: 0.161\n",
      "Epoch: 18 | Batches Done: 165/450 | Loss: 0.104\n",
      "Epoch: 18 | Batches Done: 180/450 | Loss: 0.123\n",
      "Epoch: 18 | Batches Done: 195/450 | Loss: 0.106\n",
      "Epoch: 18 | Batches Done: 210/450 | Loss: 0.116\n",
      "Epoch: 18 | Batches Done: 225/450 | Loss: 0.099\n",
      "Epoch: 18 | Batches Done: 240/450 | Loss: 0.087\n",
      "Epoch: 18 | Batches Done: 255/450 | Loss: 0.128\n",
      "Epoch: 18 | Batches Done: 270/450 | Loss: 0.097\n",
      "Epoch: 18 | Batches Done: 285/450 | Loss: 0.097\n",
      "Epoch: 18 | Batches Done: 300/450 | Loss: 0.112\n",
      "Epoch: 18 | Batches Done: 315/450 | Loss: 0.129\n",
      "Epoch: 18 | Batches Done: 330/450 | Loss: 0.095\n",
      "Epoch: 18 | Batches Done: 345/450 | Loss: 0.098\n",
      "Epoch: 18 | Batches Done: 360/450 | Loss: 0.108\n",
      "Epoch: 18 | Batches Done: 375/450 | Loss: 0.087\n",
      "Epoch: 18 | Batches Done: 390/450 | Loss: 0.100\n",
      "Epoch: 18 | Batches Done: 405/450 | Loss: 0.090\n",
      "Epoch: 18 | Batches Done: 420/450 | Loss: 0.094\n",
      "Epoch: 18 | Batches Done: 435/450 | Loss: 0.101\n",
      "Epoch: 18 | Batches Done: 450/450 | Loss: 0.077\n",
      "==================================================\n",
      "EPOCH 18 OVERALL LOSS: 0.106\n",
      "==================================================\n",
      "Epoch: 19 | Batches Done: 15/450 | Loss: 0.075\n",
      "Epoch: 19 | Batches Done: 30/450 | Loss: 0.100\n",
      "Epoch: 19 | Batches Done: 45/450 | Loss: 0.076\n",
      "Epoch: 19 | Batches Done: 60/450 | Loss: 0.103\n",
      "Epoch: 19 | Batches Done: 75/450 | Loss: 0.067\n",
      "Epoch: 19 | Batches Done: 90/450 | Loss: 0.119\n",
      "Epoch: 19 | Batches Done: 105/450 | Loss: 0.082\n",
      "Epoch: 19 | Batches Done: 120/450 | Loss: 0.102\n",
      "Epoch: 19 | Batches Done: 135/450 | Loss: 0.080\n",
      "Epoch: 19 | Batches Done: 150/450 | Loss: 0.082\n",
      "Epoch: 19 | Batches Done: 165/450 | Loss: 0.082\n",
      "Epoch: 19 | Batches Done: 180/450 | Loss: 0.110\n",
      "Epoch: 19 | Batches Done: 195/450 | Loss: 0.118\n",
      "Epoch: 19 | Batches Done: 210/450 | Loss: 0.114\n",
      "Epoch: 19 | Batches Done: 225/450 | Loss: 0.080\n",
      "Epoch: 19 | Batches Done: 240/450 | Loss: 0.088\n",
      "Epoch: 19 | Batches Done: 255/450 | Loss: 0.083\n",
      "Epoch: 19 | Batches Done: 270/450 | Loss: 0.110\n",
      "Epoch: 19 | Batches Done: 285/450 | Loss: 0.091\n",
      "Epoch: 19 | Batches Done: 300/450 | Loss: 0.115\n",
      "Epoch: 19 | Batches Done: 315/450 | Loss: 0.092\n",
      "Epoch: 19 | Batches Done: 330/450 | Loss: 0.095\n",
      "Epoch: 19 | Batches Done: 345/450 | Loss: 0.080\n",
      "Epoch: 19 | Batches Done: 360/450 | Loss: 0.083\n",
      "Epoch: 19 | Batches Done: 375/450 | Loss: 0.073\n",
      "Epoch: 19 | Batches Done: 390/450 | Loss: 0.102\n",
      "Epoch: 19 | Batches Done: 405/450 | Loss: 0.086\n",
      "Epoch: 19 | Batches Done: 420/450 | Loss: 0.112\n",
      "Epoch: 19 | Batches Done: 435/450 | Loss: 0.105\n",
      "Epoch: 19 | Batches Done: 450/450 | Loss: 0.127\n",
      "==================================================\n",
      "EPOCH 19 OVERALL LOSS: 0.094\n",
      "==================================================\n",
      "Epoch: 20 | Batches Done: 15/450 | Loss: 0.092\n",
      "Epoch: 20 | Batches Done: 30/450 | Loss: 0.086\n",
      "Epoch: 20 | Batches Done: 45/450 | Loss: 0.051\n",
      "Epoch: 20 | Batches Done: 60/450 | Loss: 0.088\n",
      "Epoch: 20 | Batches Done: 75/450 | Loss: 0.091\n",
      "Epoch: 20 | Batches Done: 90/450 | Loss: 0.170\n",
      "Epoch: 20 | Batches Done: 105/450 | Loss: 0.115\n",
      "Epoch: 20 | Batches Done: 120/450 | Loss: 0.162\n",
      "Epoch: 20 | Batches Done: 135/450 | Loss: 0.142\n",
      "Epoch: 20 | Batches Done: 150/450 | Loss: 0.129\n",
      "Epoch: 20 | Batches Done: 165/450 | Loss: 0.145\n",
      "Epoch: 20 | Batches Done: 180/450 | Loss: 0.121\n",
      "Epoch: 20 | Batches Done: 195/450 | Loss: 0.081\n",
      "Epoch: 20 | Batches Done: 210/450 | Loss: 0.083\n",
      "Epoch: 20 | Batches Done: 225/450 | Loss: 0.107\n",
      "Epoch: 20 | Batches Done: 240/450 | Loss: 0.095\n",
      "Epoch: 20 | Batches Done: 255/450 | Loss: 0.099\n",
      "Epoch: 20 | Batches Done: 270/450 | Loss: 0.079\n",
      "Epoch: 20 | Batches Done: 285/450 | Loss: 0.084\n",
      "Epoch: 20 | Batches Done: 300/450 | Loss: 0.064\n",
      "Epoch: 20 | Batches Done: 315/450 | Loss: 0.089\n",
      "Epoch: 20 | Batches Done: 330/450 | Loss: 0.120\n",
      "Epoch: 20 | Batches Done: 345/450 | Loss: 0.132\n",
      "Epoch: 20 | Batches Done: 360/450 | Loss: 0.112\n",
      "Epoch: 20 | Batches Done: 375/450 | Loss: 0.114\n",
      "Epoch: 20 | Batches Done: 390/450 | Loss: 0.093\n",
      "Epoch: 20 | Batches Done: 405/450 | Loss: 0.134\n",
      "Epoch: 20 | Batches Done: 420/450 | Loss: 0.138\n",
      "Epoch: 20 | Batches Done: 435/450 | Loss: 0.103\n",
      "Epoch: 20 | Batches Done: 450/450 | Loss: 0.233\n",
      "==================================================\n",
      "EPOCH 20 OVERALL LOSS: 0.112\n",
      "==================================================\n",
      "Saving model weights at Epoch 20 ...\n",
      "Epoch: 21 | Batches Done: 15/450 | Loss: 0.201\n",
      "Epoch: 21 | Batches Done: 30/450 | Loss: 0.164\n",
      "Epoch: 21 | Batches Done: 45/450 | Loss: 1.369\n",
      "Epoch: 21 | Batches Done: 60/450 | Loss: 0.697\n",
      "Epoch: 21 | Batches Done: 75/450 | Loss: 0.414\n",
      "Epoch: 21 | Batches Done: 90/450 | Loss: 0.268\n",
      "Epoch: 21 | Batches Done: 105/450 | Loss: 0.223\n",
      "Epoch: 21 | Batches Done: 120/450 | Loss: 0.190\n",
      "Epoch: 21 | Batches Done: 135/450 | Loss: 0.197\n",
      "Epoch: 21 | Batches Done: 150/450 | Loss: 0.184\n",
      "Epoch: 21 | Batches Done: 165/450 | Loss: 0.194\n",
      "Epoch: 21 | Batches Done: 180/450 | Loss: 0.198\n",
      "Epoch: 21 | Batches Done: 195/450 | Loss: 0.216\n",
      "Epoch: 21 | Batches Done: 210/450 | Loss: 0.188\n",
      "Epoch: 21 | Batches Done: 225/450 | Loss: 0.164\n",
      "Epoch: 21 | Batches Done: 240/450 | Loss: 0.226\n",
      "Epoch: 21 | Batches Done: 255/450 | Loss: 0.251\n",
      "Epoch: 21 | Batches Done: 270/450 | Loss: 0.257\n",
      "Epoch: 21 | Batches Done: 285/450 | Loss: 0.206\n",
      "Epoch: 21 | Batches Done: 300/450 | Loss: 0.200\n",
      "Epoch: 21 | Batches Done: 315/450 | Loss: 0.198\n",
      "Epoch: 21 | Batches Done: 330/450 | Loss: 0.182\n",
      "Epoch: 21 | Batches Done: 345/450 | Loss: 0.169\n",
      "Epoch: 21 | Batches Done: 360/450 | Loss: 0.129\n",
      "Epoch: 21 | Batches Done: 375/450 | Loss: 0.155\n",
      "Epoch: 21 | Batches Done: 390/450 | Loss: 0.188\n",
      "Epoch: 21 | Batches Done: 405/450 | Loss: 0.126\n",
      "Epoch: 21 | Batches Done: 420/450 | Loss: 0.164\n",
      "Epoch: 21 | Batches Done: 435/450 | Loss: 0.094\n",
      "Epoch: 21 | Batches Done: 450/450 | Loss: 0.138\n",
      "==================================================\n",
      "EPOCH 21 OVERALL LOSS: 0.252\n",
      "==================================================\n",
      "Epoch: 22 | Batches Done: 15/450 | Loss: 0.135\n",
      "Epoch: 22 | Batches Done: 30/450 | Loss: 0.120\n",
      "Epoch: 22 | Batches Done: 45/450 | Loss: 0.181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Batches Done: 60/450 | Loss: 0.151\n",
      "Epoch: 22 | Batches Done: 75/450 | Loss: 0.147\n",
      "Epoch: 22 | Batches Done: 90/450 | Loss: 0.135\n",
      "Epoch: 22 | Batches Done: 105/450 | Loss: 0.107\n",
      "Epoch: 22 | Batches Done: 120/450 | Loss: 0.141\n",
      "Epoch: 22 | Batches Done: 135/450 | Loss: 0.124\n",
      "Epoch: 22 | Batches Done: 150/450 | Loss: 0.117\n",
      "Epoch: 22 | Batches Done: 165/450 | Loss: 0.102\n",
      "Epoch: 22 | Batches Done: 180/450 | Loss: 0.129\n",
      "Epoch: 22 | Batches Done: 195/450 | Loss: 0.139\n",
      "Epoch: 22 | Batches Done: 210/450 | Loss: 0.116\n",
      "Epoch: 22 | Batches Done: 225/450 | Loss: 0.111\n",
      "Epoch: 22 | Batches Done: 240/450 | Loss: 0.104\n",
      "Epoch: 22 | Batches Done: 255/450 | Loss: 0.117\n",
      "Epoch: 22 | Batches Done: 270/450 | Loss: 0.142\n",
      "Epoch: 22 | Batches Done: 285/450 | Loss: 0.374\n",
      "Epoch: 22 | Batches Done: 300/450 | Loss: 0.257\n",
      "Epoch: 22 | Batches Done: 315/450 | Loss: 0.224\n",
      "Epoch: 22 | Batches Done: 330/450 | Loss: 0.173\n",
      "Epoch: 22 | Batches Done: 345/450 | Loss: 0.173\n",
      "Epoch: 22 | Batches Done: 360/450 | Loss: 0.153\n",
      "Epoch: 22 | Batches Done: 375/450 | Loss: 0.132\n",
      "Epoch: 22 | Batches Done: 390/450 | Loss: 0.148\n",
      "Epoch: 22 | Batches Done: 405/450 | Loss: 0.123\n",
      "Epoch: 22 | Batches Done: 420/450 | Loss: 0.136\n",
      "Epoch: 22 | Batches Done: 435/450 | Loss: 0.104\n",
      "Epoch: 22 | Batches Done: 450/450 | Loss: 0.155\n",
      "==================================================\n",
      "EPOCH 22 OVERALL LOSS: 0.149\n",
      "==================================================\n",
      "Epoch: 23 | Batches Done: 15/450 | Loss: 0.120\n",
      "Epoch: 23 | Batches Done: 30/450 | Loss: 0.109\n",
      "Epoch: 23 | Batches Done: 45/450 | Loss: 0.144\n",
      "Epoch: 23 | Batches Done: 60/450 | Loss: 0.142\n",
      "Epoch: 23 | Batches Done: 75/450 | Loss: 0.168\n",
      "Epoch: 23 | Batches Done: 90/450 | Loss: 0.102\n",
      "Epoch: 23 | Batches Done: 105/450 | Loss: 0.119\n",
      "Epoch: 23 | Batches Done: 120/450 | Loss: 0.134\n",
      "Epoch: 23 | Batches Done: 135/450 | Loss: 0.128\n",
      "Epoch: 23 | Batches Done: 150/450 | Loss: 0.134\n",
      "Epoch: 23 | Batches Done: 165/450 | Loss: 0.100\n",
      "Epoch: 23 | Batches Done: 180/450 | Loss: 0.136\n",
      "Epoch: 23 | Batches Done: 195/450 | Loss: 0.107\n",
      "Epoch: 23 | Batches Done: 210/450 | Loss: 0.077\n",
      "Epoch: 23 | Batches Done: 225/450 | Loss: 0.109\n",
      "Epoch: 23 | Batches Done: 240/450 | Loss: 0.095\n",
      "Epoch: 23 | Batches Done: 255/450 | Loss: 0.084\n",
      "Epoch: 23 | Batches Done: 270/450 | Loss: 0.120\n",
      "Epoch: 23 | Batches Done: 285/450 | Loss: 0.094\n",
      "Epoch: 23 | Batches Done: 300/450 | Loss: 0.125\n",
      "Epoch: 23 | Batches Done: 315/450 | Loss: 0.078\n",
      "Epoch: 23 | Batches Done: 330/450 | Loss: 0.114\n",
      "Epoch: 23 | Batches Done: 345/450 | Loss: 0.091\n",
      "Epoch: 23 | Batches Done: 360/450 | Loss: 0.105\n",
      "Epoch: 23 | Batches Done: 375/450 | Loss: 0.110\n",
      "Epoch: 23 | Batches Done: 390/450 | Loss: 0.088\n",
      "Epoch: 23 | Batches Done: 405/450 | Loss: 0.091\n",
      "Epoch: 23 | Batches Done: 420/450 | Loss: 0.112\n",
      "Epoch: 23 | Batches Done: 435/450 | Loss: 0.096\n",
      "Epoch: 23 | Batches Done: 450/450 | Loss: 0.097\n",
      "==================================================\n",
      "EPOCH 23 OVERALL LOSS: 0.111\n",
      "==================================================\n",
      "Epoch: 24 | Batches Done: 15/450 | Loss: 0.079\n",
      "Epoch: 24 | Batches Done: 30/450 | Loss: 0.081\n",
      "Epoch: 24 | Batches Done: 45/450 | Loss: 0.086\n",
      "Epoch: 24 | Batches Done: 60/450 | Loss: 0.088\n",
      "Epoch: 24 | Batches Done: 75/450 | Loss: 0.141\n",
      "Epoch: 24 | Batches Done: 90/450 | Loss: 0.089\n",
      "Epoch: 24 | Batches Done: 105/450 | Loss: 0.086\n",
      "Epoch: 24 | Batches Done: 120/450 | Loss: 0.117\n",
      "Epoch: 24 | Batches Done: 135/450 | Loss: 0.095\n",
      "Epoch: 24 | Batches Done: 150/450 | Loss: 0.110\n",
      "Epoch: 24 | Batches Done: 165/450 | Loss: 0.088\n",
      "Epoch: 24 | Batches Done: 180/450 | Loss: 0.106\n",
      "Epoch: 24 | Batches Done: 195/450 | Loss: 0.084\n",
      "Epoch: 24 | Batches Done: 210/450 | Loss: 0.111\n",
      "Epoch: 24 | Batches Done: 225/450 | Loss: 0.072\n",
      "Epoch: 24 | Batches Done: 240/450 | Loss: 0.102\n",
      "Epoch: 24 | Batches Done: 255/450 | Loss: 0.082\n",
      "Epoch: 24 | Batches Done: 270/450 | Loss: 0.076\n",
      "Epoch: 24 | Batches Done: 285/450 | Loss: 0.100\n",
      "Epoch: 24 | Batches Done: 300/450 | Loss: 0.095\n",
      "Epoch: 24 | Batches Done: 315/450 | Loss: 0.082\n",
      "Epoch: 24 | Batches Done: 330/450 | Loss: 0.060\n",
      "Epoch: 24 | Batches Done: 345/450 | Loss: 0.107\n",
      "Epoch: 24 | Batches Done: 360/450 | Loss: 0.097\n",
      "Epoch: 24 | Batches Done: 375/450 | Loss: 0.080\n",
      "Epoch: 24 | Batches Done: 390/450 | Loss: 0.082\n",
      "Epoch: 24 | Batches Done: 405/450 | Loss: 0.087\n",
      "Epoch: 24 | Batches Done: 420/450 | Loss: 0.095\n",
      "Epoch: 24 | Batches Done: 435/450 | Loss: 0.082\n",
      "Epoch: 24 | Batches Done: 450/450 | Loss: 0.059\n",
      "==================================================\n",
      "EPOCH 24 OVERALL LOSS: 0.091\n",
      "==================================================\n",
      "Epoch: 25 | Batches Done: 15/450 | Loss: 0.071\n",
      "Epoch: 25 | Batches Done: 30/450 | Loss: 0.081\n",
      "Epoch: 25 | Batches Done: 45/450 | Loss: 0.086\n",
      "Epoch: 25 | Batches Done: 60/450 | Loss: 0.086\n",
      "Epoch: 25 | Batches Done: 75/450 | Loss: 0.076\n",
      "Epoch: 25 | Batches Done: 90/450 | Loss: 0.065\n",
      "Epoch: 25 | Batches Done: 105/450 | Loss: 0.061\n",
      "Epoch: 25 | Batches Done: 120/450 | Loss: 0.082\n",
      "Epoch: 25 | Batches Done: 135/450 | Loss: 0.109\n",
      "Epoch: 25 | Batches Done: 150/450 | Loss: 0.099\n",
      "Epoch: 25 | Batches Done: 165/450 | Loss: 0.083\n",
      "Epoch: 25 | Batches Done: 180/450 | Loss: 0.097\n",
      "Epoch: 25 | Batches Done: 195/450 | Loss: 0.081\n",
      "Epoch: 25 | Batches Done: 210/450 | Loss: 0.067\n",
      "Epoch: 25 | Batches Done: 225/450 | Loss: 0.094\n",
      "Epoch: 25 | Batches Done: 240/450 | Loss: 0.086\n",
      "Epoch: 25 | Batches Done: 255/450 | Loss: 0.080\n",
      "Epoch: 25 | Batches Done: 270/450 | Loss: 0.073\n",
      "Epoch: 25 | Batches Done: 285/450 | Loss: 0.110\n",
      "Epoch: 25 | Batches Done: 300/450 | Loss: 0.076\n",
      "Epoch: 25 | Batches Done: 315/450 | Loss: 0.111\n",
      "Epoch: 25 | Batches Done: 330/450 | Loss: 0.122\n",
      "Epoch: 25 | Batches Done: 345/450 | Loss: 0.066\n",
      "Epoch: 25 | Batches Done: 360/450 | Loss: 0.130\n",
      "Epoch: 25 | Batches Done: 375/450 | Loss: 0.076\n",
      "Epoch: 25 | Batches Done: 390/450 | Loss: 0.102\n",
      "Epoch: 25 | Batches Done: 405/450 | Loss: 0.055\n",
      "Epoch: 25 | Batches Done: 420/450 | Loss: 0.100\n",
      "Epoch: 25 | Batches Done: 435/450 | Loss: 0.097\n",
      "Epoch: 25 | Batches Done: 450/450 | Loss: 0.105\n",
      "==================================================\n",
      "EPOCH 25 OVERALL LOSS: 0.088\n",
      "==================================================\n",
      "Saving model weights at Epoch 25 ...\n",
      "Epoch: 26 | Batches Done: 15/450 | Loss: 0.083\n",
      "Epoch: 26 | Batches Done: 30/450 | Loss: 0.047\n",
      "Epoch: 26 | Batches Done: 45/450 | Loss: 0.099\n",
      "Epoch: 26 | Batches Done: 60/450 | Loss: 0.115\n",
      "Epoch: 26 | Batches Done: 75/450 | Loss: 0.098\n",
      "Epoch: 26 | Batches Done: 90/450 | Loss: 0.095\n",
      "Epoch: 26 | Batches Done: 105/450 | Loss: 0.071\n",
      "Epoch: 26 | Batches Done: 120/450 | Loss: 0.079\n",
      "Epoch: 26 | Batches Done: 135/450 | Loss: 0.091\n",
      "Epoch: 26 | Batches Done: 150/450 | Loss: 0.108\n",
      "Epoch: 26 | Batches Done: 165/450 | Loss: 0.095\n",
      "Epoch: 26 | Batches Done: 180/450 | Loss: 0.079\n",
      "Epoch: 26 | Batches Done: 195/450 | Loss: 0.077\n",
      "Epoch: 26 | Batches Done: 210/450 | Loss: 0.097\n",
      "Epoch: 26 | Batches Done: 225/450 | Loss: 0.078\n",
      "Epoch: 26 | Batches Done: 240/450 | Loss: 0.086\n",
      "Epoch: 26 | Batches Done: 255/450 | Loss: 0.094\n",
      "Epoch: 26 | Batches Done: 270/450 | Loss: 0.110\n",
      "Epoch: 26 | Batches Done: 285/450 | Loss: 0.079\n",
      "Epoch: 26 | Batches Done: 300/450 | Loss: 0.082\n",
      "Epoch: 26 | Batches Done: 315/450 | Loss: 0.092\n",
      "Epoch: 26 | Batches Done: 330/450 | Loss: 0.060\n",
      "Epoch: 26 | Batches Done: 345/450 | Loss: 0.093\n",
      "Epoch: 26 | Batches Done: 360/450 | Loss: 0.081\n",
      "Epoch: 26 | Batches Done: 375/450 | Loss: 0.065\n",
      "Epoch: 26 | Batches Done: 390/450 | Loss: 0.091\n",
      "Epoch: 26 | Batches Done: 405/450 | Loss: 0.058\n",
      "Epoch: 26 | Batches Done: 420/450 | Loss: 0.095\n",
      "Epoch: 26 | Batches Done: 435/450 | Loss: 0.061\n",
      "Epoch: 26 | Batches Done: 450/450 | Loss: 0.080\n",
      "==================================================\n",
      "EPOCH 26 OVERALL LOSS: 0.085\n",
      "==================================================\n",
      "Epoch: 27 | Batches Done: 15/450 | Loss: 0.070\n",
      "Epoch: 27 | Batches Done: 30/450 | Loss: 0.058\n",
      "Epoch: 27 | Batches Done: 45/450 | Loss: 0.057\n",
      "Epoch: 27 | Batches Done: 60/450 | Loss: 0.065\n",
      "Epoch: 27 | Batches Done: 75/450 | Loss: 0.051\n",
      "Epoch: 27 | Batches Done: 90/450 | Loss: 0.087\n",
      "Epoch: 27 | Batches Done: 105/450 | Loss: 0.088\n",
      "Epoch: 27 | Batches Done: 120/450 | Loss: 0.114\n",
      "Epoch: 27 | Batches Done: 135/450 | Loss: 0.074\n",
      "Epoch: 27 | Batches Done: 150/450 | Loss: 0.095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Batches Done: 165/450 | Loss: 0.084\n",
      "Epoch: 27 | Batches Done: 180/450 | Loss: 0.067\n",
      "Epoch: 27 | Batches Done: 195/450 | Loss: 0.076\n",
      "Epoch: 27 | Batches Done: 210/450 | Loss: 0.098\n",
      "Epoch: 27 | Batches Done: 225/450 | Loss: 0.054\n",
      "Epoch: 27 | Batches Done: 240/450 | Loss: 0.066\n",
      "Epoch: 27 | Batches Done: 255/450 | Loss: 0.070\n",
      "Epoch: 27 | Batches Done: 270/450 | Loss: 0.086\n",
      "Epoch: 27 | Batches Done: 285/450 | Loss: 0.069\n",
      "Epoch: 27 | Batches Done: 300/450 | Loss: 0.064\n",
      "Epoch: 27 | Batches Done: 315/450 | Loss: 0.088\n",
      "Epoch: 27 | Batches Done: 330/450 | Loss: 0.068\n",
      "Epoch: 27 | Batches Done: 345/450 | Loss: 0.068\n",
      "Epoch: 27 | Batches Done: 360/450 | Loss: 0.067\n",
      "Epoch: 27 | Batches Done: 375/450 | Loss: 0.065\n",
      "Epoch: 27 | Batches Done: 390/450 | Loss: 0.052\n",
      "Epoch: 27 | Batches Done: 405/450 | Loss: 0.069\n",
      "Epoch: 27 | Batches Done: 420/450 | Loss: 0.053\n",
      "Epoch: 27 | Batches Done: 435/450 | Loss: 0.068\n",
      "Epoch: 27 | Batches Done: 450/450 | Loss: 0.060\n",
      "==================================================\n",
      "EPOCH 27 OVERALL LOSS: 0.072\n",
      "==================================================\n",
      "Epoch: 28 | Batches Done: 15/450 | Loss: 0.071\n",
      "Epoch: 28 | Batches Done: 30/450 | Loss: 0.086\n",
      "Epoch: 28 | Batches Done: 45/450 | Loss: 0.065\n",
      "Epoch: 28 | Batches Done: 60/450 | Loss: 0.077\n",
      "Epoch: 28 | Batches Done: 75/450 | Loss: 0.070\n",
      "Epoch: 28 | Batches Done: 90/450 | Loss: 0.092\n",
      "Epoch: 28 | Batches Done: 105/450 | Loss: 0.070\n",
      "Epoch: 28 | Batches Done: 120/450 | Loss: 0.073\n",
      "Epoch: 28 | Batches Done: 135/450 | Loss: 0.080\n",
      "Epoch: 28 | Batches Done: 150/450 | Loss: 0.048\n",
      "Epoch: 28 | Batches Done: 165/450 | Loss: 0.067\n",
      "Epoch: 28 | Batches Done: 180/450 | Loss: 0.052\n",
      "Epoch: 28 | Batches Done: 195/450 | Loss: 0.082\n",
      "Epoch: 28 | Batches Done: 210/450 | Loss: 0.059\n",
      "Epoch: 28 | Batches Done: 225/450 | Loss: 0.079\n",
      "Epoch: 28 | Batches Done: 240/450 | Loss: 0.051\n",
      "Epoch: 28 | Batches Done: 255/450 | Loss: 0.066\n",
      "Epoch: 28 | Batches Done: 270/450 | Loss: 0.056\n",
      "Epoch: 28 | Batches Done: 285/450 | Loss: 0.084\n",
      "Epoch: 28 | Batches Done: 300/450 | Loss: 0.096\n",
      "Epoch: 28 | Batches Done: 315/450 | Loss: 0.051\n",
      "Epoch: 28 | Batches Done: 330/450 | Loss: 0.086\n",
      "Epoch: 28 | Batches Done: 345/450 | Loss: 0.066\n",
      "Epoch: 28 | Batches Done: 360/450 | Loss: 0.057\n",
      "Epoch: 28 | Batches Done: 375/450 | Loss: 0.052\n",
      "Epoch: 28 | Batches Done: 390/450 | Loss: 0.066\n",
      "Epoch: 28 | Batches Done: 405/450 | Loss: 0.067\n",
      "Epoch: 28 | Batches Done: 420/450 | Loss: 0.052\n",
      "Epoch: 28 | Batches Done: 435/450 | Loss: 0.058\n",
      "Epoch: 28 | Batches Done: 450/450 | Loss: 0.087\n",
      "==================================================\n",
      "EPOCH 28 OVERALL LOSS: 0.069\n",
      "==================================================\n",
      "Epoch: 29 | Batches Done: 15/450 | Loss: 0.100\n",
      "Epoch: 29 | Batches Done: 30/450 | Loss: 0.106\n",
      "Epoch: 29 | Batches Done: 45/450 | Loss: 0.101\n",
      "Epoch: 29 | Batches Done: 60/450 | Loss: 0.080\n",
      "Epoch: 29 | Batches Done: 75/450 | Loss: 0.043\n",
      "Epoch: 29 | Batches Done: 90/450 | Loss: 0.088\n",
      "Epoch: 29 | Batches Done: 105/450 | Loss: 0.077\n",
      "Epoch: 29 | Batches Done: 120/450 | Loss: 0.095\n",
      "Epoch: 29 | Batches Done: 135/450 | Loss: 0.074\n",
      "Epoch: 29 | Batches Done: 150/450 | Loss: 0.090\n",
      "Epoch: 29 | Batches Done: 165/450 | Loss: 0.080\n",
      "Epoch: 29 | Batches Done: 180/450 | Loss: 0.048\n",
      "Epoch: 29 | Batches Done: 195/450 | Loss: 0.104\n",
      "Epoch: 29 | Batches Done: 210/450 | Loss: 0.080\n",
      "Epoch: 29 | Batches Done: 225/450 | Loss: 0.088\n",
      "Epoch: 29 | Batches Done: 240/450 | Loss: 0.048\n",
      "Epoch: 29 | Batches Done: 255/450 | Loss: 0.083\n",
      "Epoch: 29 | Batches Done: 270/450 | Loss: 0.120\n",
      "Epoch: 29 | Batches Done: 285/450 | Loss: 0.079\n",
      "Epoch: 29 | Batches Done: 300/450 | Loss: 0.088\n",
      "Epoch: 29 | Batches Done: 315/450 | Loss: 0.084\n",
      "Epoch: 29 | Batches Done: 330/450 | Loss: 0.100\n",
      "Epoch: 29 | Batches Done: 345/450 | Loss: 0.113\n",
      "Epoch: 29 | Batches Done: 360/450 | Loss: 0.144\n",
      "Epoch: 29 | Batches Done: 375/450 | Loss: 0.155\n",
      "Epoch: 29 | Batches Done: 390/450 | Loss: 0.100\n",
      "Epoch: 29 | Batches Done: 405/450 | Loss: 0.101\n",
      "Epoch: 29 | Batches Done: 420/450 | Loss: 0.061\n",
      "Epoch: 29 | Batches Done: 435/450 | Loss: 0.059\n",
      "Epoch: 29 | Batches Done: 450/450 | Loss: 0.079\n",
      "==================================================\n",
      "EPOCH 29 OVERALL LOSS: 0.089\n",
      "==================================================\n",
      "Epoch: 30 | Batches Done: 15/450 | Loss: 0.075\n",
      "Epoch: 30 | Batches Done: 30/450 | Loss: 0.066\n",
      "Epoch: 30 | Batches Done: 45/450 | Loss: 0.075\n",
      "Epoch: 30 | Batches Done: 60/450 | Loss: 0.071\n",
      "Epoch: 30 | Batches Done: 75/450 | Loss: 0.091\n",
      "Epoch: 30 | Batches Done: 90/450 | Loss: 0.075\n",
      "Epoch: 30 | Batches Done: 105/450 | Loss: 0.075\n",
      "Epoch: 30 | Batches Done: 120/450 | Loss: 0.073\n",
      "Epoch: 30 | Batches Done: 135/450 | Loss: 0.091\n",
      "Epoch: 30 | Batches Done: 150/450 | Loss: 0.076\n",
      "Epoch: 30 | Batches Done: 165/450 | Loss: 0.090\n",
      "Epoch: 30 | Batches Done: 180/450 | Loss: 0.083\n",
      "Epoch: 30 | Batches Done: 195/450 | Loss: 0.085\n",
      "Epoch: 30 | Batches Done: 210/450 | Loss: 0.060\n",
      "Epoch: 30 | Batches Done: 225/450 | Loss: 0.053\n",
      "Epoch: 30 | Batches Done: 240/450 | Loss: 0.060\n",
      "Epoch: 30 | Batches Done: 255/450 | Loss: 0.042\n",
      "Epoch: 30 | Batches Done: 270/450 | Loss: 0.058\n",
      "Epoch: 30 | Batches Done: 285/450 | Loss: 0.065\n",
      "Epoch: 30 | Batches Done: 300/450 | Loss: 0.093\n",
      "Epoch: 30 | Batches Done: 315/450 | Loss: 0.063\n",
      "Epoch: 30 | Batches Done: 330/450 | Loss: 0.091\n",
      "Epoch: 30 | Batches Done: 345/450 | Loss: 0.101\n",
      "Epoch: 30 | Batches Done: 360/450 | Loss: 0.084\n",
      "Epoch: 30 | Batches Done: 375/450 | Loss: 0.061\n",
      "Epoch: 30 | Batches Done: 390/450 | Loss: 0.069\n",
      "Epoch: 30 | Batches Done: 405/450 | Loss: 0.050\n",
      "Epoch: 30 | Batches Done: 420/450 | Loss: 0.071\n",
      "Epoch: 30 | Batches Done: 435/450 | Loss: 0.070\n",
      "Epoch: 30 | Batches Done: 450/450 | Loss: 0.055\n",
      "==================================================\n",
      "EPOCH 30 OVERALL LOSS: 0.072\n",
      "==================================================\n",
      "Saving model weights at Epoch 30 ...\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "model.train()\n",
    "losslist = []\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    epoch_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 15 == 14:    # print every 15 mini-batches\n",
    "            print(f'Epoch: {epoch+1} | Batches Done: {i+1}/450 | Loss: {(running_loss/15):.3f}')\n",
    "            epoch_loss += running_loss\n",
    "            running_loss = 0.0\n",
    "    losslist.append(epoch_loss/450)\n",
    "    print(\"=\"*50)\n",
    "    print(f\"EPOCH {epoch+1} OVERALL LOSS: {losslist[-1]:.3f}\")\n",
    "    print(\"=\"*50)\n",
    "    if epoch % 5 == 4:\n",
    "        path = f\"models/lstm_{epoch+1}_checkpoint.pth\"\n",
    "        print(f\"Saving model weights at Epoch {epoch+1} ...\")\n",
    "        torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"lstm_losslist_new.npy\", losslist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test datasets\n",
    "\n",
    "X_test = torch.from_numpy(np.load(\"data/X_test.npy\")).long()\n",
    "Y_test = torch.from_numpy(np.load(\"data/Y_test.npy\")).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sample(preds, labels, threshold):\n",
    "    \"\"\"Takes preds and labels on all subsequences of a particular music sample\n",
    "    and returns whether the prediction is correct based on majority voting.\"\"\"\n",
    "    matched = float(torch.sum(preds==labels))\n",
    "    if matched/len(preds) >= threshold:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def evaluate(net, X, Y, verbose=False, threshold=0.6):\n",
    "    \"\"\"Evaluates model performance on the input dataset using majority voting.\"\"\"\n",
    "    net.eval()\n",
    "    N = len(Y)//200 # no of music samples in test set\n",
    "    correct = 0\n",
    "    for i in range(N):\n",
    "        start = i*200\n",
    "        with torch.no_grad():\n",
    "            out = net.forward(X[start:start+200].to(device))\n",
    "        preds = torch.argmax(out, axis=-1)\n",
    "        labels = Y[start:start+200].to(device)\n",
    "        if evaluate_sample(preds, labels, threshold):\n",
    "            correct+=1\n",
    "            if verbose:\n",
    "                print(f\"Sample {i+1}/{N} classified as CORRECT\")\n",
    "        elif verbose:\n",
    "            print(f\"Sample {i+1}/{N} classified as INCORRECT\")    \n",
    "\n",
    "    accuracy = correct/N\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Accuracy of the model on {N} unseen music samples: {(accuracy*100):.2f}%\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "def evaluate_naive(net, X, Y):\n",
    "    \"\"\"Evaluate the model performance independently on the\n",
    "    subsequences in the dataset.\"\"\"\n",
    "    net.eval()\n",
    "    N = len(Y)//200 # no of music samples in test set\n",
    "    correct = 0\n",
    "    for i in range(N):\n",
    "        start = i*200\n",
    "        with torch.no_grad():\n",
    "            out = net.forward(X[start:start+200].to(device))\n",
    "        preds = torch.argmax(out, axis=-1)\n",
    "        labels = Y[start:start+200].to(device)\n",
    "        correct += torch.sum(preds==labels)\n",
    "        \n",
    "    accuracy = float(correct)/len(Y)\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Accuracy on given data: {(accuracy*100):.2f}%\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1/30 classified as INCORRECT\n",
      "Sample 2/30 classified as CORRECT\n",
      "Sample 3/30 classified as CORRECT\n",
      "Sample 4/30 classified as CORRECT\n",
      "Sample 5/30 classified as CORRECT\n",
      "Sample 6/30 classified as CORRECT\n",
      "Sample 7/30 classified as CORRECT\n",
      "Sample 8/30 classified as INCORRECT\n",
      "Sample 9/30 classified as CORRECT\n",
      "Sample 10/30 classified as CORRECT\n",
      "Sample 11/30 classified as CORRECT\n",
      "Sample 12/30 classified as CORRECT\n",
      "Sample 13/30 classified as CORRECT\n",
      "Sample 14/30 classified as CORRECT\n",
      "Sample 15/30 classified as CORRECT\n",
      "Sample 16/30 classified as CORRECT\n",
      "Sample 17/30 classified as CORRECT\n",
      "Sample 18/30 classified as CORRECT\n",
      "Sample 19/30 classified as CORRECT\n",
      "Sample 20/30 classified as CORRECT\n",
      "Sample 21/30 classified as CORRECT\n",
      "Sample 22/30 classified as CORRECT\n",
      "Sample 23/30 classified as CORRECT\n",
      "Sample 24/30 classified as CORRECT\n",
      "Sample 25/30 classified as CORRECT\n",
      "Sample 26/30 classified as CORRECT\n",
      "Sample 27/30 classified as CORRECT\n",
      "Sample 28/30 classified as CORRECT\n",
      "Sample 29/30 classified as CORRECT\n",
      "Sample 30/30 classified as CORRECT\n",
      "==================================================\n",
      "Accuracy of the model on 30 unseen music samples: 93.33%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"models/lstm_30_checkpoint.pth\"))\n",
    "evaluate(model, X_test, Y_test, threshold=0.6, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pipenv/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reloading saved model\n",
    "\n",
    "net = Model().to(device)\n",
    "net.load_state_dict(torch.load(\"models/lstm_30_checkpoint.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sample 7 vote % to correct label: 54.5%\n",
      "Test Sample 17 vote % to correct label: 58.5%\n"
     ]
    }
   ],
   "source": [
    "# see the margin on the samples which were misclassified\n",
    "\n",
    "net.eval()\n",
    "for i in [7,17]:\n",
    "    start = i*200\n",
    "    with torch.no_grad():\n",
    "        out = net.forward(X_test[start:start+200].to(device))\n",
    "    preds = torch.argmax(out, axis=-1)\n",
    "    labels = Y_test[start:start+200].to(device)\n",
    "    print(f\"Test Sample {i+1} vote % to correct label: {100*float(torch.sum(preds==labels))/len(preds)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1/30 classified as CORRECT\n",
      "Sample 2/30 classified as CORRECT\n",
      "Sample 3/30 classified as CORRECT\n",
      "Sample 4/30 classified as CORRECT\n",
      "Sample 5/30 classified as CORRECT\n",
      "Sample 6/30 classified as CORRECT\n",
      "Sample 7/30 classified as CORRECT\n",
      "Sample 8/30 classified as CORRECT\n",
      "Sample 9/30 classified as CORRECT\n",
      "Sample 10/30 classified as CORRECT\n",
      "Sample 11/30 classified as CORRECT\n",
      "Sample 12/30 classified as CORRECT\n",
      "Sample 13/30 classified as CORRECT\n",
      "Sample 14/30 classified as CORRECT\n",
      "Sample 15/30 classified as CORRECT\n",
      "Sample 16/30 classified as CORRECT\n",
      "Sample 17/30 classified as CORRECT\n",
      "Sample 18/30 classified as CORRECT\n",
      "Sample 19/30 classified as CORRECT\n",
      "Sample 20/30 classified as CORRECT\n",
      "Sample 21/30 classified as CORRECT\n",
      "Sample 22/30 classified as CORRECT\n",
      "Sample 23/30 classified as CORRECT\n",
      "Sample 24/30 classified as CORRECT\n",
      "Sample 25/30 classified as CORRECT\n",
      "Sample 26/30 classified as CORRECT\n",
      "Sample 27/30 classified as CORRECT\n",
      "Sample 28/30 classified as CORRECT\n",
      "Sample 29/30 classified as CORRECT\n",
      "Sample 30/30 classified as CORRECT\n",
      "==================================================\n",
      "Accuracy of the model on 30 unseen music samples: 100.00%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# decrease threshold\n",
    "evaluate(model, X_test, Y_test, threshold=0.5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Accuracy on given data: 98.61%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Check simple performance on train_set\n",
    "evaluate_naive(net, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Accuracy on given data: 83.92%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Check simple performance on test_set\n",
    "evaluate_naive(net, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping10 = {0: 'Suraṭi', 1: 'Mukhāri', 2: 'Varāḷi', 3: 'Ānandabhairavi', 4: 'Hussēnī',\n",
    "             5: 'Aṭāna', 6: 'Madhyamāvati', 7: 'Dēvagāndhāri', 8: 'Kāṁbhōji', 9: 'Bēgaḍa'}\n",
    "\n",
    "def predict10(net, X, threshold = 0.6, mapping=mapping10):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        out = net.forward(X.to(device))\n",
    "    preds = torch.argmax(out, axis=-1)\n",
    "    majority, _ = torch.mode(preds)\n",
    "    majority = int(majority)\n",
    "    votes = float(torch.sum(preds==majority))/X.shape[0]\n",
    "    if votes >= threshold:\n",
    "        return f\"Input music sample belongs to the {mapping[majority]} raga\"\n",
    "    return f\"CONFUSED - Closest raga predicted is {mapping[majority]} with {(votes*100):.2f}% votes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CONFUSED - Closest raga predicted is Suraṭi with 58.50% votes'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict10(model, X_test[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.303, 2.294, 2.293, ..., 0.082, 0.072, 0.094])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Plot loss curve\n",
    "\n",
    "# data = open(\"loss_data_for_graph.txt\").read().strip().split(\"\\n\")\n",
    "# data = np.array(list(map(lambda x: eval(x.split()[-1]), data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losslist=[]\n",
    "# for i in range(30):\n",
    "#     start = i*45\n",
    "#     epoch_data = np.mean(data[start:start+45])\n",
    "#     losslist.append(epoch_data)\n",
    "# losslist = np.array(losslist)\n",
    "# np.save(\"lstm_losslist.npy\", losslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losslist = np.load(\"lstm_losslist.npy\")\n",
    "plt.plot(list(np.arange(1,len(losslist)+1)), losslist)\n",
    "plt.title(\"Average Loss vs Epoch for LSTM-based model\\n\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.savefig(\"plots/lstm_loss_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quiet(net, X, Y, threshold=0.6):\n",
    "    \"\"\"Evaluates model performance on the input dataset using majority voting.\"\"\"\n",
    "    net.eval()\n",
    "    N = len(Y)//200 # no of music samples in test set\n",
    "    correct = 0\n",
    "    for i in range(N):\n",
    "        start = i*200\n",
    "        with torch.no_grad():\n",
    "            out = net.forward(X[start:start+200].to(device))\n",
    "        preds = torch.argmax(out, axis=-1)\n",
    "        labels = Y[start:start+200].to(device)\n",
    "        if evaluate_sample(preds, labels, threshold):\n",
    "            correct+=1\n",
    "\n",
    "    return correct/N   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of performance on the test set over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pfs = []\n",
    "for i in list(range(5, 31, 5)):\n",
    "    checkpoint = f\"models/lstm_{i}_checkpoint.pth\"\n",
    "    model.load_state_dict(torch.load(checkpoint))\n",
    "    pfs.append(evaluate_quiet(model, X_test, Y_test))\n",
    "    \n",
    "# Plot the data\n",
    "plt.plot(np.arange(5,31,5), pfs)\n",
    "plt.title(\"Model performance on Testset vs Epoch for LSTM-based model\\n\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig(\"plots/performance_over_epochs_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the Testset: 96.67%\n"
     ]
    }
   ],
   "source": [
    "checkpoint = f\"models/lstm_25_checkpoint.pth\"\n",
    "net.load_state_dict(torch.load(checkpoint))\n",
    "print(f\"Accuracy on the Testset: {(100*evaluate_quiet(net, X_test, Y_test)):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redefine the deepSRGM model for Raga Classification using GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7YiE6NGXnQe4"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_length=5000, embedding_size=128, hidden_size=768,\n",
    "                num_layers=1, num_classes=10, vocab_size=209, drop_prob=0.5):\n",
    "      super(Model, self).__init__()\n",
    "      self.num_layers = num_layers\n",
    "      self.hidden_size = hidden_size\n",
    "      self.fc1 = nn.Linear(hidden_size, 384)\n",
    "      self.fc2 = nn.Linear(384, num_classes)\n",
    "      # self.batchNorm1d = nn.BatchNorm1d(input_length)\n",
    "      self.dropout = nn.Dropout(0.3)\n",
    "      self.gru = nn.GRU(embedding_size, hidden_size, num_layers,\n",
    "                          dropout=drop_prob, batch_first=True)\n",
    "      self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "      self.attention_layer = Attention(hidden_size, input_length)\n",
    "      self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "      batch_size = x.size(0)\n",
    "      embeds = self.embeddings(x)\n",
    "      out, _ = self.gru(embeds) #We don't need the hidden tensor\n",
    "      # out = self.batchNorm1d(out)\n",
    "      out = self.attention_layer(out)\n",
    "     \n",
    "      out = self.relu(self.fc1(out))\n",
    "      out = self.dropout(out)\n",
    "      out = self.fc2(out)\n",
    "\n",
    "      return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new model for Raga Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oAUsE7T33CDm",
    "outputId": "fda32be9-00d8-4496-8627-327f4e3b518e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pipenv/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "# create model instance\n",
    "\n",
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Batches Done: 15/450 | Loss: 2.298\n",
      "Epoch: 1 | Batches Done: 30/450 | Loss: 2.284\n",
      "Epoch: 1 | Batches Done: 45/450 | Loss: 2.272\n",
      "Epoch: 1 | Batches Done: 60/450 | Loss: 2.242\n",
      "Epoch: 1 | Batches Done: 75/450 | Loss: 2.178\n",
      "Epoch: 1 | Batches Done: 90/450 | Loss: 2.026\n",
      "Epoch: 1 | Batches Done: 105/450 | Loss: 1.875\n",
      "Epoch: 1 | Batches Done: 120/450 | Loss: 1.755\n",
      "Epoch: 1 | Batches Done: 135/450 | Loss: 1.684\n",
      "Epoch: 1 | Batches Done: 150/450 | Loss: 1.536\n",
      "Epoch: 1 | Batches Done: 165/450 | Loss: 1.512\n",
      "Epoch: 1 | Batches Done: 180/450 | Loss: 1.500\n",
      "Epoch: 1 | Batches Done: 195/450 | Loss: 1.452\n",
      "Epoch: 1 | Batches Done: 210/450 | Loss: 1.398\n",
      "Epoch: 1 | Batches Done: 225/450 | Loss: 1.366\n",
      "Epoch: 1 | Batches Done: 240/450 | Loss: 1.298\n",
      "Epoch: 1 | Batches Done: 255/450 | Loss: 1.318\n",
      "Epoch: 1 | Batches Done: 270/450 | Loss: 1.299\n",
      "Epoch: 1 | Batches Done: 285/450 | Loss: 1.256\n",
      "Epoch: 1 | Batches Done: 300/450 | Loss: 1.199\n",
      "Epoch: 1 | Batches Done: 315/450 | Loss: 1.203\n",
      "Epoch: 1 | Batches Done: 330/450 | Loss: 1.201\n",
      "Epoch: 1 | Batches Done: 345/450 | Loss: 1.169\n",
      "Epoch: 1 | Batches Done: 360/450 | Loss: 1.071\n",
      "Epoch: 1 | Batches Done: 375/450 | Loss: 1.085\n",
      "Epoch: 1 | Batches Done: 390/450 | Loss: 1.012\n",
      "Epoch: 1 | Batches Done: 405/450 | Loss: 1.105\n",
      "Epoch: 1 | Batches Done: 420/450 | Loss: 1.044\n",
      "Epoch: 1 | Batches Done: 435/450 | Loss: 0.973\n",
      "Epoch: 1 | Batches Done: 450/450 | Loss: 1.071\n",
      "==================================================\n",
      "EPOCH 1 OVERALL LOSS: 1.489\n",
      "==================================================\n",
      "Epoch: 2 | Batches Done: 15/450 | Loss: 0.987\n",
      "Epoch: 2 | Batches Done: 30/450 | Loss: 0.990\n",
      "Epoch: 2 | Batches Done: 45/450 | Loss: 0.990\n",
      "Epoch: 2 | Batches Done: 60/450 | Loss: 0.940\n",
      "Epoch: 2 | Batches Done: 75/450 | Loss: 0.972\n",
      "Epoch: 2 | Batches Done: 90/450 | Loss: 0.917\n",
      "Epoch: 2 | Batches Done: 105/450 | Loss: 0.911\n",
      "Epoch: 2 | Batches Done: 120/450 | Loss: 0.792\n",
      "Epoch: 2 | Batches Done: 135/450 | Loss: 0.794\n",
      "Epoch: 2 | Batches Done: 150/450 | Loss: 0.807\n",
      "Epoch: 2 | Batches Done: 165/450 | Loss: 0.788\n",
      "Epoch: 2 | Batches Done: 180/450 | Loss: 0.776\n",
      "Epoch: 2 | Batches Done: 195/450 | Loss: 0.795\n",
      "Epoch: 2 | Batches Done: 210/450 | Loss: 0.841\n",
      "Epoch: 2 | Batches Done: 225/450 | Loss: 0.848\n",
      "Epoch: 2 | Batches Done: 240/450 | Loss: 0.785\n",
      "Epoch: 2 | Batches Done: 255/450 | Loss: 0.757\n",
      "Epoch: 2 | Batches Done: 270/450 | Loss: 0.740\n",
      "Epoch: 2 | Batches Done: 285/450 | Loss: 0.777\n",
      "Epoch: 2 | Batches Done: 300/450 | Loss: 0.755\n",
      "Epoch: 2 | Batches Done: 315/450 | Loss: 0.732\n",
      "Epoch: 2 | Batches Done: 330/450 | Loss: 0.712\n",
      "Epoch: 2 | Batches Done: 345/450 | Loss: 0.681\n",
      "Epoch: 2 | Batches Done: 360/450 | Loss: 0.699\n",
      "Epoch: 2 | Batches Done: 375/450 | Loss: 0.677\n",
      "Epoch: 2 | Batches Done: 390/450 | Loss: 0.701\n",
      "Epoch: 2 | Batches Done: 405/450 | Loss: 0.700\n",
      "Epoch: 2 | Batches Done: 420/450 | Loss: 0.661\n",
      "Epoch: 2 | Batches Done: 435/450 | Loss: 0.637\n",
      "Epoch: 2 | Batches Done: 450/450 | Loss: 0.663\n",
      "==================================================\n",
      "EPOCH 2 OVERALL LOSS: 0.794\n",
      "==================================================\n",
      "Epoch: 3 | Batches Done: 15/450 | Loss: 0.638\n",
      "Epoch: 3 | Batches Done: 30/450 | Loss: 0.591\n",
      "Epoch: 3 | Batches Done: 45/450 | Loss: 0.643\n",
      "Epoch: 3 | Batches Done: 60/450 | Loss: 0.667\n",
      "Epoch: 3 | Batches Done: 75/450 | Loss: 0.652\n",
      "Epoch: 3 | Batches Done: 90/450 | Loss: 0.634\n",
      "Epoch: 3 | Batches Done: 105/450 | Loss: 0.612\n",
      "Epoch: 3 | Batches Done: 120/450 | Loss: 0.681\n",
      "Epoch: 3 | Batches Done: 135/450 | Loss: 0.586\n",
      "Epoch: 3 | Batches Done: 150/450 | Loss: 0.624\n",
      "Epoch: 3 | Batches Done: 165/450 | Loss: 0.573\n",
      "Epoch: 3 | Batches Done: 180/450 | Loss: 0.591\n",
      "Epoch: 3 | Batches Done: 195/450 | Loss: 0.552\n",
      "Epoch: 3 | Batches Done: 210/450 | Loss: 0.592\n",
      "Epoch: 3 | Batches Done: 225/450 | Loss: 0.507\n",
      "Epoch: 3 | Batches Done: 240/450 | Loss: 0.548\n",
      "Epoch: 3 | Batches Done: 255/450 | Loss: 0.594\n",
      "Epoch: 3 | Batches Done: 270/450 | Loss: 0.564\n",
      "Epoch: 3 | Batches Done: 285/450 | Loss: 0.526\n",
      "Epoch: 3 | Batches Done: 300/450 | Loss: 0.514\n",
      "Epoch: 3 | Batches Done: 315/450 | Loss: 0.554\n",
      "Epoch: 3 | Batches Done: 330/450 | Loss: 0.552\n",
      "Epoch: 3 | Batches Done: 345/450 | Loss: 0.479\n",
      "Epoch: 3 | Batches Done: 360/450 | Loss: 0.460\n",
      "Epoch: 3 | Batches Done: 375/450 | Loss: 0.501\n",
      "Epoch: 3 | Batches Done: 390/450 | Loss: 0.503\n",
      "Epoch: 3 | Batches Done: 405/450 | Loss: 0.441\n",
      "Epoch: 3 | Batches Done: 420/450 | Loss: 0.478\n",
      "Epoch: 3 | Batches Done: 435/450 | Loss: 0.441\n",
      "Epoch: 3 | Batches Done: 450/450 | Loss: 0.569\n",
      "==================================================\n",
      "EPOCH 3 OVERALL LOSS: 0.562\n",
      "==================================================\n",
      "Epoch: 4 | Batches Done: 15/450 | Loss: 0.465\n",
      "Epoch: 4 | Batches Done: 30/450 | Loss: 0.451\n",
      "Epoch: 4 | Batches Done: 45/450 | Loss: 0.471\n",
      "Epoch: 4 | Batches Done: 60/450 | Loss: 0.492\n",
      "Epoch: 4 | Batches Done: 75/450 | Loss: 0.456\n",
      "Epoch: 4 | Batches Done: 90/450 | Loss: 0.443\n",
      "Epoch: 4 | Batches Done: 105/450 | Loss: 0.434\n",
      "Epoch: 4 | Batches Done: 120/450 | Loss: 0.449\n",
      "Epoch: 4 | Batches Done: 135/450 | Loss: 0.456\n",
      "Epoch: 4 | Batches Done: 150/450 | Loss: 0.436\n",
      "Epoch: 4 | Batches Done: 165/450 | Loss: 0.441\n",
      "Epoch: 4 | Batches Done: 180/450 | Loss: 0.442\n",
      "Epoch: 4 | Batches Done: 195/450 | Loss: 0.485\n",
      "Epoch: 4 | Batches Done: 210/450 | Loss: 0.467\n",
      "Epoch: 4 | Batches Done: 225/450 | Loss: 0.361\n",
      "Epoch: 4 | Batches Done: 240/450 | Loss: 0.373\n",
      "Epoch: 4 | Batches Done: 255/450 | Loss: 0.377\n",
      "Epoch: 4 | Batches Done: 270/450 | Loss: 0.394\n",
      "Epoch: 4 | Batches Done: 285/450 | Loss: 0.361\n",
      "Epoch: 4 | Batches Done: 300/450 | Loss: 0.367\n",
      "Epoch: 4 | Batches Done: 315/450 | Loss: 0.395\n",
      "Epoch: 4 | Batches Done: 330/450 | Loss: 0.376\n",
      "Epoch: 4 | Batches Done: 345/450 | Loss: 0.397\n",
      "Epoch: 4 | Batches Done: 360/450 | Loss: 0.364\n",
      "Epoch: 4 | Batches Done: 375/450 | Loss: 0.417\n",
      "Epoch: 4 | Batches Done: 390/450 | Loss: 0.389\n",
      "Epoch: 4 | Batches Done: 405/450 | Loss: 0.418\n",
      "Epoch: 4 | Batches Done: 420/450 | Loss: 0.383\n",
      "Epoch: 4 | Batches Done: 435/450 | Loss: 0.413\n",
      "Epoch: 4 | Batches Done: 450/450 | Loss: 0.397\n",
      "==================================================\n",
      "EPOCH 4 OVERALL LOSS: 0.419\n",
      "==================================================\n",
      "Epoch: 5 | Batches Done: 15/450 | Loss: 0.411\n",
      "Epoch: 5 | Batches Done: 30/450 | Loss: 0.385\n",
      "Epoch: 5 | Batches Done: 45/450 | Loss: 0.363\n",
      "Epoch: 5 | Batches Done: 60/450 | Loss: 0.330\n",
      "Epoch: 5 | Batches Done: 75/450 | Loss: 0.319\n",
      "Epoch: 5 | Batches Done: 90/450 | Loss: 0.343\n",
      "Epoch: 5 | Batches Done: 105/450 | Loss: 0.324\n",
      "Epoch: 5 | Batches Done: 120/450 | Loss: 0.294\n",
      "Epoch: 5 | Batches Done: 135/450 | Loss: 0.333\n",
      "Epoch: 5 | Batches Done: 150/450 | Loss: 0.365\n",
      "Epoch: 5 | Batches Done: 165/450 | Loss: 0.345\n",
      "Epoch: 5 | Batches Done: 180/450 | Loss: 0.295\n",
      "Epoch: 5 | Batches Done: 195/450 | Loss: 0.303\n",
      "Epoch: 5 | Batches Done: 210/450 | Loss: 0.332\n",
      "Epoch: 5 | Batches Done: 225/450 | Loss: 0.315\n",
      "Epoch: 5 | Batches Done: 240/450 | Loss: 0.320\n",
      "Epoch: 5 | Batches Done: 255/450 | Loss: 0.378\n",
      "Epoch: 5 | Batches Done: 270/450 | Loss: 0.282\n",
      "Epoch: 5 | Batches Done: 285/450 | Loss: 0.297\n",
      "Epoch: 5 | Batches Done: 300/450 | Loss: 0.285\n",
      "Epoch: 5 | Batches Done: 315/450 | Loss: 0.337\n",
      "Epoch: 5 | Batches Done: 330/450 | Loss: 0.322\n",
      "Epoch: 5 | Batches Done: 345/450 | Loss: 0.289\n",
      "Epoch: 5 | Batches Done: 360/450 | Loss: 0.324\n",
      "Epoch: 5 | Batches Done: 375/450 | Loss: 0.248\n",
      "Epoch: 5 | Batches Done: 390/450 | Loss: 0.283\n",
      "Epoch: 5 | Batches Done: 405/450 | Loss: 0.295\n",
      "Epoch: 5 | Batches Done: 420/450 | Loss: 0.286\n",
      "Epoch: 5 | Batches Done: 435/450 | Loss: 0.283\n",
      "Epoch: 5 | Batches Done: 450/450 | Loss: 0.308\n",
      "==================================================\n",
      "EPOCH 5 OVERALL LOSS: 0.320\n",
      "==================================================\n",
      "Saving model weights at Epoch 5 ...\n",
      "Epoch: 6 | Batches Done: 15/450 | Loss: 0.264\n",
      "Epoch: 6 | Batches Done: 30/450 | Loss: 0.288\n",
      "Epoch: 6 | Batches Done: 45/450 | Loss: 0.257\n",
      "Epoch: 6 | Batches Done: 60/450 | Loss: 0.236\n",
      "Epoch: 6 | Batches Done: 75/450 | Loss: 0.262\n",
      "Epoch: 6 | Batches Done: 90/450 | Loss: 0.247\n",
      "Epoch: 6 | Batches Done: 105/450 | Loss: 0.210\n",
      "Epoch: 6 | Batches Done: 120/450 | Loss: 0.232\n",
      "Epoch: 6 | Batches Done: 135/450 | Loss: 0.229\n",
      "Epoch: 6 | Batches Done: 150/450 | Loss: 0.246\n",
      "Epoch: 6 | Batches Done: 165/450 | Loss: 0.269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Batches Done: 180/450 | Loss: 0.201\n",
      "Epoch: 6 | Batches Done: 195/450 | Loss: 0.271\n",
      "Epoch: 6 | Batches Done: 210/450 | Loss: 0.266\n",
      "Epoch: 6 | Batches Done: 225/450 | Loss: 0.211\n",
      "Epoch: 6 | Batches Done: 240/450 | Loss: 0.238\n",
      "Epoch: 6 | Batches Done: 255/450 | Loss: 0.226\n",
      "Epoch: 6 | Batches Done: 270/450 | Loss: 0.241\n",
      "Epoch: 6 | Batches Done: 285/450 | Loss: 0.305\n",
      "Epoch: 6 | Batches Done: 300/450 | Loss: 0.292\n",
      "Epoch: 6 | Batches Done: 315/450 | Loss: 0.290\n",
      "Epoch: 6 | Batches Done: 330/450 | Loss: 0.293\n",
      "Epoch: 6 | Batches Done: 345/450 | Loss: 0.236\n",
      "Epoch: 6 | Batches Done: 360/450 | Loss: 0.262\n",
      "Epoch: 6 | Batches Done: 375/450 | Loss: 0.252\n",
      "Epoch: 6 | Batches Done: 390/450 | Loss: 0.251\n",
      "Epoch: 6 | Batches Done: 405/450 | Loss: 0.248\n",
      "Epoch: 6 | Batches Done: 420/450 | Loss: 0.224\n",
      "Epoch: 6 | Batches Done: 435/450 | Loss: 0.255\n",
      "Epoch: 6 | Batches Done: 450/450 | Loss: 0.234\n",
      "==================================================\n",
      "EPOCH 6 OVERALL LOSS: 0.251\n",
      "==================================================\n",
      "Epoch: 7 | Batches Done: 15/450 | Loss: 0.239\n",
      "Epoch: 7 | Batches Done: 30/450 | Loss: 0.233\n",
      "Epoch: 7 | Batches Done: 45/450 | Loss: 0.221\n",
      "Epoch: 7 | Batches Done: 60/450 | Loss: 0.233\n",
      "Epoch: 7 | Batches Done: 75/450 | Loss: 0.200\n",
      "Epoch: 7 | Batches Done: 90/450 | Loss: 0.196\n",
      "Epoch: 7 | Batches Done: 105/450 | Loss: 0.200\n",
      "Epoch: 7 | Batches Done: 120/450 | Loss: 0.209\n",
      "Epoch: 7 | Batches Done: 135/450 | Loss: 0.192\n",
      "Epoch: 7 | Batches Done: 150/450 | Loss: 0.182\n",
      "Epoch: 7 | Batches Done: 165/450 | Loss: 0.222\n",
      "Epoch: 7 | Batches Done: 180/450 | Loss: 0.194\n",
      "Epoch: 7 | Batches Done: 195/450 | Loss: 0.187\n",
      "Epoch: 7 | Batches Done: 210/450 | Loss: 0.209\n",
      "Epoch: 7 | Batches Done: 225/450 | Loss: 0.186\n",
      "Epoch: 7 | Batches Done: 240/450 | Loss: 0.271\n",
      "Epoch: 7 | Batches Done: 255/450 | Loss: 0.183\n",
      "Epoch: 7 | Batches Done: 270/450 | Loss: 0.223\n",
      "Epoch: 7 | Batches Done: 285/450 | Loss: 0.187\n",
      "Epoch: 7 | Batches Done: 300/450 | Loss: 0.177\n",
      "Epoch: 7 | Batches Done: 315/450 | Loss: 0.194\n",
      "Epoch: 7 | Batches Done: 330/450 | Loss: 0.230\n",
      "Epoch: 7 | Batches Done: 345/450 | Loss: 0.186\n",
      "Epoch: 7 | Batches Done: 360/450 | Loss: 0.181\n",
      "Epoch: 7 | Batches Done: 375/450 | Loss: 0.174\n",
      "Epoch: 7 | Batches Done: 390/450 | Loss: 0.189\n",
      "Epoch: 7 | Batches Done: 405/450 | Loss: 0.203\n",
      "Epoch: 7 | Batches Done: 420/450 | Loss: 0.177\n",
      "Epoch: 7 | Batches Done: 435/450 | Loss: 0.171\n",
      "Epoch: 7 | Batches Done: 450/450 | Loss: 0.163\n",
      "==================================================\n",
      "EPOCH 7 OVERALL LOSS: 0.200\n",
      "==================================================\n",
      "Epoch: 8 | Batches Done: 15/450 | Loss: 0.134\n",
      "Epoch: 8 | Batches Done: 30/450 | Loss: 0.198\n",
      "Epoch: 8 | Batches Done: 45/450 | Loss: 0.147\n",
      "Epoch: 8 | Batches Done: 60/450 | Loss: 0.229\n",
      "Epoch: 8 | Batches Done: 75/450 | Loss: 0.147\n",
      "Epoch: 8 | Batches Done: 90/450 | Loss: 0.173\n",
      "Epoch: 8 | Batches Done: 105/450 | Loss: 0.149\n",
      "Epoch: 8 | Batches Done: 120/450 | Loss: 0.152\n",
      "Epoch: 8 | Batches Done: 135/450 | Loss: 0.147\n",
      "Epoch: 8 | Batches Done: 150/450 | Loss: 0.207\n",
      "Epoch: 8 | Batches Done: 165/450 | Loss: 0.177\n",
      "Epoch: 8 | Batches Done: 180/450 | Loss: 0.322\n",
      "Epoch: 8 | Batches Done: 195/450 | Loss: 0.324\n",
      "Epoch: 8 | Batches Done: 210/450 | Loss: 0.240\n",
      "Epoch: 8 | Batches Done: 225/450 | Loss: 0.245\n",
      "Epoch: 8 | Batches Done: 240/450 | Loss: 0.201\n",
      "Epoch: 8 | Batches Done: 255/450 | Loss: 0.158\n",
      "Epoch: 8 | Batches Done: 270/450 | Loss: 0.209\n",
      "Epoch: 8 | Batches Done: 285/450 | Loss: 0.170\n",
      "Epoch: 8 | Batches Done: 300/450 | Loss: 0.151\n",
      "Epoch: 8 | Batches Done: 315/450 | Loss: 0.214\n",
      "Epoch: 8 | Batches Done: 330/450 | Loss: 0.174\n",
      "Epoch: 8 | Batches Done: 345/450 | Loss: 0.178\n",
      "Epoch: 8 | Batches Done: 360/450 | Loss: 0.160\n",
      "Epoch: 8 | Batches Done: 375/450 | Loss: 0.171\n",
      "Epoch: 8 | Batches Done: 390/450 | Loss: 0.162\n",
      "Epoch: 8 | Batches Done: 405/450 | Loss: 0.171\n",
      "Epoch: 8 | Batches Done: 420/450 | Loss: 0.165\n",
      "Epoch: 8 | Batches Done: 435/450 | Loss: 0.160\n",
      "Epoch: 8 | Batches Done: 450/450 | Loss: 0.165\n",
      "==================================================\n",
      "EPOCH 8 OVERALL LOSS: 0.187\n",
      "==================================================\n",
      "Epoch: 9 | Batches Done: 15/450 | Loss: 0.182\n",
      "Epoch: 9 | Batches Done: 30/450 | Loss: 0.145\n",
      "Epoch: 9 | Batches Done: 45/450 | Loss: 0.138\n",
      "Epoch: 9 | Batches Done: 60/450 | Loss: 0.162\n",
      "Epoch: 9 | Batches Done: 75/450 | Loss: 0.127\n",
      "Epoch: 9 | Batches Done: 90/450 | Loss: 0.149\n",
      "Epoch: 9 | Batches Done: 105/450 | Loss: 0.188\n",
      "Epoch: 9 | Batches Done: 120/450 | Loss: 0.174\n",
      "Epoch: 9 | Batches Done: 135/450 | Loss: 0.185\n",
      "Epoch: 9 | Batches Done: 150/450 | Loss: 0.132\n",
      "Epoch: 9 | Batches Done: 165/450 | Loss: 0.159\n",
      "Epoch: 9 | Batches Done: 180/450 | Loss: 0.123\n",
      "Epoch: 9 | Batches Done: 195/450 | Loss: 0.153\n",
      "Epoch: 9 | Batches Done: 210/450 | Loss: 0.154\n",
      "Epoch: 9 | Batches Done: 225/450 | Loss: 0.149\n",
      "Epoch: 9 | Batches Done: 240/450 | Loss: 0.154\n",
      "Epoch: 9 | Batches Done: 255/450 | Loss: 0.181\n",
      "Epoch: 9 | Batches Done: 270/450 | Loss: 0.120\n",
      "Epoch: 9 | Batches Done: 285/450 | Loss: 0.152\n",
      "Epoch: 9 | Batches Done: 300/450 | Loss: 0.157\n",
      "Epoch: 9 | Batches Done: 315/450 | Loss: 0.184\n",
      "Epoch: 9 | Batches Done: 330/450 | Loss: 0.131\n",
      "Epoch: 9 | Batches Done: 345/450 | Loss: 0.158\n",
      "Epoch: 9 | Batches Done: 360/450 | Loss: 0.157\n",
      "Epoch: 9 | Batches Done: 375/450 | Loss: 0.134\n",
      "Epoch: 9 | Batches Done: 390/450 | Loss: 0.119\n",
      "Epoch: 9 | Batches Done: 405/450 | Loss: 0.125\n",
      "Epoch: 9 | Batches Done: 420/450 | Loss: 0.130\n",
      "Epoch: 9 | Batches Done: 435/450 | Loss: 0.159\n",
      "Epoch: 9 | Batches Done: 450/450 | Loss: 0.143\n",
      "==================================================\n",
      "EPOCH 9 OVERALL LOSS: 0.151\n",
      "==================================================\n",
      "Epoch: 10 | Batches Done: 15/450 | Loss: 0.153\n",
      "Epoch: 10 | Batches Done: 30/450 | Loss: 0.121\n",
      "Epoch: 10 | Batches Done: 45/450 | Loss: 0.094\n",
      "Epoch: 10 | Batches Done: 60/450 | Loss: 0.174\n",
      "Epoch: 10 | Batches Done: 75/450 | Loss: 0.096\n",
      "Epoch: 10 | Batches Done: 90/450 | Loss: 0.153\n",
      "Epoch: 10 | Batches Done: 105/450 | Loss: 0.112\n",
      "Epoch: 10 | Batches Done: 120/450 | Loss: 0.131\n",
      "Epoch: 10 | Batches Done: 135/450 | Loss: 0.159\n",
      "Epoch: 10 | Batches Done: 150/450 | Loss: 0.202\n",
      "Epoch: 10 | Batches Done: 165/450 | Loss: 0.151\n",
      "Epoch: 10 | Batches Done: 180/450 | Loss: 0.174\n",
      "Epoch: 10 | Batches Done: 195/450 | Loss: 0.118\n",
      "Epoch: 10 | Batches Done: 210/450 | Loss: 0.138\n",
      "Epoch: 10 | Batches Done: 225/450 | Loss: 0.132\n",
      "Epoch: 10 | Batches Done: 240/450 | Loss: 0.181\n",
      "Epoch: 10 | Batches Done: 255/450 | Loss: 0.152\n",
      "Epoch: 10 | Batches Done: 270/450 | Loss: 0.118\n",
      "Epoch: 10 | Batches Done: 285/450 | Loss: 0.106\n",
      "Epoch: 10 | Batches Done: 300/450 | Loss: 0.122\n",
      "Epoch: 10 | Batches Done: 315/450 | Loss: 0.151\n",
      "Epoch: 10 | Batches Done: 330/450 | Loss: 0.121\n",
      "Epoch: 10 | Batches Done: 345/450 | Loss: 0.136\n",
      "Epoch: 10 | Batches Done: 360/450 | Loss: 0.122\n",
      "Epoch: 10 | Batches Done: 375/450 | Loss: 0.115\n",
      "Epoch: 10 | Batches Done: 390/450 | Loss: 0.133\n",
      "Epoch: 10 | Batches Done: 405/450 | Loss: 0.152\n",
      "Epoch: 10 | Batches Done: 420/450 | Loss: 0.140\n",
      "Epoch: 10 | Batches Done: 435/450 | Loss: 0.163\n",
      "Epoch: 10 | Batches Done: 450/450 | Loss: 0.166\n",
      "==================================================\n",
      "EPOCH 10 OVERALL LOSS: 0.140\n",
      "==================================================\n",
      "Saving model weights at Epoch 10 ...\n",
      "Epoch: 11 | Batches Done: 15/450 | Loss: 0.133\n",
      "Epoch: 11 | Batches Done: 30/450 | Loss: 0.134\n",
      "Epoch: 11 | Batches Done: 45/450 | Loss: 0.109\n",
      "Epoch: 11 | Batches Done: 60/450 | Loss: 0.094\n",
      "Epoch: 11 | Batches Done: 75/450 | Loss: 0.118\n",
      "Epoch: 11 | Batches Done: 90/450 | Loss: 0.101\n",
      "Epoch: 11 | Batches Done: 105/450 | Loss: 0.145\n",
      "Epoch: 11 | Batches Done: 120/450 | Loss: 0.139\n",
      "Epoch: 11 | Batches Done: 135/450 | Loss: 0.102\n",
      "Epoch: 11 | Batches Done: 150/450 | Loss: 0.100\n",
      "Epoch: 11 | Batches Done: 165/450 | Loss: 0.165\n",
      "Epoch: 11 | Batches Done: 180/450 | Loss: 0.107\n",
      "Epoch: 11 | Batches Done: 195/450 | Loss: 0.119\n",
      "Epoch: 11 | Batches Done: 210/450 | Loss: 0.129\n",
      "Epoch: 11 | Batches Done: 225/450 | Loss: 0.126\n",
      "Epoch: 11 | Batches Done: 240/450 | Loss: 0.142\n",
      "Epoch: 11 | Batches Done: 255/450 | Loss: 0.095\n",
      "Epoch: 11 | Batches Done: 270/450 | Loss: 0.105\n",
      "Epoch: 11 | Batches Done: 285/450 | Loss: 0.106\n",
      "Epoch: 11 | Batches Done: 300/450 | Loss: 0.134\n",
      "Epoch: 11 | Batches Done: 315/450 | Loss: 0.144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Batches Done: 330/450 | Loss: 0.101\n",
      "Epoch: 11 | Batches Done: 345/450 | Loss: 0.135\n",
      "Epoch: 11 | Batches Done: 360/450 | Loss: 0.142\n",
      "Epoch: 11 | Batches Done: 375/450 | Loss: 0.139\n",
      "Epoch: 11 | Batches Done: 390/450 | Loss: 0.120\n",
      "Epoch: 11 | Batches Done: 405/450 | Loss: 0.123\n",
      "Epoch: 11 | Batches Done: 420/450 | Loss: 0.093\n",
      "Epoch: 11 | Batches Done: 435/450 | Loss: 0.095\n",
      "Epoch: 11 | Batches Done: 450/450 | Loss: 0.095\n",
      "==================================================\n",
      "EPOCH 11 OVERALL LOSS: 0.120\n",
      "==================================================\n",
      "Epoch: 12 | Batches Done: 15/450 | Loss: 0.094\n",
      "Epoch: 12 | Batches Done: 30/450 | Loss: 0.074\n",
      "Epoch: 12 | Batches Done: 45/450 | Loss: 0.100\n",
      "Epoch: 12 | Batches Done: 60/450 | Loss: 0.122\n",
      "Epoch: 12 | Batches Done: 75/450 | Loss: 0.124\n",
      "Epoch: 12 | Batches Done: 90/450 | Loss: 0.107\n",
      "Epoch: 12 | Batches Done: 105/450 | Loss: 0.116\n",
      "Epoch: 12 | Batches Done: 120/450 | Loss: 0.132\n",
      "Epoch: 12 | Batches Done: 135/450 | Loss: 0.130\n",
      "Epoch: 12 | Batches Done: 150/450 | Loss: 0.127\n",
      "Epoch: 12 | Batches Done: 165/450 | Loss: 0.118\n",
      "Epoch: 12 | Batches Done: 180/450 | Loss: 0.143\n",
      "Epoch: 12 | Batches Done: 195/450 | Loss: 0.102\n",
      "Epoch: 12 | Batches Done: 210/450 | Loss: 0.159\n",
      "Epoch: 12 | Batches Done: 225/450 | Loss: 0.126\n",
      "Epoch: 12 | Batches Done: 240/450 | Loss: 0.100\n",
      "Epoch: 12 | Batches Done: 255/450 | Loss: 0.130\n",
      "Epoch: 12 | Batches Done: 270/450 | Loss: 0.105\n",
      "Epoch: 12 | Batches Done: 285/450 | Loss: 0.100\n",
      "Epoch: 12 | Batches Done: 300/450 | Loss: 0.166\n",
      "Epoch: 12 | Batches Done: 315/450 | Loss: 0.182\n",
      "Epoch: 12 | Batches Done: 330/450 | Loss: 0.152\n",
      "Epoch: 12 | Batches Done: 345/450 | Loss: 0.159\n",
      "Epoch: 12 | Batches Done: 360/450 | Loss: 0.120\n",
      "Epoch: 12 | Batches Done: 375/450 | Loss: 0.115\n",
      "Epoch: 12 | Batches Done: 390/450 | Loss: 0.129\n",
      "Epoch: 12 | Batches Done: 405/450 | Loss: 0.093\n",
      "Epoch: 12 | Batches Done: 420/450 | Loss: 0.090\n",
      "Epoch: 12 | Batches Done: 435/450 | Loss: 0.088\n",
      "Epoch: 12 | Batches Done: 450/450 | Loss: 0.063\n",
      "==================================================\n",
      "EPOCH 12 OVERALL LOSS: 0.119\n",
      "==================================================\n",
      "Epoch: 13 | Batches Done: 15/450 | Loss: 0.094\n",
      "Epoch: 13 | Batches Done: 30/450 | Loss: 0.110\n",
      "Epoch: 13 | Batches Done: 45/450 | Loss: 0.106\n",
      "Epoch: 13 | Batches Done: 60/450 | Loss: 0.131\n",
      "Epoch: 13 | Batches Done: 75/450 | Loss: 0.186\n",
      "Epoch: 13 | Batches Done: 90/450 | Loss: 0.112\n",
      "Epoch: 13 | Batches Done: 105/450 | Loss: 0.122\n",
      "Epoch: 13 | Batches Done: 120/450 | Loss: 0.096\n",
      "Epoch: 13 | Batches Done: 135/450 | Loss: 0.101\n",
      "Epoch: 13 | Batches Done: 150/450 | Loss: 0.080\n",
      "Epoch: 13 | Batches Done: 165/450 | Loss: 0.130\n",
      "Epoch: 13 | Batches Done: 180/450 | Loss: 0.111\n",
      "Epoch: 13 | Batches Done: 195/450 | Loss: 0.164\n",
      "Epoch: 13 | Batches Done: 210/450 | Loss: 0.119\n",
      "Epoch: 13 | Batches Done: 225/450 | Loss: 0.161\n",
      "Epoch: 13 | Batches Done: 240/450 | Loss: 0.105\n",
      "Epoch: 13 | Batches Done: 255/450 | Loss: 0.132\n",
      "Epoch: 13 | Batches Done: 270/450 | Loss: 0.118\n",
      "Epoch: 13 | Batches Done: 285/450 | Loss: 0.081\n",
      "Epoch: 13 | Batches Done: 300/450 | Loss: 0.113\n",
      "Epoch: 13 | Batches Done: 315/450 | Loss: 0.087\n",
      "Epoch: 13 | Batches Done: 330/450 | Loss: 0.080\n",
      "Epoch: 13 | Batches Done: 345/450 | Loss: 0.084\n",
      "Epoch: 13 | Batches Done: 360/450 | Loss: 0.101\n",
      "Epoch: 13 | Batches Done: 375/450 | Loss: 0.106\n",
      "Epoch: 13 | Batches Done: 390/450 | Loss: 0.080\n",
      "Epoch: 13 | Batches Done: 405/450 | Loss: 0.083\n",
      "Epoch: 13 | Batches Done: 420/450 | Loss: 0.109\n",
      "Epoch: 13 | Batches Done: 435/450 | Loss: 0.096\n",
      "Epoch: 13 | Batches Done: 450/450 | Loss: 0.061\n",
      "==================================================\n",
      "EPOCH 13 OVERALL LOSS: 0.109\n",
      "==================================================\n",
      "Epoch: 14 | Batches Done: 15/450 | Loss: 0.079\n",
      "Epoch: 14 | Batches Done: 30/450 | Loss: 0.055\n",
      "Epoch: 14 | Batches Done: 45/450 | Loss: 0.131\n",
      "Epoch: 14 | Batches Done: 60/450 | Loss: 0.111\n",
      "Epoch: 14 | Batches Done: 75/450 | Loss: 0.082\n",
      "Epoch: 14 | Batches Done: 90/450 | Loss: 0.077\n",
      "Epoch: 14 | Batches Done: 105/450 | Loss: 0.103\n",
      "Epoch: 14 | Batches Done: 120/450 | Loss: 0.094\n",
      "Epoch: 14 | Batches Done: 135/450 | Loss: 0.091\n",
      "Epoch: 14 | Batches Done: 150/450 | Loss: 0.094\n",
      "Epoch: 14 | Batches Done: 165/450 | Loss: 0.096\n",
      "Epoch: 14 | Batches Done: 180/450 | Loss: 0.084\n",
      "Epoch: 14 | Batches Done: 195/450 | Loss: 0.084\n",
      "Epoch: 14 | Batches Done: 210/450 | Loss: 0.111\n",
      "Epoch: 14 | Batches Done: 225/450 | Loss: 0.081\n",
      "Epoch: 14 | Batches Done: 240/450 | Loss: 0.080\n",
      "Epoch: 14 | Batches Done: 255/450 | Loss: 0.075\n",
      "Epoch: 14 | Batches Done: 270/450 | Loss: 0.050\n",
      "Epoch: 14 | Batches Done: 285/450 | Loss: 0.069\n",
      "Epoch: 14 | Batches Done: 300/450 | Loss: 0.089\n",
      "Epoch: 14 | Batches Done: 315/450 | Loss: 0.087\n",
      "Epoch: 14 | Batches Done: 330/450 | Loss: 0.076\n",
      "Epoch: 14 | Batches Done: 345/450 | Loss: 0.075\n",
      "Epoch: 14 | Batches Done: 360/450 | Loss: 0.101\n",
      "Epoch: 14 | Batches Done: 375/450 | Loss: 0.106\n",
      "Epoch: 14 | Batches Done: 390/450 | Loss: 0.068\n",
      "Epoch: 14 | Batches Done: 405/450 | Loss: 0.063\n",
      "Epoch: 14 | Batches Done: 420/450 | Loss: 0.089\n",
      "Epoch: 14 | Batches Done: 435/450 | Loss: 0.109\n",
      "Epoch: 14 | Batches Done: 450/450 | Loss: 0.105\n",
      "==================================================\n",
      "EPOCH 14 OVERALL LOSS: 0.087\n",
      "==================================================\n",
      "Epoch: 15 | Batches Done: 15/450 | Loss: 0.098\n",
      "Epoch: 15 | Batches Done: 30/450 | Loss: 0.090\n",
      "Epoch: 15 | Batches Done: 45/450 | Loss: 0.083\n",
      "Epoch: 15 | Batches Done: 60/450 | Loss: 0.097\n",
      "Epoch: 15 | Batches Done: 75/450 | Loss: 0.082\n",
      "Epoch: 15 | Batches Done: 90/450 | Loss: 0.082\n",
      "Epoch: 15 | Batches Done: 105/450 | Loss: 0.078\n",
      "Epoch: 15 | Batches Done: 120/450 | Loss: 0.091\n",
      "Epoch: 15 | Batches Done: 135/450 | Loss: 0.086\n",
      "Epoch: 15 | Batches Done: 150/450 | Loss: 0.062\n",
      "Epoch: 15 | Batches Done: 165/450 | Loss: 0.082\n",
      "Epoch: 15 | Batches Done: 180/450 | Loss: 0.086\n",
      "Epoch: 15 | Batches Done: 195/450 | Loss: 0.081\n",
      "Epoch: 15 | Batches Done: 210/450 | Loss: 0.102\n",
      "Epoch: 15 | Batches Done: 225/450 | Loss: 0.076\n",
      "Epoch: 15 | Batches Done: 240/450 | Loss: 0.088\n",
      "Epoch: 15 | Batches Done: 255/450 | Loss: 0.099\n",
      "Epoch: 15 | Batches Done: 270/450 | Loss: 0.097\n",
      "Epoch: 15 | Batches Done: 285/450 | Loss: 0.064\n",
      "Epoch: 15 | Batches Done: 300/450 | Loss: 0.103\n",
      "Epoch: 15 | Batches Done: 315/450 | Loss: 0.091\n",
      "Epoch: 15 | Batches Done: 330/450 | Loss: 0.101\n",
      "Epoch: 15 | Batches Done: 345/450 | Loss: 0.074\n",
      "Epoch: 15 | Batches Done: 360/450 | Loss: 0.064\n",
      "Epoch: 15 | Batches Done: 375/450 | Loss: 0.074\n",
      "Epoch: 15 | Batches Done: 390/450 | Loss: 0.074\n",
      "Epoch: 15 | Batches Done: 405/450 | Loss: 0.075\n",
      "Epoch: 15 | Batches Done: 420/450 | Loss: 0.076\n",
      "Epoch: 15 | Batches Done: 435/450 | Loss: 0.093\n",
      "Epoch: 15 | Batches Done: 450/450 | Loss: 0.082\n",
      "==================================================\n",
      "EPOCH 15 OVERALL LOSS: 0.084\n",
      "==================================================\n",
      "Saving model weights at Epoch 15 ...\n",
      "Epoch: 16 | Batches Done: 15/450 | Loss: 0.069\n",
      "Epoch: 16 | Batches Done: 30/450 | Loss: 0.070\n",
      "Epoch: 16 | Batches Done: 45/450 | Loss: 0.075\n",
      "Epoch: 16 | Batches Done: 60/450 | Loss: 0.063\n",
      "Epoch: 16 | Batches Done: 75/450 | Loss: 0.089\n",
      "Epoch: 16 | Batches Done: 90/450 | Loss: 0.051\n",
      "Epoch: 16 | Batches Done: 105/450 | Loss: 0.061\n",
      "Epoch: 16 | Batches Done: 120/450 | Loss: 0.051\n",
      "Epoch: 16 | Batches Done: 135/450 | Loss: 0.056\n",
      "Epoch: 16 | Batches Done: 150/450 | Loss: 0.090\n",
      "Epoch: 16 | Batches Done: 165/450 | Loss: 0.105\n",
      "Epoch: 16 | Batches Done: 180/450 | Loss: 0.099\n",
      "Epoch: 16 | Batches Done: 195/450 | Loss: 0.064\n",
      "Epoch: 16 | Batches Done: 210/450 | Loss: 0.060\n",
      "Epoch: 16 | Batches Done: 225/450 | Loss: 0.078\n",
      "Epoch: 16 | Batches Done: 240/450 | Loss: 0.046\n",
      "Epoch: 16 | Batches Done: 255/450 | Loss: 0.090\n",
      "Epoch: 16 | Batches Done: 270/450 | Loss: 0.100\n",
      "Epoch: 16 | Batches Done: 285/450 | Loss: 0.074\n",
      "Epoch: 16 | Batches Done: 300/450 | Loss: 0.086\n",
      "Epoch: 16 | Batches Done: 315/450 | Loss: 0.085\n",
      "Epoch: 16 | Batches Done: 330/450 | Loss: 0.088\n",
      "Epoch: 16 | Batches Done: 345/450 | Loss: 0.067\n",
      "Epoch: 16 | Batches Done: 360/450 | Loss: 0.088\n",
      "Epoch: 16 | Batches Done: 375/450 | Loss: 0.067\n",
      "Epoch: 16 | Batches Done: 390/450 | Loss: 0.078\n",
      "Epoch: 16 | Batches Done: 405/450 | Loss: 0.108\n",
      "Epoch: 16 | Batches Done: 420/450 | Loss: 0.102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Batches Done: 435/450 | Loss: 0.074\n",
      "Epoch: 16 | Batches Done: 450/450 | Loss: 0.086\n",
      "==================================================\n",
      "EPOCH 16 OVERALL LOSS: 0.077\n",
      "==================================================\n",
      "Epoch: 17 | Batches Done: 15/450 | Loss: 0.073\n",
      "Epoch: 17 | Batches Done: 30/450 | Loss: 0.068\n",
      "Epoch: 17 | Batches Done: 45/450 | Loss: 0.059\n",
      "Epoch: 17 | Batches Done: 60/450 | Loss: 0.061\n",
      "Epoch: 17 | Batches Done: 75/450 | Loss: 0.086\n",
      "Epoch: 17 | Batches Done: 90/450 | Loss: 0.082\n",
      "Epoch: 17 | Batches Done: 105/450 | Loss: 0.066\n",
      "Epoch: 17 | Batches Done: 120/450 | Loss: 0.059\n",
      "Epoch: 17 | Batches Done: 135/450 | Loss: 0.059\n",
      "Epoch: 17 | Batches Done: 150/450 | Loss: 0.056\n",
      "Epoch: 17 | Batches Done: 165/450 | Loss: 0.076\n",
      "Epoch: 17 | Batches Done: 180/450 | Loss: 0.063\n",
      "Epoch: 17 | Batches Done: 195/450 | Loss: 0.071\n",
      "Epoch: 17 | Batches Done: 210/450 | Loss: 0.060\n",
      "Epoch: 17 | Batches Done: 225/450 | Loss: 0.070\n",
      "Epoch: 17 | Batches Done: 240/450 | Loss: 0.072\n",
      "Epoch: 17 | Batches Done: 255/450 | Loss: 0.206\n",
      "Epoch: 17 | Batches Done: 270/450 | Loss: 0.144\n",
      "Epoch: 17 | Batches Done: 285/450 | Loss: 0.125\n",
      "Epoch: 17 | Batches Done: 300/450 | Loss: 0.095\n",
      "Epoch: 17 | Batches Done: 315/450 | Loss: 0.110\n",
      "Epoch: 17 | Batches Done: 330/450 | Loss: 0.067\n",
      "Epoch: 17 | Batches Done: 345/450 | Loss: 0.072\n",
      "Epoch: 17 | Batches Done: 360/450 | Loss: 0.091\n",
      "Epoch: 17 | Batches Done: 375/450 | Loss: 0.092\n",
      "Epoch: 17 | Batches Done: 390/450 | Loss: 0.147\n",
      "Epoch: 17 | Batches Done: 405/450 | Loss: 0.132\n",
      "Epoch: 17 | Batches Done: 420/450 | Loss: 0.104\n",
      "Epoch: 17 | Batches Done: 435/450 | Loss: 0.077\n",
      "Epoch: 17 | Batches Done: 450/450 | Loss: 0.070\n",
      "==================================================\n",
      "EPOCH 17 OVERALL LOSS: 0.087\n",
      "==================================================\n",
      "Epoch: 18 | Batches Done: 15/450 | Loss: 0.070\n",
      "Epoch: 18 | Batches Done: 30/450 | Loss: 0.080\n",
      "Epoch: 18 | Batches Done: 45/450 | Loss: 0.061\n",
      "Epoch: 18 | Batches Done: 60/450 | Loss: 0.064\n",
      "Epoch: 18 | Batches Done: 75/450 | Loss: 0.064\n",
      "Epoch: 18 | Batches Done: 90/450 | Loss: 0.068\n",
      "Epoch: 18 | Batches Done: 105/450 | Loss: 0.074\n",
      "Epoch: 18 | Batches Done: 120/450 | Loss: 0.065\n",
      "Epoch: 18 | Batches Done: 135/450 | Loss: 0.054\n",
      "Epoch: 18 | Batches Done: 150/450 | Loss: 0.043\n",
      "Epoch: 18 | Batches Done: 165/450 | Loss: 0.068\n",
      "Epoch: 18 | Batches Done: 180/450 | Loss: 0.084\n",
      "Epoch: 18 | Batches Done: 195/450 | Loss: 0.071\n",
      "Epoch: 18 | Batches Done: 210/450 | Loss: 0.059\n",
      "Epoch: 18 | Batches Done: 225/450 | Loss: 0.067\n",
      "Epoch: 18 | Batches Done: 240/450 | Loss: 0.053\n",
      "Epoch: 18 | Batches Done: 255/450 | Loss: 0.050\n",
      "Epoch: 18 | Batches Done: 270/450 | Loss: 0.059\n",
      "Epoch: 18 | Batches Done: 285/450 | Loss: 0.054\n",
      "Epoch: 18 | Batches Done: 300/450 | Loss: 0.054\n",
      "Epoch: 18 | Batches Done: 315/450 | Loss: 0.081\n",
      "Epoch: 18 | Batches Done: 330/450 | Loss: 0.049\n",
      "Epoch: 18 | Batches Done: 345/450 | Loss: 0.058\n",
      "Epoch: 18 | Batches Done: 360/450 | Loss: 0.068\n",
      "Epoch: 18 | Batches Done: 375/450 | Loss: 0.045\n",
      "Epoch: 18 | Batches Done: 390/450 | Loss: 0.024\n",
      "Epoch: 18 | Batches Done: 405/450 | Loss: 0.056\n",
      "Epoch: 18 | Batches Done: 420/450 | Loss: 0.050\n",
      "Epoch: 18 | Batches Done: 435/450 | Loss: 0.070\n",
      "Epoch: 18 | Batches Done: 450/450 | Loss: 0.054\n",
      "==================================================\n",
      "EPOCH 18 OVERALL LOSS: 0.061\n",
      "==================================================\n",
      "Epoch: 19 | Batches Done: 15/450 | Loss: 0.063\n",
      "Epoch: 19 | Batches Done: 30/450 | Loss: 0.053\n",
      "Epoch: 19 | Batches Done: 45/450 | Loss: 0.045\n",
      "Epoch: 19 | Batches Done: 60/450 | Loss: 0.076\n",
      "Epoch: 19 | Batches Done: 75/450 | Loss: 0.045\n",
      "Epoch: 19 | Batches Done: 90/450 | Loss: 0.055\n",
      "Epoch: 19 | Batches Done: 105/450 | Loss: 0.059\n",
      "Epoch: 19 | Batches Done: 120/450 | Loss: 0.084\n",
      "Epoch: 19 | Batches Done: 135/450 | Loss: 0.079\n",
      "Epoch: 19 | Batches Done: 150/450 | Loss: 0.077\n",
      "Epoch: 19 | Batches Done: 165/450 | Loss: 0.096\n",
      "Epoch: 19 | Batches Done: 180/450 | Loss: 0.082\n",
      "Epoch: 19 | Batches Done: 195/450 | Loss: 0.079\n",
      "Epoch: 19 | Batches Done: 210/450 | Loss: 0.070\n",
      "Epoch: 19 | Batches Done: 225/450 | Loss: 0.058\n",
      "Epoch: 19 | Batches Done: 240/450 | Loss: 0.071\n",
      "Epoch: 19 | Batches Done: 255/450 | Loss: 0.058\n",
      "Epoch: 19 | Batches Done: 270/450 | Loss: 0.057\n",
      "Epoch: 19 | Batches Done: 285/450 | Loss: 0.062\n",
      "Epoch: 19 | Batches Done: 300/450 | Loss: 0.048\n",
      "Epoch: 19 | Batches Done: 315/450 | Loss: 0.039\n",
      "Epoch: 19 | Batches Done: 330/450 | Loss: 0.065\n",
      "Epoch: 19 | Batches Done: 345/450 | Loss: 0.054\n",
      "Epoch: 19 | Batches Done: 360/450 | Loss: 0.082\n",
      "Epoch: 19 | Batches Done: 375/450 | Loss: 0.054\n",
      "Epoch: 19 | Batches Done: 390/450 | Loss: 0.064\n",
      "Epoch: 19 | Batches Done: 405/450 | Loss: 0.051\n",
      "Epoch: 19 | Batches Done: 420/450 | Loss: 0.060\n",
      "Epoch: 19 | Batches Done: 435/450 | Loss: 0.044\n",
      "Epoch: 19 | Batches Done: 450/450 | Loss: 0.083\n",
      "==================================================\n",
      "EPOCH 19 OVERALL LOSS: 0.064\n",
      "==================================================\n",
      "Epoch: 20 | Batches Done: 15/450 | Loss: 0.059\n",
      "Epoch: 20 | Batches Done: 30/450 | Loss: 0.054\n",
      "Epoch: 20 | Batches Done: 45/450 | Loss: 0.032\n",
      "Epoch: 20 | Batches Done: 60/450 | Loss: 0.053\n",
      "Epoch: 20 | Batches Done: 75/450 | Loss: 0.049\n",
      "Epoch: 20 | Batches Done: 90/450 | Loss: 0.054\n",
      "Epoch: 20 | Batches Done: 105/450 | Loss: 0.045\n",
      "Epoch: 20 | Batches Done: 120/450 | Loss: 0.074\n",
      "Epoch: 20 | Batches Done: 135/450 | Loss: 0.033\n",
      "Epoch: 20 | Batches Done: 150/450 | Loss: 0.039\n",
      "Epoch: 20 | Batches Done: 165/450 | Loss: 0.061\n",
      "Epoch: 20 | Batches Done: 180/450 | Loss: 0.045\n",
      "Epoch: 20 | Batches Done: 195/450 | Loss: 0.048\n",
      "Epoch: 20 | Batches Done: 210/450 | Loss: 0.059\n",
      "Epoch: 20 | Batches Done: 225/450 | Loss: 0.052\n",
      "Epoch: 20 | Batches Done: 240/450 | Loss: 0.035\n",
      "Epoch: 20 | Batches Done: 255/450 | Loss: 0.071\n",
      "Epoch: 20 | Batches Done: 270/450 | Loss: 0.065\n",
      "Epoch: 20 | Batches Done: 285/450 | Loss: 0.061\n",
      "Epoch: 20 | Batches Done: 300/450 | Loss: 0.051\n",
      "Epoch: 20 | Batches Done: 315/450 | Loss: 0.063\n",
      "Epoch: 20 | Batches Done: 330/450 | Loss: 0.047\n",
      "Epoch: 20 | Batches Done: 345/450 | Loss: 0.047\n",
      "Epoch: 20 | Batches Done: 360/450 | Loss: 0.048\n",
      "Epoch: 20 | Batches Done: 375/450 | Loss: 0.046\n",
      "Epoch: 20 | Batches Done: 390/450 | Loss: 0.089\n",
      "Epoch: 20 | Batches Done: 405/450 | Loss: 0.097\n",
      "Epoch: 20 | Batches Done: 420/450 | Loss: 0.092\n",
      "Epoch: 20 | Batches Done: 435/450 | Loss: 0.075\n",
      "Epoch: 20 | Batches Done: 450/450 | Loss: 0.087\n",
      "==================================================\n",
      "EPOCH 20 OVERALL LOSS: 0.058\n",
      "==================================================\n",
      "Saving model weights at Epoch 20 ...\n",
      "Epoch: 21 | Batches Done: 15/450 | Loss: 0.077\n",
      "Epoch: 21 | Batches Done: 30/450 | Loss: 0.083\n",
      "Epoch: 21 | Batches Done: 45/450 | Loss: 0.082\n",
      "Epoch: 21 | Batches Done: 60/450 | Loss: 0.083\n",
      "Epoch: 21 | Batches Done: 75/450 | Loss: 0.051\n",
      "Epoch: 21 | Batches Done: 90/450 | Loss: 0.066\n",
      "Epoch: 21 | Batches Done: 105/450 | Loss: 0.058\n",
      "Epoch: 21 | Batches Done: 120/450 | Loss: 0.044\n",
      "Epoch: 21 | Batches Done: 135/450 | Loss: 0.055\n",
      "Epoch: 21 | Batches Done: 150/450 | Loss: 0.089\n",
      "Epoch: 21 | Batches Done: 165/450 | Loss: 0.065\n",
      "Epoch: 21 | Batches Done: 180/450 | Loss: 0.053\n",
      "Epoch: 21 | Batches Done: 195/450 | Loss: 0.058\n",
      "Epoch: 21 | Batches Done: 210/450 | Loss: 0.066\n",
      "Epoch: 21 | Batches Done: 225/450 | Loss: 0.046\n",
      "Epoch: 21 | Batches Done: 240/450 | Loss: 0.033\n",
      "Epoch: 21 | Batches Done: 255/450 | Loss: 0.061\n",
      "Epoch: 21 | Batches Done: 270/450 | Loss: 0.071\n",
      "Epoch: 21 | Batches Done: 285/450 | Loss: 0.064\n",
      "Epoch: 21 | Batches Done: 300/450 | Loss: 0.049\n",
      "Epoch: 21 | Batches Done: 315/450 | Loss: 0.066\n",
      "Epoch: 21 | Batches Done: 330/450 | Loss: 0.054\n",
      "Epoch: 21 | Batches Done: 345/450 | Loss: 0.043\n",
      "Epoch: 21 | Batches Done: 360/450 | Loss: 0.051\n",
      "Epoch: 21 | Batches Done: 375/450 | Loss: 0.049\n",
      "Epoch: 21 | Batches Done: 390/450 | Loss: 0.052\n",
      "Epoch: 21 | Batches Done: 405/450 | Loss: 0.072\n",
      "Epoch: 21 | Batches Done: 420/450 | Loss: 0.055\n",
      "Epoch: 21 | Batches Done: 435/450 | Loss: 0.032\n",
      "Epoch: 21 | Batches Done: 450/450 | Loss: 0.058\n",
      "==================================================\n",
      "EPOCH 21 OVERALL LOSS: 0.060\n",
      "==================================================\n",
      "Epoch: 22 | Batches Done: 15/450 | Loss: 0.050\n",
      "Epoch: 22 | Batches Done: 30/450 | Loss: 0.038\n",
      "Epoch: 22 | Batches Done: 45/450 | Loss: 0.045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Batches Done: 60/450 | Loss: 0.043\n",
      "Epoch: 22 | Batches Done: 75/450 | Loss: 0.049\n",
      "Epoch: 22 | Batches Done: 90/450 | Loss: 0.034\n",
      "Epoch: 22 | Batches Done: 105/450 | Loss: 0.052\n",
      "Epoch: 22 | Batches Done: 120/450 | Loss: 0.063\n",
      "Epoch: 22 | Batches Done: 135/450 | Loss: 0.037\n",
      "Epoch: 22 | Batches Done: 150/450 | Loss: 0.049\n",
      "Epoch: 22 | Batches Done: 165/450 | Loss: 0.036\n",
      "Epoch: 22 | Batches Done: 180/450 | Loss: 0.055\n",
      "Epoch: 22 | Batches Done: 195/450 | Loss: 0.041\n",
      "Epoch: 22 | Batches Done: 210/450 | Loss: 0.061\n",
      "Epoch: 22 | Batches Done: 225/450 | Loss: 0.040\n",
      "Epoch: 22 | Batches Done: 240/450 | Loss: 0.029\n",
      "Epoch: 22 | Batches Done: 255/450 | Loss: 0.047\n",
      "Epoch: 22 | Batches Done: 270/450 | Loss: 0.064\n",
      "Epoch: 22 | Batches Done: 285/450 | Loss: 0.043\n",
      "Epoch: 22 | Batches Done: 300/450 | Loss: 0.030\n",
      "Epoch: 22 | Batches Done: 315/450 | Loss: 0.054\n",
      "Epoch: 22 | Batches Done: 330/450 | Loss: 0.017\n",
      "Epoch: 22 | Batches Done: 345/450 | Loss: 0.044\n",
      "Epoch: 22 | Batches Done: 360/450 | Loss: 0.043\n",
      "Epoch: 22 | Batches Done: 375/450 | Loss: 0.065\n",
      "Epoch: 22 | Batches Done: 390/450 | Loss: 0.039\n",
      "Epoch: 22 | Batches Done: 405/450 | Loss: 0.088\n",
      "Epoch: 22 | Batches Done: 420/450 | Loss: 0.039\n",
      "Epoch: 22 | Batches Done: 435/450 | Loss: 0.074\n",
      "Epoch: 22 | Batches Done: 450/450 | Loss: 0.076\n",
      "==================================================\n",
      "EPOCH 22 OVERALL LOSS: 0.048\n",
      "==================================================\n",
      "Epoch: 23 | Batches Done: 15/450 | Loss: 0.146\n",
      "Epoch: 23 | Batches Done: 30/450 | Loss: 0.145\n",
      "Epoch: 23 | Batches Done: 45/450 | Loss: 0.115\n",
      "Epoch: 23 | Batches Done: 60/450 | Loss: 0.105\n",
      "Epoch: 23 | Batches Done: 75/450 | Loss: 0.073\n",
      "Epoch: 23 | Batches Done: 90/450 | Loss: 0.062\n",
      "Epoch: 23 | Batches Done: 105/450 | Loss: 0.051\n",
      "Epoch: 23 | Batches Done: 120/450 | Loss: 0.036\n",
      "Epoch: 23 | Batches Done: 135/450 | Loss: 0.052\n",
      "Epoch: 23 | Batches Done: 150/450 | Loss: 0.062\n",
      "Epoch: 23 | Batches Done: 165/450 | Loss: 0.034\n",
      "Epoch: 23 | Batches Done: 180/450 | Loss: 0.051\n",
      "Epoch: 23 | Batches Done: 195/450 | Loss: 0.045\n",
      "Epoch: 23 | Batches Done: 210/450 | Loss: 0.047\n",
      "Epoch: 23 | Batches Done: 225/450 | Loss: 0.065\n",
      "Epoch: 23 | Batches Done: 240/450 | Loss: 0.064\n",
      "Epoch: 23 | Batches Done: 255/450 | Loss: 0.047\n",
      "Epoch: 23 | Batches Done: 270/450 | Loss: 0.049\n",
      "Epoch: 23 | Batches Done: 285/450 | Loss: 0.058\n",
      "Epoch: 23 | Batches Done: 300/450 | Loss: 0.038\n",
      "Epoch: 23 | Batches Done: 315/450 | Loss: 0.027\n",
      "Epoch: 23 | Batches Done: 330/450 | Loss: 0.036\n",
      "Epoch: 23 | Batches Done: 345/450 | Loss: 0.038\n",
      "Epoch: 23 | Batches Done: 360/450 | Loss: 0.031\n",
      "Epoch: 23 | Batches Done: 375/450 | Loss: 0.049\n",
      "Epoch: 23 | Batches Done: 390/450 | Loss: 0.100\n",
      "Epoch: 23 | Batches Done: 405/450 | Loss: 0.133\n",
      "Epoch: 23 | Batches Done: 420/450 | Loss: 0.079\n",
      "Epoch: 23 | Batches Done: 435/450 | Loss: 0.061\n",
      "Epoch: 23 | Batches Done: 450/450 | Loss: 0.039\n",
      "==================================================\n",
      "EPOCH 23 OVERALL LOSS: 0.065\n",
      "==================================================\n",
      "Epoch: 24 | Batches Done: 15/450 | Loss: 0.045\n",
      "Epoch: 24 | Batches Done: 30/450 | Loss: 0.034\n",
      "Epoch: 24 | Batches Done: 45/450 | Loss: 0.044\n",
      "Epoch: 24 | Batches Done: 60/450 | Loss: 0.048\n",
      "Epoch: 24 | Batches Done: 75/450 | Loss: 0.027\n",
      "Epoch: 24 | Batches Done: 90/450 | Loss: 0.045\n",
      "Epoch: 24 | Batches Done: 105/450 | Loss: 0.048\n",
      "Epoch: 24 | Batches Done: 120/450 | Loss: 0.075\n",
      "Epoch: 24 | Batches Done: 135/450 | Loss: 0.045\n",
      "Epoch: 24 | Batches Done: 150/450 | Loss: 0.058\n",
      "Epoch: 24 | Batches Done: 165/450 | Loss: 0.048\n",
      "Epoch: 24 | Batches Done: 180/450 | Loss: 0.034\n",
      "Epoch: 24 | Batches Done: 195/450 | Loss: 0.050\n",
      "Epoch: 24 | Batches Done: 210/450 | Loss: 0.030\n",
      "Epoch: 24 | Batches Done: 225/450 | Loss: 0.041\n",
      "Epoch: 24 | Batches Done: 240/450 | Loss: 0.029\n",
      "Epoch: 24 | Batches Done: 255/450 | Loss: 0.044\n",
      "Epoch: 24 | Batches Done: 270/450 | Loss: 0.027\n",
      "Epoch: 24 | Batches Done: 285/450 | Loss: 0.041\n",
      "Epoch: 24 | Batches Done: 300/450 | Loss: 0.021\n",
      "Epoch: 24 | Batches Done: 315/450 | Loss: 0.034\n",
      "Epoch: 24 | Batches Done: 330/450 | Loss: 0.034\n",
      "Epoch: 24 | Batches Done: 345/450 | Loss: 0.033\n",
      "Epoch: 24 | Batches Done: 360/450 | Loss: 0.040\n",
      "Epoch: 24 | Batches Done: 375/450 | Loss: 0.025\n",
      "Epoch: 24 | Batches Done: 390/450 | Loss: 0.013\n",
      "Epoch: 24 | Batches Done: 405/450 | Loss: 0.036\n",
      "Epoch: 24 | Batches Done: 420/450 | Loss: 0.046\n",
      "Epoch: 24 | Batches Done: 435/450 | Loss: 0.041\n",
      "Epoch: 24 | Batches Done: 450/450 | Loss: 0.029\n",
      "==================================================\n",
      "EPOCH 24 OVERALL LOSS: 0.039\n",
      "==================================================\n",
      "Epoch: 25 | Batches Done: 15/450 | Loss: 0.041\n",
      "Epoch: 25 | Batches Done: 30/450 | Loss: 0.036\n",
      "Epoch: 25 | Batches Done: 45/450 | Loss: 0.044\n",
      "Epoch: 25 | Batches Done: 60/450 | Loss: 0.025\n",
      "Epoch: 25 | Batches Done: 75/450 | Loss: 0.018\n",
      "Epoch: 25 | Batches Done: 90/450 | Loss: 0.028\n",
      "Epoch: 25 | Batches Done: 105/450 | Loss: 0.027\n",
      "Epoch: 25 | Batches Done: 120/450 | Loss: 0.053\n",
      "Epoch: 25 | Batches Done: 135/450 | Loss: 0.034\n",
      "Epoch: 25 | Batches Done: 150/450 | Loss: 0.052\n",
      "Epoch: 25 | Batches Done: 165/450 | Loss: 0.029\n",
      "Epoch: 25 | Batches Done: 180/450 | Loss: 0.036\n",
      "Epoch: 25 | Batches Done: 195/450 | Loss: 0.046\n",
      "Epoch: 25 | Batches Done: 210/450 | Loss: 0.044\n",
      "Epoch: 25 | Batches Done: 225/450 | Loss: 0.051\n",
      "Epoch: 25 | Batches Done: 240/450 | Loss: 0.046\n",
      "Epoch: 25 | Batches Done: 255/450 | Loss: 0.048\n",
      "Epoch: 25 | Batches Done: 270/450 | Loss: 0.045\n",
      "Epoch: 25 | Batches Done: 285/450 | Loss: 0.047\n",
      "Epoch: 25 | Batches Done: 300/450 | Loss: 0.035\n",
      "Epoch: 25 | Batches Done: 315/450 | Loss: 0.025\n",
      "Epoch: 25 | Batches Done: 330/450 | Loss: 0.033\n",
      "Epoch: 25 | Batches Done: 345/450 | Loss: 0.032\n",
      "Epoch: 25 | Batches Done: 360/450 | Loss: 0.028\n",
      "Epoch: 25 | Batches Done: 375/450 | Loss: 0.044\n",
      "Epoch: 25 | Batches Done: 390/450 | Loss: 0.031\n",
      "Epoch: 25 | Batches Done: 405/450 | Loss: 0.031\n",
      "Epoch: 25 | Batches Done: 420/450 | Loss: 0.043\n",
      "Epoch: 25 | Batches Done: 435/450 | Loss: 0.042\n",
      "Epoch: 25 | Batches Done: 450/450 | Loss: 0.042\n",
      "==================================================\n",
      "EPOCH 25 OVERALL LOSS: 0.038\n",
      "==================================================\n",
      "Saving model weights at Epoch 25 ...\n",
      "Epoch: 26 | Batches Done: 15/450 | Loss: 0.044\n",
      "Epoch: 26 | Batches Done: 30/450 | Loss: 0.032\n",
      "Epoch: 26 | Batches Done: 45/450 | Loss: 0.039\n",
      "Epoch: 26 | Batches Done: 60/450 | Loss: 0.048\n",
      "Epoch: 26 | Batches Done: 75/450 | Loss: 0.031\n",
      "Epoch: 26 | Batches Done: 90/450 | Loss: 0.038\n",
      "Epoch: 26 | Batches Done: 105/450 | Loss: 0.031\n",
      "Epoch: 26 | Batches Done: 120/450 | Loss: 0.015\n",
      "Epoch: 26 | Batches Done: 135/450 | Loss: 0.025\n",
      "Epoch: 26 | Batches Done: 150/450 | Loss: 0.019\n",
      "Epoch: 26 | Batches Done: 165/450 | Loss: 0.033\n",
      "Epoch: 26 | Batches Done: 180/450 | Loss: 0.034\n",
      "Epoch: 26 | Batches Done: 195/450 | Loss: 0.023\n",
      "Epoch: 26 | Batches Done: 210/450 | Loss: 0.044\n",
      "Epoch: 26 | Batches Done: 225/450 | Loss: 0.034\n",
      "Epoch: 26 | Batches Done: 240/450 | Loss: 0.028\n",
      "Epoch: 26 | Batches Done: 255/450 | Loss: 0.048\n",
      "Epoch: 26 | Batches Done: 270/450 | Loss: 0.053\n",
      "Epoch: 26 | Batches Done: 285/450 | Loss: 0.049\n",
      "Epoch: 26 | Batches Done: 300/450 | Loss: 0.054\n",
      "Epoch: 26 | Batches Done: 315/450 | Loss: 0.025\n",
      "Epoch: 26 | Batches Done: 330/450 | Loss: 0.032\n",
      "Epoch: 26 | Batches Done: 345/450 | Loss: 0.041\n",
      "Epoch: 26 | Batches Done: 360/450 | Loss: 0.039\n",
      "Epoch: 26 | Batches Done: 375/450 | Loss: 0.044\n",
      "Epoch: 26 | Batches Done: 390/450 | Loss: 0.032\n",
      "Epoch: 26 | Batches Done: 405/450 | Loss: 0.022\n",
      "Epoch: 26 | Batches Done: 420/450 | Loss: 0.027\n",
      "Epoch: 26 | Batches Done: 435/450 | Loss: 0.030\n",
      "Epoch: 26 | Batches Done: 450/450 | Loss: 0.025\n",
      "==================================================\n",
      "EPOCH 26 OVERALL LOSS: 0.035\n",
      "==================================================\n",
      "Epoch: 27 | Batches Done: 15/450 | Loss: 0.032\n",
      "Epoch: 27 | Batches Done: 30/450 | Loss: 0.033\n",
      "Epoch: 27 | Batches Done: 45/450 | Loss: 0.015\n",
      "Epoch: 27 | Batches Done: 60/450 | Loss: 0.016\n",
      "Epoch: 27 | Batches Done: 75/450 | Loss: 0.037\n",
      "Epoch: 27 | Batches Done: 90/450 | Loss: 0.039\n",
      "Epoch: 27 | Batches Done: 105/450 | Loss: 0.049\n",
      "Epoch: 27 | Batches Done: 120/450 | Loss: 0.027\n",
      "Epoch: 27 | Batches Done: 135/450 | Loss: 0.042\n",
      "Epoch: 27 | Batches Done: 150/450 | Loss: 0.028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Batches Done: 165/450 | Loss: 0.045\n",
      "Epoch: 27 | Batches Done: 180/450 | Loss: 0.034\n",
      "Epoch: 27 | Batches Done: 195/450 | Loss: 0.024\n",
      "Epoch: 27 | Batches Done: 210/450 | Loss: 0.036\n",
      "Epoch: 27 | Batches Done: 225/450 | Loss: 0.023\n",
      "Epoch: 27 | Batches Done: 240/450 | Loss: 0.015\n",
      "Epoch: 27 | Batches Done: 255/450 | Loss: 0.036\n",
      "Epoch: 27 | Batches Done: 270/450 | Loss: 0.031\n",
      "Epoch: 27 | Batches Done: 285/450 | Loss: 0.050\n",
      "Epoch: 27 | Batches Done: 300/450 | Loss: 0.023\n",
      "Epoch: 27 | Batches Done: 315/450 | Loss: 0.046\n",
      "Epoch: 27 | Batches Done: 330/450 | Loss: 0.065\n",
      "Epoch: 27 | Batches Done: 345/450 | Loss: 0.025\n",
      "Epoch: 27 | Batches Done: 360/450 | Loss: 0.032\n",
      "Epoch: 27 | Batches Done: 375/450 | Loss: 0.077\n",
      "Epoch: 27 | Batches Done: 390/450 | Loss: 0.111\n",
      "Epoch: 27 | Batches Done: 405/450 | Loss: 0.143\n",
      "Epoch: 27 | Batches Done: 420/450 | Loss: 0.077\n",
      "Epoch: 27 | Batches Done: 435/450 | Loss: 0.042\n",
      "Epoch: 27 | Batches Done: 450/450 | Loss: 0.068\n",
      "==================================================\n",
      "EPOCH 27 OVERALL LOSS: 0.044\n",
      "==================================================\n",
      "Epoch: 28 | Batches Done: 15/450 | Loss: 0.072\n",
      "Epoch: 28 | Batches Done: 30/450 | Loss: 0.048\n",
      "Epoch: 28 | Batches Done: 45/450 | Loss: 0.045\n",
      "Epoch: 28 | Batches Done: 60/450 | Loss: 0.043\n",
      "Epoch: 28 | Batches Done: 75/450 | Loss: 0.036\n",
      "Epoch: 28 | Batches Done: 90/450 | Loss: 0.037\n",
      "Epoch: 28 | Batches Done: 105/450 | Loss: 0.027\n",
      "Epoch: 28 | Batches Done: 120/450 | Loss: 0.027\n",
      "Epoch: 28 | Batches Done: 135/450 | Loss: 0.019\n",
      "Epoch: 28 | Batches Done: 150/450 | Loss: 0.047\n",
      "Epoch: 28 | Batches Done: 165/450 | Loss: 0.037\n",
      "Epoch: 28 | Batches Done: 180/450 | Loss: 0.033\n",
      "Epoch: 28 | Batches Done: 195/450 | Loss: 0.044\n",
      "Epoch: 28 | Batches Done: 210/450 | Loss: 0.022\n",
      "Epoch: 28 | Batches Done: 225/450 | Loss: 0.021\n",
      "Epoch: 28 | Batches Done: 240/450 | Loss: 0.022\n",
      "Epoch: 28 | Batches Done: 255/450 | Loss: 0.014\n",
      "Epoch: 28 | Batches Done: 270/450 | Loss: 0.020\n",
      "Epoch: 28 | Batches Done: 285/450 | Loss: 0.027\n",
      "Epoch: 28 | Batches Done: 300/450 | Loss: 0.023\n",
      "Epoch: 28 | Batches Done: 315/450 | Loss: 0.022\n",
      "Epoch: 28 | Batches Done: 330/450 | Loss: 0.036\n",
      "Epoch: 28 | Batches Done: 345/450 | Loss: 0.027\n",
      "Epoch: 28 | Batches Done: 360/450 | Loss: 0.038\n",
      "Epoch: 28 | Batches Done: 375/450 | Loss: 0.039\n",
      "Epoch: 28 | Batches Done: 390/450 | Loss: 0.021\n",
      "Epoch: 28 | Batches Done: 405/450 | Loss: 0.024\n",
      "Epoch: 28 | Batches Done: 420/450 | Loss: 0.036\n",
      "Epoch: 28 | Batches Done: 435/450 | Loss: 0.032\n",
      "Epoch: 28 | Batches Done: 450/450 | Loss: 0.053\n",
      "==================================================\n",
      "EPOCH 28 OVERALL LOSS: 0.033\n",
      "==================================================\n",
      "Epoch: 29 | Batches Done: 15/450 | Loss: 0.036\n",
      "Epoch: 29 | Batches Done: 30/450 | Loss: 0.024\n",
      "Epoch: 29 | Batches Done: 45/450 | Loss: 0.020\n",
      "Epoch: 29 | Batches Done: 60/450 | Loss: 0.024\n",
      "Epoch: 29 | Batches Done: 75/450 | Loss: 0.035\n",
      "Epoch: 29 | Batches Done: 90/450 | Loss: 0.023\n",
      "Epoch: 29 | Batches Done: 105/450 | Loss: 0.032\n",
      "Epoch: 29 | Batches Done: 120/450 | Loss: 0.035\n",
      "Epoch: 29 | Batches Done: 135/450 | Loss: 0.015\n",
      "Epoch: 29 | Batches Done: 150/450 | Loss: 0.035\n",
      "Epoch: 29 | Batches Done: 165/450 | Loss: 0.041\n",
      "Epoch: 29 | Batches Done: 180/450 | Loss: 0.025\n",
      "Epoch: 29 | Batches Done: 195/450 | Loss: 0.023\n",
      "Epoch: 29 | Batches Done: 210/450 | Loss: 0.025\n",
      "Epoch: 29 | Batches Done: 225/450 | Loss: 0.039\n",
      "Epoch: 29 | Batches Done: 240/450 | Loss: 0.050\n",
      "Epoch: 29 | Batches Done: 255/450 | Loss: 0.026\n",
      "Epoch: 29 | Batches Done: 270/450 | Loss: 0.024\n",
      "Epoch: 29 | Batches Done: 285/450 | Loss: 0.024\n",
      "Epoch: 29 | Batches Done: 300/450 | Loss: 0.034\n",
      "Epoch: 29 | Batches Done: 315/450 | Loss: 0.051\n",
      "Epoch: 29 | Batches Done: 330/450 | Loss: 0.032\n",
      "Epoch: 29 | Batches Done: 345/450 | Loss: 0.036\n",
      "Epoch: 29 | Batches Done: 360/450 | Loss: 0.039\n",
      "Epoch: 29 | Batches Done: 375/450 | Loss: 0.028\n",
      "Epoch: 29 | Batches Done: 390/450 | Loss: 0.032\n",
      "Epoch: 29 | Batches Done: 405/450 | Loss: 0.038\n",
      "Epoch: 29 | Batches Done: 420/450 | Loss: 0.015\n",
      "Epoch: 29 | Batches Done: 435/450 | Loss: 0.017\n",
      "Epoch: 29 | Batches Done: 450/450 | Loss: 0.035\n",
      "==================================================\n",
      "EPOCH 29 OVERALL LOSS: 0.030\n",
      "==================================================\n",
      "Epoch: 30 | Batches Done: 15/450 | Loss: 0.021\n",
      "Epoch: 30 | Batches Done: 30/450 | Loss: 0.034\n",
      "Epoch: 30 | Batches Done: 45/450 | Loss: 0.017\n",
      "Epoch: 30 | Batches Done: 60/450 | Loss: 0.012\n",
      "Epoch: 30 | Batches Done: 75/450 | Loss: 0.016\n",
      "Epoch: 30 | Batches Done: 90/450 | Loss: 0.031\n",
      "Epoch: 30 | Batches Done: 105/450 | Loss: 0.023\n",
      "Epoch: 30 | Batches Done: 120/450 | Loss: 0.012\n",
      "Epoch: 30 | Batches Done: 135/450 | Loss: 0.027\n",
      "Epoch: 30 | Batches Done: 150/450 | Loss: 0.024\n",
      "Epoch: 30 | Batches Done: 165/450 | Loss: 0.015\n",
      "Epoch: 30 | Batches Done: 180/450 | Loss: 0.022\n",
      "Epoch: 30 | Batches Done: 195/450 | Loss: 0.038\n",
      "Epoch: 30 | Batches Done: 210/450 | Loss: 0.044\n",
      "Epoch: 30 | Batches Done: 225/450 | Loss: 0.021\n",
      "Epoch: 30 | Batches Done: 240/450 | Loss: 0.051\n",
      "Epoch: 30 | Batches Done: 255/450 | Loss: 0.041\n",
      "Epoch: 30 | Batches Done: 270/450 | Loss: 0.032\n",
      "Epoch: 30 | Batches Done: 285/450 | Loss: 0.040\n",
      "Epoch: 30 | Batches Done: 300/450 | Loss: 0.047\n",
      "Epoch: 30 | Batches Done: 315/450 | Loss: 0.048\n",
      "Epoch: 30 | Batches Done: 330/450 | Loss: 0.040\n",
      "Epoch: 30 | Batches Done: 345/450 | Loss: 0.044\n",
      "Epoch: 30 | Batches Done: 360/450 | Loss: 0.037\n",
      "Epoch: 30 | Batches Done: 375/450 | Loss: 0.030\n",
      "Epoch: 30 | Batches Done: 390/450 | Loss: 0.034\n",
      "Epoch: 30 | Batches Done: 405/450 | Loss: 0.061\n",
      "Epoch: 30 | Batches Done: 420/450 | Loss: 0.047\n",
      "Epoch: 30 | Batches Done: 435/450 | Loss: 0.038\n",
      "Epoch: 30 | Batches Done: 450/450 | Loss: 0.036\n",
      "==================================================\n",
      "EPOCH 30 OVERALL LOSS: 0.033\n",
      "==================================================\n",
      "Saving model weights at Epoch 30 ...\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "model.train()\n",
    "losslist = []\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    epoch_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 15 == 14:    # print every 15 mini-batches\n",
    "            print(f'Epoch: {epoch+1} | Batches Done: {i+1}/450 | Loss: {(running_loss/15):.3f}')\n",
    "            epoch_loss += running_loss\n",
    "            running_loss = 0.0\n",
    "    losslist.append(epoch_loss/450)\n",
    "    print(\"=\"*50)\n",
    "    print(f\"EPOCH {epoch+1} OVERALL LOSS: {losslist[-1]:.3f}\")\n",
    "    print(\"=\"*50)\n",
    "    if epoch % 5 == 4:\n",
    "        path = f\"models/gru_{epoch+1}_checkpoint.pth\"\n",
    "        print(f\"Saving model weights at Epoch {epoch+1} ...\")\n",
    "        torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save losslist for later comparison\n",
    "np.save('gru_losslist.npy', losslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading saved model\n",
    "model = Model().to(device)\n",
    "model.load_state_dict(torch.load(\"models/gru_30_checkpoint.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM classified 8 and 18 as incorrect on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1/30 classified as CORRECT\n",
      "Sample 2/30 classified as CORRECT\n",
      "Sample 3/30 classified as CORRECT\n",
      "Sample 4/30 classified as CORRECT\n",
      "Sample 5/30 classified as CORRECT\n",
      "Sample 6/30 classified as CORRECT\n",
      "Sample 7/30 classified as CORRECT\n",
      "Sample 8/30 classified as INCORRECT\n",
      "Sample 9/30 classified as CORRECT\n",
      "Sample 10/30 classified as CORRECT\n",
      "Sample 11/30 classified as CORRECT\n",
      "Sample 12/30 classified as CORRECT\n",
      "Sample 13/30 classified as CORRECT\n",
      "Sample 14/30 classified as CORRECT\n",
      "Sample 15/30 classified as INCORRECT\n",
      "Sample 16/30 classified as CORRECT\n",
      "Sample 17/30 classified as CORRECT\n",
      "Sample 18/30 classified as CORRECT\n",
      "Sample 19/30 classified as CORRECT\n",
      "Sample 20/30 classified as CORRECT\n",
      "Sample 21/30 classified as CORRECT\n",
      "Sample 22/30 classified as CORRECT\n",
      "Sample 23/30 classified as CORRECT\n",
      "Sample 24/30 classified as CORRECT\n",
      "Sample 25/30 classified as CORRECT\n",
      "Sample 26/30 classified as CORRECT\n",
      "Sample 27/30 classified as CORRECT\n",
      "Sample 28/30 classified as CORRECT\n",
      "Sample 29/30 classified as CORRECT\n",
      "Sample 30/30 classified as CORRECT\n",
      "==================================================\n",
      "Accuracy of the model on 30 unseen music samples: 93.33%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, X_test, Y_test, threshold=0.6, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reloading saved model\n",
    "model = Model().to(device)\n",
    "model.load_state_dict(torch.load(\"gru_30_checkpoint.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Accuracy on given data: 99.21%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "evaluate_naive(model, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Accuracy on given data: 83.68%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "evaluate_naive(model, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss curve for comparison with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losslist_lstm = np.load('lstm_losslist.npy')\n",
    "losslist_gru = np.load('gru_losslist.npy')\n",
    "\n",
    "plt.plot(list(np.arange(1,len(losslist_lstm)+1)), losslist_lstm, label=\"LSTM-based\")\n",
    "plt.plot(list(np.arange(1,len(losslist_gru)+1)), losslist_gru, label=\"GRU-based\")\n",
    "plt.title(\"Average Loss vs Epoch for LSTM with Attention model\\n\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"plots/comparison_loss_plot.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "deepSRGM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
